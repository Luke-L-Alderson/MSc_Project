Starting Sweep: Batch Size: 64, Learning Rate: 0.1
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Subsets
Training: 6000
Testing: 1000
(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0078, 0.2039, 0.4078, 0.1882, 0.0353, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0902, 0.6980, 0.9922, 0.9922, 0.9922, 0.9529, 0.6627, 0.6627,
          0.3373, 0.2980, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,
          0.8196, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961,
          0.9922, 0.9922, 0.9451, 0.6471, 0.1059, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5020,
          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961,
          0.9922, 0.9922, 0.9922, 0.9922, 0.9098, 0.6863, 0.0706, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2314, 0.9255,
          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9961,
          0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9255, 0.2275,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9922,
          0.9922, 0.5804, 0.7020, 0.8431, 0.9922, 0.9922, 0.9922, 0.8392,
          0.4039, 0.5529, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.9333,
          0.2353, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9765, 0.9922,
          0.5608, 0.0118, 0.0000, 0.1725, 0.7294, 0.3882, 0.1451, 0.1098,
          0.0000, 0.0000, 0.0431, 0.4196, 0.9294, 0.9922, 0.9922, 0.9922,
          0.9373, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8980, 0.9922, 0.8275,
          0.0863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.2784, 0.8824, 0.9922, 0.9922,
          0.9922, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.9961, 0.4941,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2471, 0.9843, 0.9961,
          0.9961, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.2196,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6275, 0.9922,
          0.8000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9961, 0.9922, 0.2353,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.2863, 0.9020, 0.9451,
          0.2353, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8863, 0.9922, 0.8431,
          0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0549, 0.4863, 0.9922, 0.9922, 0.5216,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2392, 0.9725, 0.9922,
          0.9451, 0.7137, 0.3922, 0.0549, 0.0431, 0.2353, 0.0000, 0.0000,
          0.1647, 0.2000, 0.5529, 0.9059, 0.9765, 0.6824, 0.2235, 0.0118,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2196, 0.8353,
          0.9647, 0.9922, 0.9922, 0.8627, 0.8510, 0.8627, 0.4431, 0.7922,
          0.9098, 0.9608, 0.9176, 0.5725, 0.2353, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.3059, 0.6588, 0.9294, 0.9922, 0.9922, 0.8667, 0.7451, 0.3569,
          0.0471, 0.1725, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0275, 0.0353, 0.0353, 0.0235, 0.0118, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]]), 0)
<class 'tuple'>
2
Making Dataloaders
Defining network
2024-06-05 17:23:09.872483
Training!
0:00:00.014385
[1/3, 5/94] Training Loss: 15.970468521118164
[1/3, 10/94] Training Loss: 1.1274198293685913
[1/3, 15/94] Training Loss: 1.1032640933990479
[1/3, 20/94] Training Loss: 1.132344126701355
[1/3, 25/94] Training Loss: 1.081933856010437
[1/3, 30/94] Training Loss: 1.0668424367904663
[1/3, 35/94] Training Loss: 1.0985599756240845
[1/3, 40/94] Training Loss: 1.093408465385437
[1/3, 45/94] Training Loss: 1.0816000699996948
[1/3, 50/94] Training Loss: 1.0836176872253418
[1/3, 55/94] Training Loss: 1.094039797782898
[1/3, 60/94] Training Loss: 1.0906411409378052
[1/3, 65/94] Training Loss: 1.118013858795166
[1/3, 70/94] Training Loss: 1.1180893182754517
[1/3, 75/94] Training Loss: 1.1713117361068726
[1/3, 80/94] Training Loss: 1.1034204959869385
[1/3, 85/94] Training Loss: 1.123422384262085
[1/3, 90/94] Training Loss: 1.140832781791687
Testing!
[1/3, 1/16]
[1/3, 2/16]
[1/3, 3/16]
[1/3, 4/16]
[1/3, 5/16]
[1/3, 6/16]
[1/3, 7/16]
[1/3, 8/16]
[1/3, 9/16]
[1/3, 10/16]
[1/3, 11/16]
[1/3, 12/16]
[1/3, 13/16]
[1/3, 14/16]
[1/3, 15/16]
[1/3, 16/16]
Testing Loss: 1.1742584705352783
Training!
0:02:03.835399
[2/3, 5/94] Training Loss: 1.1112383604049683
[2/3, 10/94] Training Loss: 1.1534709930419922
[2/3, 15/94] Training Loss: 1.1129519939422607
[2/3, 20/94] Training Loss: 1.048676609992981
[2/3, 25/94] Training Loss: 1.0977520942687988
[2/3, 30/94] Training Loss: 1.0766000747680664
[2/3, 35/94] Training Loss: 1.111493468284607
[2/3, 40/94] Training Loss: 1.1373085975646973
[2/3, 45/94] Training Loss: 1.1202837228775024
[2/3, 50/94] Training Loss: 1.141700029373169
[2/3, 55/94] Training Loss: 1.1066597700119019
[2/3, 60/94] Training Loss: 1.1319599151611328
[2/3, 65/94] Training Loss: 1.061381220817566
[2/3, 70/94] Training Loss: 1.0721310377120972
[2/3, 75/94] Training Loss: 1.119024395942688
[2/3, 80/94] Training Loss: 1.0565873384475708
[2/3, 85/94] Training Loss: 1.1438413858413696
[2/3, 90/94] Training Loss: 1.0854891538619995
Testing!
[2/3, 1/16]
[2/3, 2/16]
[2/3, 3/16]
[2/3, 4/16]
[2/3, 5/16]
[2/3, 6/16]
[2/3, 7/16]
[2/3, 8/16]
[2/3, 9/16]
[2/3, 10/16]
[2/3, 11/16]
[2/3, 12/16]
[2/3, 13/16]
[2/3, 14/16]
[2/3, 15/16]
[2/3, 16/16]
Testing Loss: 1.1006110906600952
Training!
0:04:08.572303
[3/3, 5/94] Training Loss: 1.135537028312683
[3/3, 10/94] Training Loss: 1.1425448656082153
[3/3, 15/94] Training Loss: 1.137082815170288
[3/3, 20/94] Training Loss: 1.1416220664978027
[3/3, 25/94] Training Loss: 1.1504026651382446
[3/3, 30/94] Training Loss: 1.0927538871765137
[3/3, 35/94] Training Loss: 1.049207329750061
[3/3, 40/94] Training Loss: 1.1288319826126099
[3/3, 45/94] Training Loss: 1.130862832069397
[3/3, 50/94] Training Loss: 1.0989134311676025
[3/3, 55/94] Training Loss: 1.1288869380950928
[3/3, 60/94] Training Loss: 1.0931222438812256
[3/3, 65/94] Training Loss: 1.1100825071334839
[3/3, 70/94] Training Loss: 1.1081292629241943
[3/3, 75/94] Training Loss: 1.1841827630996704
[3/3, 80/94] Training Loss: 1.1267834901809692
[3/3, 85/94] Training Loss: 1.1528202295303345
[3/3, 90/94] Training Loss: 1.0787209272384644
Testing!
[3/3, 1/16]
[3/3, 2/16]
[3/3, 3/16]
[3/3, 4/16]
[3/3, 5/16]
[3/3, 6/16]
[3/3, 7/16]
[3/3, 8/16]
[3/3, 9/16]
[3/3, 10/16]
[3/3, 11/16]
[3/3, 12/16]
[3/3, 13/16]
[3/3, 14/16]
[3/3, 15/16]
[3/3, 16/16]
Testing Loss: 1.183777928352356
Training and Testing Finished
1.183777928352356
<class 'float'>
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\sklearn\decomposition\_pca.py:640: RuntimeWarning: invalid value encountered in divide
  self.explained_variance_ratio_ = self.explained_variance_ / total_var
 [py.warnings]
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\sklearn\manifold\_t_sne.py:987: RuntimeWarning: invalid value encountered in divide
  X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4
