Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/7500] Training Loss: 92.99549050331116
[1/3, 20/7500] Training Loss: 1.1996031761169434
[1/3, 30/7500] Training Loss: 1.245904016494751
[1/3, 40/7500] Training Loss: 1.1908380508422851
[1/3, 50/7500] Training Loss: 1.1688960373401642
[1/3, 60/7500] Training Loss: 1.1944653272628785
[1/3, 70/7500] Training Loss: 1.2346187233924866
[1/3, 80/7500] Training Loss: 1.1880975127220155
[1/3, 90/7500] Training Loss: 1.1989458560943604
[1/3, 100/7500] Training Loss: 1.3003203988075256
[1/3, 110/7500] Training Loss: 1.2494083166122436
[1/3, 120/7500] Training Loss: 1.1961222529411315
[1/3, 130/7500] Training Loss: 1.1533024787902832
[1/3, 140/7500] Training Loss: 1.1811740517616272
[1/3, 150/7500] Training Loss: 1.2033616304397583
[1/3, 160/7500] Training Loss: 1.3004099130630493
[1/3, 170/7500] Training Loss: 1.2234059572219849
[1/3, 180/7500] Training Loss: 1.1581271171569825
[1/3, 190/7500] Training Loss: 1.2069951295852661
[1/3, 200/7500] Training Loss: 1.2228606700897218
[1/3, 210/7500] Training Loss: 1.2684243321418762
[1/3, 220/7500] Training Loss: 1.2367983639240265
[1/3, 230/7500] Training Loss: 1.2126607418060302
[1/3, 240/7500] Training Loss: 1.2758626818656922
[1/3, 250/7500] Training Loss: 1.2217694342136383
[1/3, 260/7500] Training Loss: 1.216040128469467
[1/3, 270/7500] Training Loss: 1.2519956231117249
[1/3, 280/7500] Training Loss: 1.245275330543518
[1/3, 290/7500] Training Loss: 1.1966193199157715
[1/3, 300/7500] Training Loss: 1.163558840751648
[1/3, 310/7500] Training Loss: 1.186809277534485
[1/3, 320/7500] Training Loss: 1.223586630821228
[1/3, 330/7500] Training Loss: 1.21130051612854
[1/3, 340/7500] Training Loss: 1.2859925508499146
[1/3, 350/7500] Training Loss: 1.2136166095733643
[1/3, 360/7500] Training Loss: 1.2585942506790162
[1/3, 370/7500] Training Loss: 1.1743809044361115
[1/3, 380/7500] Training Loss: 1.2417049825191497
[1/3, 390/7500] Training Loss: 1.1597466588020324
[1/3, 400/7500] Training Loss: 1.1721808850765227
[1/3, 410/7500] Training Loss: 1.226515007019043
[1/3, 420/7500] Training Loss: 1.2421046257019044
[1/3, 430/7500] Training Loss: 1.2249578356742858
[1/3, 440/7500] Training Loss: 1.2061374187469482
[1/3, 450/7500] Training Loss: 1.2096423149108886
[1/3, 460/7500] Training Loss: 1.2274885773658752
[1/3, 470/7500] Training Loss: 1.1801833033561706
[1/3, 480/7500] Training Loss: 1.22273131608963
[1/3, 490/7500] Training Loss: 1.236858344078064
[1/3, 500/7500] Training Loss: 1.2184656262397766
[1/3, 510/7500] Training Loss: 1.1312757730484009
[1/3, 520/7500] Training Loss: 1.1727191507816315
[1/3, 530/7500] Training Loss: 1.1748066782951354
[1/3, 540/7500] Training Loss: 1.2040079295635224
[1/3, 550/7500] Training Loss: 1.1628544449806213
[1/3, 560/7500] Training Loss: 1.2525673627853393
[1/3, 570/7500] Training Loss: 1.196161937713623
[1/3, 580/7500] Training Loss: 1.2442823648452759
[1/3, 590/7500] Training Loss: 1.2975724577903747
[1/3, 600/7500] Training Loss: 1.1692500948905944
[1/3, 610/7500] Training Loss: 1.2324619531631469
[1/3, 620/7500] Training Loss: 1.1826296031475068
[1/3, 630/7500] Training Loss: 1.2288453578948975
[1/3, 640/7500] Training Loss: 1.2197376787662506
[1/3, 650/7500] Training Loss: 1.2033769965171814
[1/3, 660/7500] Training Loss: 1.271196711063385
[1/3, 670/7500] Training Loss: 1.2486672878265381
[1/3, 680/7500] Training Loss: 1.2478977203369142
[1/3, 690/7500] Training Loss: 1.1982529938220978
[1/3, 700/7500] Training Loss: 1.1911313593387605
[1/3, 710/7500] Training Loss: 1.1787940680980682
[1/3, 720/7500] Training Loss: 1.125152015686035
[1/3, 730/7500] Training Loss: 1.2368830561637878
[1/3, 740/7500] Training Loss: 1.2261361837387086
[1/3, 750/7500] Training Loss: 1.1825087368488312
[1/3, 760/7500] Training Loss: 1.1795874953269958
[1/3, 770/7500] Training Loss: 1.2010248899459839
[1/3, 780/7500] Training Loss: 1.212307220697403
[1/3, 790/7500] Training Loss: 1.2171486616134644
[1/3, 800/7500] Training Loss: 1.1484334409236907
[1/3, 810/7500] Training Loss: 1.1782811343669892
[1/3, 820/7500] Training Loss: 1.254463940858841
[1/3, 830/7500] Training Loss: 1.2365355253219605
[1/3, 840/7500] Training Loss: 1.2157747328281403
[1/3, 850/7500] Training Loss: 1.2389531493186952
[1/3, 860/7500] Training Loss: 1.205859076976776
[1/3, 870/7500] Training Loss: 1.1684349954128266
[1/3, 880/7500] Training Loss: 1.2237074971199036
[1/3, 890/7500] Training Loss: 1.180867314338684
[1/3, 900/7500] Training Loss: 1.1263620793819427
[1/3, 910/7500] Training Loss: 1.18251633644104
[1/3, 920/7500] Training Loss: 1.1492795944213867
[1/3, 930/7500] Training Loss: 1.1850846409797668
[1/3, 940/7500] Training Loss: 1.1380235135555268
[1/3, 950/7500] Training Loss: 1.1363306403160096
[1/3, 960/7500] Training Loss: 1.1712003946304321
[1/3, 970/7500] Training Loss: 1.1056131362915038
[1/3, 980/7500] Training Loss: 1.19052734375
[1/3, 990/7500] Training Loss: 1.095196408033371
[1/3, 1000/7500] Training Loss: 1.0956292629241944
[1/3, 1010/7500] Training Loss: 1.1183461546897888
[1/3, 1020/7500] Training Loss: 1.0987128913402557
[1/3, 1030/7500] Training Loss: 1.1627639651298523
[1/3, 1040/7500] Training Loss: 1.0457025289535522
[1/3, 1050/7500] Training Loss: 1.0763490617275238
[1/3, 1060/7500] Training Loss: 1.0421795845031738
[1/3, 1070/7500] Training Loss: 1.1494681179523467
[1/3, 1080/7500] Training Loss: 1.1379880130290985
[1/3, 1090/7500] Training Loss: 1.237429940700531
[1/3, 1100/7500] Training Loss: 1.3063434839248658
[1/3, 1110/7500] Training Loss: 1.2377671003341675
[1/3, 1120/7500] Training Loss: 1.2020909309387207
[1/3, 1130/7500] Training Loss: 1.2551053285598754
[1/3, 1140/7500] Training Loss: 1.183084100484848
[1/3, 1150/7500] Training Loss: 1.1699412703514098
[1/3, 1160/7500] Training Loss: 1.2103949904441833
[1/3, 1170/7500] Training Loss: 1.2241353154182435
[1/3, 1180/7500] Training Loss: 1.2154696822166442
[1/3, 1190/7500] Training Loss: 1.1838136315345764
[1/3, 1200/7500] Training Loss: 1.251635527610779
[1/3, 1210/7500] Training Loss: 1.2469500422477722
[1/3, 1220/7500] Training Loss: 1.1871400773525238
[1/3, 1230/7500] Training Loss: 1.2295903325080872
[1/3, 1240/7500] Training Loss: 1.2268186330795288
[1/3, 1250/7500] Training Loss: 1.1663210153579713
[1/3, 1260/7500] Training Loss: 1.2162670493125916
[1/3, 1270/7500] Training Loss: 1.1258700013160705
[1/3, 1280/7500] Training Loss: 1.2081644177436828
[1/3, 1290/7500] Training Loss: 1.1819734573364258
[1/3, 1300/7500] Training Loss: 1.1808851659297943
[1/3, 1310/7500] Training Loss: 1.202335810661316
[1/3, 1320/7500] Training Loss: 1.1297203838825225
[1/3, 1330/7500] Training Loss: 1.228208029270172
[1/3, 1340/7500] Training Loss: 1.197087675333023
[1/3, 1350/7500] Training Loss: 1.1770047426223755
[1/3, 1360/7500] Training Loss: 1.1789120852947235
[1/3, 1370/7500] Training Loss: 1.152689915895462
[1/3, 1380/7500] Training Loss: 1.2539724111557007
[1/3, 1390/7500] Training Loss: 1.1441591262817383
[1/3, 1400/7500] Training Loss: 1.1961795687675476
[1/3, 1410/7500] Training Loss: 1.2362221717834472
[1/3, 1420/7500] Training Loss: 1.211506003141403
[1/3, 1430/7500] Training Loss: 1.2282567143440246
[1/3, 1440/7500] Training Loss: 1.1667886614799499
[1/3, 1450/7500] Training Loss: 1.1768357753753662
[1/3, 1460/7500] Training Loss: 1.1814087629318237
[1/3, 1470/7500] Training Loss: 1.2032727956771851
[1/3, 1480/7500] Training Loss: 1.1285773396492005
[1/3, 1490/7500] Training Loss: 1.1283061683177948
[1/3, 1500/7500] Training Loss: 1.1843949377536773
[1/3, 1510/7500] Training Loss: 1.1529270946979522
[1/3, 1520/7500] Training Loss: 1.2432325959205628
[1/3, 1530/7500] Training Loss: 1.1707709729671478
[1/3, 1540/7500] Training Loss: 1.1737695217132569
[1/3, 1550/7500] Training Loss: 1.1637182176113128
[1/3, 1560/7500] Training Loss: 1.1245113253593444
[1/3, 1570/7500] Training Loss: 1.1671875596046448
[1/3, 1580/7500] Training Loss: 1.2042392015457153
[1/3, 1590/7500] Training Loss: 1.150641715526581
[1/3, 1600/7500] Training Loss: 1.250593864917755
[1/3, 1610/7500] Training Loss: 1.151556932926178
[1/3, 1620/7500] Training Loss: 1.196261477470398
[1/3, 1630/7500] Training Loss: 1.2276294589042664
[1/3, 1640/7500] Training Loss: 1.1357724130153657
[1/3, 1650/7500] Training Loss: 1.2244875133037567
[1/3, 1660/7500] Training Loss: 1.1207867741584778
[1/3, 1670/7500] Training Loss: 1.1500620722770691
[1/3, 1680/7500] Training Loss: 1.1297330737113953
[1/3, 1690/7500] Training Loss: 1.0721731543540955
[1/3, 1700/7500] Training Loss: 1.1051111221313477
[1/3, 1710/7500] Training Loss: 1.2013911485671998
[1/3, 1720/7500] Training Loss: 1.1178857803344726
[1/3, 1730/7500] Training Loss: 1.1689352810382843
[1/3, 1740/7500] Training Loss: 1.1042081236839294
[1/3, 1750/7500] Training Loss: 1.1487327218055725
[1/3, 1760/7500] Training Loss: 1.168188601732254
[1/3, 1770/7500] Training Loss: 1.121930903196335
[1/3, 1780/7500] Training Loss: 1.1255312502384185
[1/3, 1790/7500] Training Loss: 1.1200194656848907
[1/3, 1800/7500] Training Loss: 1.0966980159282684
[1/3, 1810/7500] Training Loss: 1.1575728058815002
[1/3, 1820/7500] Training Loss: 1.0554807424545287
[1/3, 1830/7500] Training Loss: 1.1793322145938874
[1/3, 1840/7500] Training Loss: 1.1603701412677765
[1/3, 1850/7500] Training Loss: 1.2018605709075927
[1/3, 1860/7500] Training Loss: 1.1624425590038299
[1/3, 1870/7500] Training Loss: 1.123453825712204
[1/3, 1880/7500] Training Loss: 1.0748321294784546
[1/3, 1890/7500] Training Loss: 1.1429024636745453
[1/3, 1900/7500] Training Loss: 1.1580260157585145
[1/3, 1910/7500] Training Loss: 1.1457813918590545
[1/3, 1920/7500] Training Loss: 1.1328836679458618
[1/3, 1930/7500] Training Loss: 1.1141920208930969
[1/3, 1940/7500] Training Loss: 1.1999242663383485
[1/3, 1950/7500] Training Loss: 1.1334497570991515
[1/3, 1960/7500] Training Loss: 1.1405278980731963
[1/3, 1970/7500] Training Loss: 1.1740813612937928
[1/3, 1980/7500] Training Loss: 1.1823891520500183
[1/3, 1990/7500] Training Loss: 1.2285151779651642
[1/3, 2000/7500] Training Loss: 1.1344462275505065
[1/3, 2010/7500] Training Loss: 1.1160380244255066
[1/3, 2020/7500] Training Loss: 1.132191663980484
[1/3, 2030/7500] Training Loss: 1.1680131733417511
[1/3, 2040/7500] Training Loss: 1.1594568371772767
[1/3, 2050/7500] Training Loss: 1.1565382957458497
[1/3, 2060/7500] Training Loss: 1.05739883184433
[1/3, 2070/7500] Training Loss: 1.1226728320121766
[1/3, 2080/7500] Training Loss: 1.1308179378509522
[1/3, 2090/7500] Training Loss: 1.0873873353004455
[1/3, 2100/7500] Training Loss: 1.155728965997696
[1/3, 2110/7500] Training Loss: 1.1649212956428527
[1/3, 2120/7500] Training Loss: 1.2243168830871582
[1/3, 2130/7500] Training Loss: 1.1232014894485474
[1/3, 2140/7500] Training Loss: 1.1947210431098938
[1/3, 2150/7500] Training Loss: 1.0759006083011626
[1/3, 2160/7500] Training Loss: 1.0912765383720398
[1/3, 2170/7500] Training Loss: 1.0552861750125886
[1/3, 2180/7500] Training Loss: 1.20890052318573
[1/3, 2190/7500] Training Loss: 1.1583427906036377
[1/3, 2200/7500] Training Loss: 1.1212073743343354
[1/3, 2210/7500] Training Loss: 1.2209537744522094
[1/3, 2220/7500] Training Loss: 1.1312668323516846
[1/3, 2230/7500] Training Loss: 1.0774539768695832
[1/3, 2240/7500] Training Loss: 1.186616265773773
[1/3, 2250/7500] Training Loss: 1.157200074195862
[1/3, 2260/7500] Training Loss: 1.1326180458068849
[1/3, 2270/7500] Training Loss: 1.1316760182380676
[1/3, 2280/7500] Training Loss: 1.109432762861252
[1/3, 2290/7500] Training Loss: 1.1000948905944825
[1/3, 2300/7500] Training Loss: 1.1631241142749786
[1/3, 2310/7500] Training Loss: 1.0770754158496856
[1/3, 2320/7500] Training Loss: 1.1844263195991516
[1/3, 2330/7500] Training Loss: 1.0589058578014374
[1/3, 2340/7500] Training Loss: 1.085068166255951
[1/3, 2350/7500] Training Loss: 1.1789627134799958
[1/3, 2360/7500] Training Loss: 1.1054278016090393
[1/3, 2370/7500] Training Loss: 1.125911146402359
[1/3, 2380/7500] Training Loss: 1.0542317390441895
[1/3, 2390/7500] Training Loss: 1.1432158589363097
[1/3, 2400/7500] Training Loss: 1.0880481123924255
[1/3, 2410/7500] Training Loss: 1.1629645109176636
[1/3, 2420/7500] Training Loss: 1.1137584924697876
[1/3, 2430/7500] Training Loss: 1.186982774734497
[1/3, 2440/7500] Training Loss: 1.0823456168174743
[1/3, 2450/7500] Training Loss: 1.110818088054657
[1/3, 2460/7500] Training Loss: 1.0595921218395232
[1/3, 2470/7500] Training Loss: 1.095146155357361
[1/3, 2480/7500] Training Loss: 1.0898441612720489
[1/3, 2490/7500] Training Loss: 1.0961717903614043
[1/3, 2500/7500] Training Loss: 1.1058541238307953
[1/3, 2510/7500] Training Loss: 1.0884039163589478
[1/3, 2520/7500] Training Loss: 1.1281537115573883
[1/3, 2530/7500] Training Loss: 1.1084017157554626
[1/3, 2540/7500] Training Loss: 1.1352219343185426
[1/3, 2550/7500] Training Loss: 1.1276317536830902
[1/3, 2560/7500] Training Loss: 1.1006650745868682
[1/3, 2570/7500] Training Loss: 1.1280167877674103
[1/3, 2580/7500] Training Loss: 1.10569885969162
[1/3, 2590/7500] Training Loss: 1.1097339808940887
[1/3, 2600/7500] Training Loss: 1.0718712508678436
[1/3, 2610/7500] Training Loss: 1.057384192943573
[1/3, 2620/7500] Training Loss: 1.1500098168849946
[1/3, 2630/7500] Training Loss: 1.0827250242233277
[1/3, 2640/7500] Training Loss: 1.1569669127464295
[1/3, 2650/7500] Training Loss: 1.1600383877754212
[1/3, 2660/7500] Training Loss: 1.11401207447052
[1/3, 2670/7500] Training Loss: 1.039646679162979
[1/3, 2680/7500] Training Loss: 1.1002132177352906
[1/3, 2690/7500] Training Loss: 1.191331672668457
[1/3, 2700/7500] Training Loss: 1.195138895511627
[1/3, 2710/7500] Training Loss: 1.1373944759368897
[1/3, 2720/7500] Training Loss: 1.0819183051586152
[1/3, 2730/7500] Training Loss: 1.065765380859375
[1/3, 2740/7500] Training Loss: 1.0792905926704406
[1/3, 2750/7500] Training Loss: 1.1169896304607392
[1/3, 2760/7500] Training Loss: 1.0769270360469818
[1/3, 2770/7500] Training Loss: 1.0286558866500854
[1/3, 2780/7500] Training Loss: 1.1181472837924957
[1/3, 2790/7500] Training Loss: 1.1587548017501832
[1/3, 2800/7500] Training Loss: 1.1340889573097228
[1/3, 2810/7500] Training Loss: 1.1430247008800507
[1/3, 2820/7500] Training Loss: 1.1365496218204498
[1/3, 2830/7500] Training Loss: 1.1315596640110015
[1/3, 2840/7500] Training Loss: 1.0696508646011353
[1/3, 2850/7500] Training Loss: 1.1383156895637512
[1/3, 2860/7500] Training Loss: 1.1479063987731934
[1/3, 2870/7500] Training Loss: 1.1393108546733857
[1/3, 2880/7500] Training Loss: 1.1456987679004669
[1/3, 2890/7500] Training Loss: 1.0995757102966308
[1/3, 2900/7500] Training Loss: 1.1565868556499481
[1/3, 2910/7500] Training Loss: 1.108766210079193
[1/3, 2920/7500] Training Loss: 1.138530194759369
[1/3, 2930/7500] Training Loss: 1.0672721862792969
[1/3, 2940/7500] Training Loss: 1.0581318795681
[1/3, 2950/7500] Training Loss: 1.1715581297874451
[1/3, 2960/7500] Training Loss: 1.1355385541915894
[1/3, 2970/7500] Training Loss: 1.1793892085552216
[1/3, 2980/7500] Training Loss: 1.0649047493934631
[1/3, 2990/7500] Training Loss: 1.0934809863567352
[1/3, 3000/7500] Training Loss: 1.0964787304401398
[1/3, 3010/7500] Training Loss: 1.1466073274612427
[1/3, 3020/7500] Training Loss: 1.1578844845294953
[1/3, 3030/7500] Training Loss: 1.172282838821411
[1/3, 3040/7500] Training Loss: 1.1137726664543153
[1/3, 3050/7500] Training Loss: 1.0805277049541473
[1/3, 3060/7500] Training Loss: 1.0883444964885711
[1/3, 3070/7500] Training Loss: 1.0893389225006103
[1/3, 3080/7500] Training Loss: 1.0813331842422484
[1/3, 3090/7500] Training Loss: 1.1276923298835755
[1/3, 3100/7500] Training Loss: 1.1159212112426757
[1/3, 3110/7500] Training Loss: 1.0977241098880768
[1/3, 3120/7500] Training Loss: 1.1324122071266174
[1/3, 3130/7500] Training Loss: 1.0942198038101196
[1/3, 3140/7500] Training Loss: 1.1233651995658875
[1/3, 3150/7500] Training Loss: 1.125027734041214
[1/3, 3160/7500] Training Loss: 1.140872496366501
[1/3, 3170/7500] Training Loss: 1.0670259058475495
[1/3, 3180/7500] Training Loss: 1.1161161482334137
[1/3, 3190/7500] Training Loss: 1.1816294312477111
[1/3, 3200/7500] Training Loss: 1.113679152727127
[1/3, 3210/7500] Training Loss: 1.0786076068878174
[1/3, 3220/7500] Training Loss: 1.1290935218334197
[1/3, 3230/7500] Training Loss: 1.1957216739654541
[1/3, 3240/7500] Training Loss: 1.0780308723449707
[1/3, 3250/7500] Training Loss: 1.0429921269416809
[1/3, 3260/7500] Training Loss: 1.0985847890377045
[1/3, 3270/7500] Training Loss: 1.0475396811962128
[1/3, 3280/7500] Training Loss: 1.112036156654358
[1/3, 3290/7500] Training Loss: 1.128507876396179
[1/3, 3300/7500] Training Loss: 1.1392251133918763
[1/3, 3310/7500] Training Loss: 1.1689411640167235
[1/3, 3320/7500] Training Loss: 1.1638400852680206
[1/3, 3330/7500] Training Loss: 1.15602787733078
[1/3, 3340/7500] Training Loss: 1.0930541098117827
[1/3, 3350/7500] Training Loss: 1.0948704183101654
[1/3, 3360/7500] Training Loss: 1.1598206460475922
[1/3, 3370/7500] Training Loss: 1.1004274785518646
[1/3, 3380/7500] Training Loss: 1.1587244749069214
[1/3, 3390/7500] Training Loss: 1.1154931604862213
[1/3, 3400/7500] Training Loss: 1.1381296992301941
[1/3, 3410/7500] Training Loss: 1.1540088653564453
[1/3, 3420/7500] Training Loss: 1.036367952823639
[1/3, 3430/7500] Training Loss: 1.10698744058609
[1/3, 3440/7500] Training Loss: 1.1368421256542205
[1/3, 3450/7500] Training Loss: 1.0819801032543181
[1/3, 3460/7500] Training Loss: 1.092830228805542
[1/3, 3470/7500] Training Loss: 1.1108547508716584
[1/3, 3480/7500] Training Loss: 1.0949736058712005
[1/3, 3490/7500] Training Loss: 1.1521166622638703
[1/3, 3500/7500] Training Loss: 1.1315947353839875
[1/3, 3510/7500] Training Loss: 1.0670955777168274
[1/3, 3520/7500] Training Loss: 1.141265046596527
[1/3, 3530/7500] Training Loss: 1.0286926984786988
[1/3, 3540/7500] Training Loss: 1.1156722784042359
[1/3, 3550/7500] Training Loss: 1.0820972263813018
[1/3, 3560/7500] Training Loss: 1.0678824841976167
[1/3, 3570/7500] Training Loss: 1.0843151450157165
[1/3, 3580/7500] Training Loss: 1.0982435584068297
[1/3, 3590/7500] Training Loss: 1.0902385115623474
[1/3, 3600/7500] Training Loss: 1.007592350244522
[1/3, 3610/7500] Training Loss: 1.1129660904407501
[1/3, 3620/7500] Training Loss: 1.1455325722694396
[1/3, 3630/7500] Training Loss: 1.0425935089588165
[1/3, 3640/7500] Training Loss: 1.1161638736724853
[1/3, 3650/7500] Training Loss: 1.0871776163578033
[1/3, 3660/7500] Training Loss: 1.1456805169582367
[1/3, 3670/7500] Training Loss: 1.0617596328258514
[1/3, 3680/7500] Training Loss: 1.172083741426468
[1/3, 3690/7500] Training Loss: 1.1593645811080933
[1/3, 3700/7500] Training Loss: 1.1678162574768067
[1/3, 3710/7500] Training Loss: 1.0305803835391998
[1/3, 3720/7500] Training Loss: 1.0581212639808655
[1/3, 3730/7500] Training Loss: 1.1359005331993104
[1/3, 3740/7500] Training Loss: 1.0748189091682434
[1/3, 3750/7500] Training Loss: 1.056027901172638
[1/3, 3760/7500] Training Loss: 1.0195420265197754
[1/3, 3770/7500] Training Loss: 1.0618718028068543
[1/3, 3780/7500] Training Loss: 1.077469402551651
[1/3, 3790/7500] Training Loss: 1.0923978984355927
[1/3, 3800/7500] Training Loss: 1.1768942952156067
[1/3, 3810/7500] Training Loss: 1.0561988413333894
[1/3, 3820/7500] Training Loss: 1.1128486096858978
[1/3, 3830/7500] Training Loss: 1.0900743067264558
[1/3, 3840/7500] Training Loss: 1.1231581747531891
[1/3, 3850/7500] Training Loss: 1.1178593993186952
[1/3, 3860/7500] Training Loss: 1.0776888012886048
[1/3, 3870/7500] Training Loss: 1.1510751962661743
[1/3, 3880/7500] Training Loss: 1.1005356669425965
[1/3, 3890/7500] Training Loss: 1.1328916013240815
[1/3, 3900/7500] Training Loss: 1.0504380702972411
[1/3, 3910/7500] Training Loss: 1.087610113620758
[1/3, 3920/7500] Training Loss: 1.0701151132583617
[1/3, 3930/7500] Training Loss: 1.1116098880767822
[1/3, 3940/7500] Training Loss: 1.1406829595565795
[1/3, 3950/7500] Training Loss: 1.0633830726146698
[1/3, 3960/7500] Training Loss: 1.1522023797035217
[1/3, 3970/7500] Training Loss: 1.091694062948227
[1/3, 3980/7500] Training Loss: 1.0846245884895325
[1/3, 3990/7500] Training Loss: 1.1395062625408172
[1/3, 4000/7500] Training Loss: 1.0773526430130005
[1/3, 4010/7500] Training Loss: 1.1473379611968995
[1/3, 4020/7500] Training Loss: 1.1290396332740784
[1/3, 4030/7500] Training Loss: 1.1246082365512848
[1/3, 4040/7500] Training Loss: 1.1192918956279754
[1/3, 4050/7500] Training Loss: 1.1307824313640595
[1/3, 4060/7500] Training Loss: 1.0837205529212952
[1/3, 4070/7500] Training Loss: 1.0905476629734039
[1/3, 4080/7500] Training Loss: 1.078506553173065
[1/3, 4090/7500] Training Loss: 1.1348104298114776
[1/3, 4100/7500] Training Loss: 1.0140413641929626
[1/3, 4110/7500] Training Loss: 1.0083079397678376
[1/3, 4120/7500] Training Loss: 1.1235544323921203
[1/3, 4130/7500] Training Loss: 1.097773504257202
[1/3, 4140/7500] Training Loss: 1.1032843887805939
[1/3, 4150/7500] Training Loss: 1.108769601583481
[1/3, 4160/7500] Training Loss: 1.1185945451259613
[1/3, 4170/7500] Training Loss: 1.0933183014392853
[1/3, 4180/7500] Training Loss: 1.0353238761425019
[1/3, 4190/7500] Training Loss: 1.0607977271080018
[1/3, 4200/7500] Training Loss: 1.0613139688968658
[1/3, 4210/7500] Training Loss: 1.1896628260612487
[1/3, 4220/7500] Training Loss: 1.0942441463470458
[1/3, 4230/7500] Training Loss: 1.07392498254776
[1/3, 4240/7500] Training Loss: 1.0457506895065307
[1/3, 4250/7500] Training Loss: 1.0708655655384063
[1/3, 4260/7500] Training Loss: 1.0624196112155915
[1/3, 4270/7500] Training Loss: 1.0890737414360045
[1/3, 4280/7500] Training Loss: 1.1234067738056184
[1/3, 4290/7500] Training Loss: 1.1025790095329284
[1/3, 4300/7500] Training Loss: 1.1105079054832458
[1/3, 4310/7500] Training Loss: 1.1606442093849183
[1/3, 4320/7500] Training Loss: 1.1252393662929534
[1/3, 4330/7500] Training Loss: 1.0565777182579041
[1/3, 4340/7500] Training Loss: 1.0259822607040405
[1/3, 4350/7500] Training Loss: 1.0995198845863343
[1/3, 4360/7500] Training Loss: 1.1142369508743286
[1/3, 4370/7500] Training Loss: 1.079351407289505
[1/3, 4380/7500] Training Loss: 1.1267750084400177
[1/3, 4390/7500] Training Loss: 1.1568740606307983
[1/3, 4400/7500] Training Loss: 1.136344850063324
[1/3, 4410/7500] Training Loss: 1.0672483563423156
[1/3, 4420/7500] Training Loss: 1.0625240445137023
[1/3, 4430/7500] Training Loss: 1.1134928822517396
[1/3, 4440/7500] Training Loss: 1.0408739030361176
[1/3, 4450/7500] Training Loss: 1.1971579432487487
[1/3, 4460/7500] Training Loss: 1.0572656214237213
[1/3, 4470/7500] Training Loss: 1.121835571527481
[1/3, 4480/7500] Training Loss: 1.090104526281357
[1/3, 4490/7500] Training Loss: 1.1464572906494142
[1/3, 4500/7500] Training Loss: 1.0405906081199645
[1/3, 4510/7500] Training Loss: 1.0207603096961975
[1/3, 4520/7500] Training Loss: 1.1347816467285157
[1/3, 4530/7500] Training Loss: 1.0728516340255738
[1/3, 4540/7500] Training Loss: 1.082862001657486
[1/3, 4550/7500] Training Loss: 1.0808077931404114
[1/3, 4560/7500] Training Loss: 1.11650670170784
[1/3, 4570/7500] Training Loss: 1.0859792828559875
[1/3, 4580/7500] Training Loss: 1.1270873546600342
[1/3, 4590/7500] Training Loss: 1.0796252965927124
[1/3, 4600/7500] Training Loss: 1.1064152359962462
[1/3, 4610/7500] Training Loss: 1.0532720506191253
[1/3, 4620/7500] Training Loss: 1.0885704696178435
[1/3, 4630/7500] Training Loss: 1.0546666204929351
[1/3, 4640/7500] Training Loss: 1.0730834603309631
[1/3, 4650/7500] Training Loss: 1.0674355506896973
[1/3, 4660/7500] Training Loss: 1.0931851327419282
[1/3, 4670/7500] Training Loss: 1.1056237816810608
[1/3, 4680/7500] Training Loss: 1.131964385509491
[1/3, 4690/7500] Training Loss: 1.1038372695446015
[1/3, 4700/7500] Training Loss: 1.123378735780716
[1/3, 4710/7500] Training Loss: 1.1387868046760559
[1/3, 4720/7500] Training Loss: 1.0753103077411652
[1/3, 4730/7500] Training Loss: 1.168422567844391
[1/3, 4740/7500] Training Loss: 1.1056377053260804
[1/3, 4750/7500] Training Loss: 1.1208616018295288
[1/3, 4760/7500] Training Loss: 1.1089660108089447
[1/3, 4770/7500] Training Loss: 1.0890225529670716
[1/3, 4780/7500] Training Loss: 1.069637769460678
[1/3, 4790/7500] Training Loss: 1.100255411863327
[1/3, 4800/7500] Training Loss: 1.1094577074050904
[1/3, 4810/7500] Training Loss: 1.1423728346824646
[1/3, 4820/7500] Training Loss: 1.1191002786159516
[1/3, 4830/7500] Training Loss: 1.1967925429344177
[1/3, 4840/7500] Training Loss: 1.032316541671753
[1/3, 4850/7500] Training Loss: 1.07113037109375
[1/3, 4860/7500] Training Loss: 1.071698546409607
[1/3, 4870/7500] Training Loss: 1.0980714678764343
[1/3, 4880/7500] Training Loss: 1.0809655010700225
[1/3, 4890/7500] Training Loss: 1.0868658363819121
[1/3, 4900/7500] Training Loss: 1.04447660446167
[1/3, 4910/7500] Training Loss: 1.1087700366973876
[1/3, 4920/7500] Training Loss: 1.1209090590476989
[1/3, 4930/7500] Training Loss: 1.16549956202507
[1/3, 4940/7500] Training Loss: 1.1290745258331298
[1/3, 4950/7500] Training Loss: 1.0609337449073792
[1/3, 4960/7500] Training Loss: 1.008740133047104
[1/3, 4970/7500] Training Loss: 1.1719727635383606
[1/3, 4980/7500] Training Loss: 1.1079434275627136
[1/3, 4990/7500] Training Loss: 1.0398576140403748
[1/3, 5000/7500] Training Loss: 1.0532702505588531
[1/3, 5010/7500] Training Loss: 1.1446985483169556
[1/3, 5020/7500] Training Loss: 1.1115803003311158
[1/3, 5030/7500] Training Loss: 1.084354931116104
[1/3, 5040/7500] Training Loss: 1.108128160238266
[1/3, 5050/7500] Training Loss: 1.1538013696670533
[1/3, 5060/7500] Training Loss: 1.0476355135440827
[1/3, 5070/7500] Training Loss: 1.1183185935020448
[1/3, 5080/7500] Training Loss: 1.1553026199340821
[1/3, 5090/7500] Training Loss: 1.1728513419628144
[1/3, 5100/7500] Training Loss: 1.0787180185317993
[1/3, 5110/7500] Training Loss: 1.108946841955185
[1/3, 5120/7500] Training Loss: 1.119533109664917
[1/3, 5130/7500] Training Loss: 1.104043072462082
[1/3, 5140/7500] Training Loss: 1.029046082496643
[1/3, 5150/7500] Training Loss: 1.0885014533996582
[1/3, 5160/7500] Training Loss: 1.110771769285202
[1/3, 5170/7500] Training Loss: 1.04362610578537
[1/3, 5180/7500] Training Loss: 1.041104805469513
[1/3, 5190/7500] Training Loss: 1.06178275346756
[1/3, 5200/7500] Training Loss: 1.092924702167511
[1/3, 5210/7500] Training Loss: 1.1233466386795044
[1/3, 5220/7500] Training Loss: 1.0781859159469604
[1/3, 5230/7500] Training Loss: 1.107185173034668
[1/3, 5240/7500] Training Loss: 1.169247841835022
[1/3, 5250/7500] Training Loss: 1.0702430427074432
[1/3, 5260/7500] Training Loss: 1.0798118948936462
[1/3, 5270/7500] Training Loss: 1.151256024837494
[1/3, 5280/7500] Training Loss: 1.1154924869537353
[1/3, 5290/7500] Training Loss: 1.1127541065216064
[1/3, 5300/7500] Training Loss: 1.1018684029579162
[1/3, 5310/7500] Training Loss: 1.1512381315231324
[1/3, 5320/7500] Training Loss: 1.0114621758460998
[1/3, 5330/7500] Training Loss: 1.0658417463302612
[1/3, 5340/7500] Training Loss: 1.135501217842102
[1/3, 5350/7500] Training Loss: 1.1096458733081818
[1/3, 5360/7500] Training Loss: 1.0718707323074341
[1/3, 5370/7500] Training Loss: 1.0797259271144868
[1/3, 5380/7500] Training Loss: 1.0940149486064912
[1/3, 5390/7500] Training Loss: 1.1278316259384156
[1/3, 5400/7500] Training Loss: 1.1024016439914703
[1/3, 5410/7500] Training Loss: 1.0727223098278045
[1/3, 5420/7500] Training Loss: 1.040151733160019
[1/3, 5430/7500] Training Loss: 1.1084128677845002
[1/3, 5440/7500] Training Loss: 1.0813275516033172
[1/3, 5450/7500] Training Loss: 1.0228323221206665
[1/3, 5460/7500] Training Loss: 1.085249525308609
[1/3, 5470/7500] Training Loss: 1.1354822397232056
[1/3, 5480/7500] Training Loss: 1.0728200614452361
[1/3, 5490/7500] Training Loss: 1.1352818310260773
[1/3, 5500/7500] Training Loss: 1.0816950380802155
[1/3, 5510/7500] Training Loss: 1.0831533193588256
[1/3, 5520/7500] Training Loss: 1.0664857804775238
[1/3, 5530/7500] Training Loss: 1.0912575423717499
[1/3, 5540/7500] Training Loss: 1.1264041900634765
[1/3, 5550/7500] Training Loss: 1.115025669336319
[1/3, 5560/7500] Training Loss: 1.117527389526367
[1/3, 5570/7500] Training Loss: 1.0733913838863374
[1/3, 5580/7500] Training Loss: 1.0769591152667999
[1/3, 5590/7500] Training Loss: 1.0514554977416992
[1/3, 5600/7500] Training Loss: 1.1070038259029389
[1/3, 5610/7500] Training Loss: 1.095349383354187
[1/3, 5620/7500] Training Loss: 1.077143484354019
[1/3, 5630/7500] Training Loss: 1.058457714319229
[1/3, 5640/7500] Training Loss: 1.0488925337791444
[1/3, 5650/7500] Training Loss: 1.0863261103630066
[1/3, 5660/7500] Training Loss: 1.0861741423606872
[1/3, 5670/7500] Training Loss: 1.1312705993652343
[1/3, 5680/7500] Training Loss: 1.0646224200725556
[1/3, 5690/7500] Training Loss: 1.1213994443416595
[1/3, 5700/7500] Training Loss: 1.1223469436168672
[1/3, 5710/7500] Training Loss: 1.1507539749145508
[1/3, 5720/7500] Training Loss: 1.0978988707065582
[1/3, 5730/7500] Training Loss: 1.0701418280601502
[1/3, 5740/7500] Training Loss: 1.0543511033058166
[1/3, 5750/7500] Training Loss: 1.0843414962291718
[1/3, 5760/7500] Training Loss: 1.1299505889415742
[1/3, 5770/7500] Training Loss: 1.0854471325874329
[1/3, 5780/7500] Training Loss: 1.0647166192531585
[1/3, 5790/7500] Training Loss: 1.078642600774765
[1/3, 5800/7500] Training Loss: 1.0669893383979798
[1/3, 5810/7500] Training Loss: 1.0949187755584717
[1/3, 5820/7500] Training Loss: 1.0894113302230835
[1/3, 5830/7500] Training Loss: 1.055177527666092
[1/3, 5840/7500] Training Loss: 1.0120212197303773
[1/3, 5850/7500] Training Loss: 1.076607918739319
[1/3, 5860/7500] Training Loss: 1.1440235435962678
[1/3, 5870/7500] Training Loss: 1.109321618080139
[1/3, 5880/7500] Training Loss: 1.0828980267047883
[1/3, 5890/7500] Training Loss: 1.0602992355823517
[1/3, 5900/7500] Training Loss: 1.1330425202846528
[1/3, 5910/7500] Training Loss: 1.0251773416996002
[1/3, 5920/7500] Training Loss: 0.989063811302185
[1/3, 5930/7500] Training Loss: 1.0327730000019073
[1/3, 5940/7500] Training Loss: 1.0364758133888246
[1/3, 5950/7500] Training Loss: 1.0230387210845948
[1/3, 5960/7500] Training Loss: 1.0459143042564392
[1/3, 5970/7500] Training Loss: 1.103290432691574
[1/3, 5980/7500] Training Loss: 1.0509370803833007
[1/3, 5990/7500] Training Loss: 1.0680738806724548
[1/3, 6000/7500] Training Loss: 1.2144700169563294
[1/3, 6010/7500] Training Loss: 1.1120594143867493
[1/3, 6020/7500] Training Loss: 1.1453362703323364
[1/3, 6030/7500] Training Loss: 1.0786744952201843
[1/3, 6040/7500] Training Loss: 1.0755602657794952
[1/3, 6050/7500] Training Loss: 1.1095631837844848
[1/3, 6060/7500] Training Loss: 1.1044846296310424
[1/3, 6070/7500] Training Loss: 1.1149921715259552
[1/3, 6080/7500] Training Loss: 1.1399220049381256
[1/3, 6090/7500] Training Loss: 1.0830620348453521
[1/3, 6100/7500] Training Loss: 1.1703345775604248
[1/3, 6110/7500] Training Loss: 1.14534752368927
[1/3, 6120/7500] Training Loss: 1.0986148774623872
[1/3, 6130/7500] Training Loss: 1.1079650700092316
[1/3, 6140/7500] Training Loss: 1.0108850717544555
[1/3, 6150/7500] Training Loss: 1.080043476819992
[1/3, 6160/7500] Training Loss: 1.1181477904319763
[1/3, 6170/7500] Training Loss: 1.0032387554645539
[1/3, 6180/7500] Training Loss: 1.1199997782707214
[1/3, 6190/7500] Training Loss: 1.037716370820999
[1/3, 6200/7500] Training Loss: 1.0770369470119476
[1/3, 6210/7500] Training Loss: 1.1110286235809326
[1/3, 6220/7500] Training Loss: 1.103629821538925
[1/3, 6230/7500] Training Loss: 1.132332968711853
[1/3, 6240/7500] Training Loss: 1.0801430881023406
[1/3, 6250/7500] Training Loss: 1.1089405417442322
[1/3, 6260/7500] Training Loss: 1.0159861147403717
[1/3, 6270/7500] Training Loss: 1.150638258457184
[1/3, 6280/7500] Training Loss: 1.1372036933898926
[1/3, 6290/7500] Training Loss: 1.1577872276306151
[1/3, 6300/7500] Training Loss: 1.234813678264618
[1/3, 6310/7500] Training Loss: 1.1446331143379211
[1/3, 6320/7500] Training Loss: 1.0256262481212617
[1/3, 6330/7500] Training Loss: 1.1317592680454254
[1/3, 6340/7500] Training Loss: 1.1056565403938294
[1/3, 6350/7500] Training Loss: 1.073658949136734
[1/3, 6360/7500] Training Loss: 1.1470957279205323
[1/3, 6370/7500] Training Loss: 1.1510085105895995
[1/3, 6380/7500] Training Loss: 1.193067693710327
[1/3, 6390/7500] Training Loss: 1.191252100467682
[1/3, 6400/7500] Training Loss: 1.1062114477157592
[1/3, 6410/7500] Training Loss: 1.165097999572754
[1/3, 6420/7500] Training Loss: 1.044837635755539
[1/3, 6430/7500] Training Loss: 1.0879809498786925
[1/3, 6440/7500] Training Loss: 1.116526597738266
[1/3, 6450/7500] Training Loss: 1.072564160823822
[1/3, 6460/7500] Training Loss: 1.1643507301807403
[1/3, 6470/7500] Training Loss: 1.180737155675888
[1/3, 6480/7500] Training Loss: 1.1056278109550477
[1/3, 6490/7500] Training Loss: 1.0740397155284882
[1/3, 6500/7500] Training Loss: 1.060782504081726
[1/3, 6510/7500] Training Loss: 1.1151266992092133
[1/3, 6520/7500] Training Loss: 1.0950448989868165
[1/3, 6530/7500] Training Loss: 1.1893183827400207
[1/3, 6540/7500] Training Loss: 1.012185925245285
[1/3, 6550/7500] Training Loss: 1.0936106026172638
[1/3, 6560/7500] Training Loss: 1.1342150747776032
[1/3, 6570/7500] Training Loss: 1.1521417856216432
[1/3, 6580/7500] Training Loss: 1.101082444190979
[1/3, 6590/7500] Training Loss: 1.1348556697368621
[1/3, 6600/7500] Training Loss: 1.0766037821769714
[1/3, 6610/7500] Training Loss: 1.108646321296692
[1/3, 6620/7500] Training Loss: 1.104043436050415
[1/3, 6630/7500] Training Loss: 1.176779818534851
[1/3, 6640/7500] Training Loss: 1.1141895413398744
[1/3, 6650/7500] Training Loss: 1.7073352217674256
[1/3, 6660/7500] Training Loss: 1.2491023302078248
[1/3, 6670/7500] Training Loss: 1.1759417235851288
[1/3, 6680/7500] Training Loss: 1.2269871234893799
[1/3, 6690/7500] Training Loss: 1.265917420387268
[1/3, 6700/7500] Training Loss: 1.2384413480758667
[1/3, 6710/7500] Training Loss: 1.1752153515815735
[1/3, 6720/7500] Training Loss: 1.2672519207000732
[1/3, 6730/7500] Training Loss: 1.1716147661209106
[1/3, 6740/7500] Training Loss: 1.2262854337692262
[1/3, 6750/7500] Training Loss: 1.195792317390442
[1/3, 6760/7500] Training Loss: 1.2943718552589416
[1/3, 6770/7500] Training Loss: 1.2222939014434815
[1/3, 6780/7500] Training Loss: 1.2341284990310668
[1/3, 6790/7500] Training Loss: 1.2296351671218873
[1/3, 6800/7500] Training Loss: 1.2256805181503296
[1/3, 6810/7500] Training Loss: 1.1907261729240417
[1/3, 6820/7500] Training Loss: 1.213840627670288
[1/3, 6830/7500] Training Loss: 1.2456741809844971
[1/3, 6840/7500] Training Loss: 1.189825141429901
[1/3, 6850/7500] Training Loss: 1.2741538524627685
[1/3, 6860/7500] Training Loss: 1.272542554140091
[1/3, 6870/7500] Training Loss: 1.2347570061683655
[1/3, 6880/7500] Training Loss: 1.2166454434394836
[1/3, 6890/7500] Training Loss: 1.2180297732353211
[1/3, 6900/7500] Training Loss: 1.1813479363918304
[1/3, 6910/7500] Training Loss: 1.1602298498153687
[1/3, 6920/7500] Training Loss: 1.2247626066207886
[1/3, 6930/7500] Training Loss: 1.2347742080688477
[1/3, 6940/7500] Training Loss: 1.2268202483654023
[1/3, 6950/7500] Training Loss: 1.265664529800415
[1/3, 6960/7500] Training Loss: 1.2175143957138062
[1/3, 6970/7500] Training Loss: 1.2931751608848572
[1/3, 6980/7500] Training Loss: 1.2103772699832915
[1/3, 6990/7500] Training Loss: 1.243599134683609
[1/3, 7000/7500] Training Loss: 1.2550630569458008
[1/3, 7010/7500] Training Loss: 1.2061683535575867
[1/3, 7020/7500] Training Loss: 1.2274955332279205
[1/3, 7030/7500] Training Loss: 1.1415587961673737
[1/3, 7040/7500] Training Loss: 1.2191317915916442
[1/3, 7050/7500] Training Loss: 1.1597502827644348
[1/3, 7060/7500] Training Loss: 1.2349907994270324
[1/3, 7070/7500] Training Loss: 1.2279091358184815
[1/3, 7080/7500] Training Loss: 1.2315434336662292
[1/3, 7090/7500] Training Loss: 1.252732479572296
[1/3, 7100/7500] Training Loss: 1.2068551063537598
[1/3, 7110/7500] Training Loss: 1.2375366866588593
[1/3, 7120/7500] Training Loss: 1.1818873286247253
[1/3, 7130/7500] Training Loss: 1.296922218799591
[1/3, 7140/7500] Training Loss: 1.3154733300209045
[1/3, 7150/7500] Training Loss: 1.240648090839386
[1/3, 7160/7500] Training Loss: 1.2748218059539795
[1/3, 7170/7500] Training Loss: 1.22443630695343
[1/3, 7180/7500] Training Loss: 1.1568947315216065
[1/3, 7190/7500] Training Loss: 1.203596556186676
[1/3, 7200/7500] Training Loss: 1.2412037014961244
[1/3, 7210/7500] Training Loss: 1.229841685295105
[1/3, 7220/7500] Training Loss: 1.1967466950416565
[1/3, 7230/7500] Training Loss: 1.242833387851715
[1/3, 7240/7500] Training Loss: 1.24931857585907
[1/3, 7250/7500] Training Loss: 1.2123178124427796
[1/3, 7260/7500] Training Loss: 1.256357443332672
[1/3, 7270/7500] Training Loss: 1.2169685065746307
[1/3, 7280/7500] Training Loss: 1.2926211714744569
[1/3, 7290/7500] Training Loss: 1.168549507856369
[1/3, 7300/7500] Training Loss: 1.250664508342743
[1/3, 7310/7500] Training Loss: 1.2990602135658265
[1/3, 7320/7500] Training Loss: 1.2273199915885926
[1/3, 7330/7500] Training Loss: 1.267521119117737
[1/3, 7340/7500] Training Loss: 1.18925598859787
[1/3, 7350/7500] Training Loss: 1.1402981996536254
[1/3, 7360/7500] Training Loss: 1.258537197113037
[1/3, 7370/7500] Training Loss: 1.214300310611725
[1/3, 7380/7500] Training Loss: 1.2603466868400575
[1/3, 7390/7500] Training Loss: 1.2521701097488402
[1/3, 7400/7500] Training Loss: 1.147853922843933
[1/3, 7410/7500] Training Loss: 1.1737961232662202
[1/3, 7420/7500] Training Loss: 1.2541560292243958
[1/3, 7430/7500] Training Loss: 1.2267526865005494
[1/3, 7440/7500] Training Loss: 1.1864862322807312
[1/3, 7450/7500] Training Loss: 1.2741065859794616
[1/3, 7460/7500] Training Loss: 1.2798266649246215
[1/3, 7470/7500] Training Loss: 1.2128717601299286
[1/3, 7480/7500] Training Loss: 1.1864152312278748
[1/3, 7490/7500] Training Loss: 1.2439912796020507
[1/3, 7500/7500] Training Loss: 1.204070895910263
5/5: Testing network...
Testing Loss: 1.2380939183712005
5/5: Training network...
[2/3, 10/7500] Training Loss: 1.294034969806671
[2/3, 20/7500] Training Loss: 1.2218443036079407
[2/3, 30/7500] Training Loss: 1.1573896706104279
[2/3, 40/7500] Training Loss: 1.1885567605495453
[2/3, 50/7500] Training Loss: 1.1844195783138276
[2/3, 60/7500] Training Loss: 1.2880112767219543
[2/3, 70/7500] Training Loss: 1.3091777563095093
[2/3, 80/7500] Training Loss: 1.2541804909706116
[2/3, 90/7500] Training Loss: 1.2243997216224671
[2/3, 100/7500] Training Loss: 1.2934042692184449
[2/3, 110/7500] Training Loss: 1.2667571306228638
[2/3, 120/7500] Training Loss: 1.2385743021965028
[2/3, 130/7500] Training Loss: 1.2258681416511537
[2/3, 140/7500] Training Loss: 1.177921998500824
[2/3, 150/7500] Training Loss: 1.2752267599105835
[2/3, 160/7500] Training Loss: 1.1905830562114716
[2/3, 170/7500] Training Loss: 1.2340939164161682
[2/3, 180/7500] Training Loss: 1.2358801007270812
[2/3, 190/7500] Training Loss: 1.254344493150711
[2/3, 200/7500] Training Loss: 1.1317765235900878
[2/3, 210/7500] Training Loss: 1.2225315928459168
[2/3, 220/7500] Training Loss: 1.2818879246711732
[2/3, 230/7500] Training Loss: 1.1863598346710205
[2/3, 240/7500] Training Loss: 1.1777979552745819
[2/3, 250/7500] Training Loss: 1.2684996008872986
[2/3, 260/7500] Training Loss: 1.236772322654724
[2/3, 270/7500] Training Loss: 1.2240641713142395
[2/3, 280/7500] Training Loss: 1.2270438015460967
[2/3, 290/7500] Training Loss: 1.2484685897827148
[2/3, 300/7500] Training Loss: 1.18205224275589
[2/3, 310/7500] Training Loss: 1.227688717842102
[2/3, 320/7500] Training Loss: 1.1647323787212371
[2/3, 330/7500] Training Loss: 1.2189449429512025
[2/3, 340/7500] Training Loss: 1.2247505187988281
[2/3, 350/7500] Training Loss: 1.2734010815620422
[2/3, 360/7500] Training Loss: 1.2457199692726135
[2/3, 370/7500] Training Loss: 1.2681801915168762
[2/3, 380/7500] Training Loss: 1.1971961319446565
[2/3, 390/7500] Training Loss: 1.262625551223755
[2/3, 400/7500] Training Loss: 1.2432870030403138
[2/3, 410/7500] Training Loss: 1.256736969947815
[2/3, 420/7500] Training Loss: 1.229108214378357
[2/3, 430/7500] Training Loss: 1.180929708480835
[2/3, 440/7500] Training Loss: 1.2426360845565796
[2/3, 450/7500] Training Loss: 1.2691243052482606
[2/3, 460/7500] Training Loss: 1.2175953269004822
[2/3, 470/7500] Training Loss: 1.2722874999046325
[2/3, 480/7500] Training Loss: 1.2553983330726624
[2/3, 490/7500] Training Loss: 1.1910160899162292
[2/3, 500/7500] Training Loss: 1.2768295049667358
[2/3, 510/7500] Training Loss: 1.1460278630256653
[2/3, 520/7500] Training Loss: 1.2071094274520875
[2/3, 530/7500] Training Loss: 1.2539759397506713
[2/3, 540/7500] Training Loss: 1.137980341911316
[2/3, 550/7500] Training Loss: 1.158122330904007
[2/3, 560/7500] Training Loss: 1.213292169570923
[2/3, 570/7500] Training Loss: 1.1720875382423401
[2/3, 580/7500] Training Loss: 1.1599693715572357
[2/3, 590/7500] Training Loss: 1.1967565298080445
[2/3, 600/7500] Training Loss: 1.2014255702495575
[2/3, 610/7500] Training Loss: 1.2104775786399842
[2/3, 620/7500] Training Loss: 1.2299249172210693
[2/3, 630/7500] Training Loss: 1.2625107884407043
[2/3, 640/7500] Training Loss: 1.201498234272003
[2/3, 650/7500] Training Loss: 1.1683897137641908
[2/3, 660/7500] Training Loss: 1.2053256630897522
[2/3, 670/7500] Training Loss: 1.2071430921554565
[2/3, 680/7500] Training Loss: 1.2234896063804626
[2/3, 690/7500] Training Loss: 1.3044390678405762
[2/3, 700/7500] Training Loss: 1.2433778047561646
[2/3, 710/7500] Training Loss: 1.1775828719139099
[2/3, 720/7500] Training Loss: 1.2059468507766724
[2/3, 730/7500] Training Loss: 1.233650517463684
[2/3, 740/7500] Training Loss: 1.149867683649063
[2/3, 750/7500] Training Loss: 1.2420608758926392
[2/3, 760/7500] Training Loss: 1.1803111553192138
[2/3, 770/7500] Training Loss: 1.2295654177665711
[2/3, 780/7500] Training Loss: 1.1764791905879974
[2/3, 790/7500] Training Loss: 1.1349911868572236
[2/3, 800/7500] Training Loss: 1.2281006097793579
[2/3, 810/7500] Training Loss: 1.2535626888275146
[2/3, 820/7500] Training Loss: 1.2549338340759277
[2/3, 830/7500] Training Loss: 1.238801121711731
[2/3, 840/7500] Training Loss: 1.2416888117790221
[2/3, 850/7500] Training Loss: 1.2577285289764404
[2/3, 860/7500] Training Loss: 1.2519639611244202
[2/3, 870/7500] Training Loss: 1.1275075852870942
[2/3, 880/7500] Training Loss: 1.2380317568778991
[2/3, 890/7500] Training Loss: 1.277418279647827
[2/3, 900/7500] Training Loss: 1.223134696483612
[2/3, 910/7500] Training Loss: 1.2887410283088685
[2/3, 920/7500] Training Loss: 1.2060127317905427
[2/3, 930/7500] Training Loss: 1.2540868043899536
[2/3, 940/7500] Training Loss: 1.222948396205902
[2/3, 950/7500] Training Loss: 1.1977562189102173
[2/3, 960/7500] Training Loss: 1.2285616040229796
[2/3, 970/7500] Training Loss: 1.1897247433662415
[2/3, 980/7500] Training Loss: 1.2034187138080596
[2/3, 990/7500] Training Loss: 1.2871682405471803
[2/3, 1000/7500] Training Loss: 1.1493852198123933
[2/3, 1010/7500] Training Loss: 1.2247170090675354
[2/3, 1020/7500] Training Loss: 1.2852121829986571
[2/3, 1030/7500] Training Loss: 1.231922972202301
[2/3, 1040/7500] Training Loss: 1.2078652679920197
[2/3, 1050/7500] Training Loss: 1.2009032011032104
[2/3, 1060/7500] Training Loss: 1.2684283971786499
[2/3, 1070/7500] Training Loss: 1.1876195609569549
[2/3, 1080/7500] Training Loss: 1.1825563311576843
[2/3, 1090/7500] Training Loss: 1.2113930940628053
[2/3, 1100/7500] Training Loss: 1.2522929310798645
[2/3, 1110/7500] Training Loss: 1.182191389799118
[2/3, 1120/7500] Training Loss: 1.2709083080291748
[2/3, 1130/7500] Training Loss: 1.2190229058265687
[2/3, 1140/7500] Training Loss: 1.3054436564445495
[2/3, 1150/7500] Training Loss: 1.215009742975235
[2/3, 1160/7500] Training Loss: 1.215195631980896
[2/3, 1170/7500] Training Loss: 1.220034110546112
[2/3, 1180/7500] Training Loss: 1.2365728855133056
[2/3, 1190/7500] Training Loss: 1.3216971516609193
[2/3, 1200/7500] Training Loss: 1.261460256576538
[2/3, 1210/7500] Training Loss: 1.1943185091018678
[2/3, 1220/7500] Training Loss: 1.2014537811279298
[2/3, 1230/7500] Training Loss: 1.2861198425292968
[2/3, 1240/7500] Training Loss: 1.266086459159851
[2/3, 1250/7500] Training Loss: 1.1999499678611756
[2/3, 1260/7500] Training Loss: 1.2426787257194518
[2/3, 1270/7500] Training Loss: 1.1890504240989686
[2/3, 1280/7500] Training Loss: 1.207468295097351
[2/3, 1290/7500] Training Loss: 1.1724642634391784
[2/3, 1300/7500] Training Loss: 1.253947949409485
[2/3, 1310/7500] Training Loss: 1.2067198038101197
[2/3, 1320/7500] Training Loss: 1.2373689770698548
[2/3, 1330/7500] Training Loss: 1.2292309403419495
[2/3, 1340/7500] Training Loss: 1.212879490852356
[2/3, 1350/7500] Training Loss: 1.2124476313591004
[2/3, 1360/7500] Training Loss: 1.2196296811103822
[2/3, 1370/7500] Training Loss: 1.2174238860607147
[2/3, 1380/7500] Training Loss: 1.1969021916389466
[2/3, 1390/7500] Training Loss: 1.2677423000335692
[2/3, 1400/7500] Training Loss: 1.1763009905815125
[2/3, 1410/7500] Training Loss: 1.2727460384368896
[2/3, 1420/7500] Training Loss: 1.2011885046958923
[2/3, 1430/7500] Training Loss: 1.2061864495277406
[2/3, 1440/7500] Training Loss: 1.2093451261520385
[2/3, 1450/7500] Training Loss: 1.2552866101264955
[2/3, 1460/7500] Training Loss: 1.2018857955932618
[2/3, 1470/7500] Training Loss: 1.254609578847885
[2/3, 1480/7500] Training Loss: 1.1898706436157227
[2/3, 1490/7500] Training Loss: 1.179100352525711
[2/3, 1500/7500] Training Loss: 1.2397937595844268
[2/3, 1510/7500] Training Loss: 1.269850206375122
[2/3, 1520/7500] Training Loss: 1.240784502029419
[2/3, 1530/7500] Training Loss: 1.2273436427116393
[2/3, 1540/7500] Training Loss: 1.250166380405426
[2/3, 1550/7500] Training Loss: 1.2064746260643004
[2/3, 1560/7500] Training Loss: 1.1956343412399293
[2/3, 1570/7500] Training Loss: 1.2087852656841278
[2/3, 1580/7500] Training Loss: 1.2235707998275758
[2/3, 1590/7500] Training Loss: 1.2102454662323
[2/3, 1600/7500] Training Loss: 1.182554167509079
[2/3, 1610/7500] Training Loss: 1.2510310769081117
[2/3, 1620/7500] Training Loss: 1.3075523018836974
[2/3, 1630/7500] Training Loss: 1.3041933536529542
[2/3, 1640/7500] Training Loss: 1.2187592387199402
[2/3, 1650/7500] Training Loss: 1.1972162663936614
[2/3, 1660/7500] Training Loss: 1.2179928779602052
[2/3, 1670/7500] Training Loss: 1.260788631439209
[2/3, 1680/7500] Training Loss: 1.2106382608413697
[2/3, 1690/7500] Training Loss: 1.189828062057495
[2/3, 1700/7500] Training Loss: 1.27077374458313
[2/3, 1710/7500] Training Loss: 1.2567183256149292
[2/3, 1720/7500] Training Loss: 1.2048385500907899
[2/3, 1730/7500] Training Loss: 1.1829649090766907
[2/3, 1740/7500] Training Loss: 1.279730224609375
[2/3, 1750/7500] Training Loss: 1.227333664894104
[2/3, 1760/7500] Training Loss: 1.2473023056983947
[2/3, 1770/7500] Training Loss: 1.1691517949104309
[2/3, 1780/7500] Training Loss: 1.1931867063045503
[2/3, 1790/7500] Training Loss: 1.1965160489082336
[2/3, 1800/7500] Training Loss: 1.1715434432029723
[2/3, 1810/7500] Training Loss: 1.1215929567813874
[2/3, 1820/7500] Training Loss: 1.3048126459121705
[2/3, 1830/7500] Training Loss: 1.246749222278595
[2/3, 1840/7500] Training Loss: 1.1764895558357238
[2/3, 1850/7500] Training Loss: 1.2464244723320008
[2/3, 1860/7500] Training Loss: 1.241148090362549
[2/3, 1870/7500] Training Loss: 1.1271963655948638
[2/3, 1880/7500] Training Loss: 1.2071043491363525
[2/3, 1890/7500] Training Loss: 1.2425684869289397
[2/3, 1900/7500] Training Loss: 1.1861010909080505
[2/3, 1910/7500] Training Loss: 1.3018830180168153
[2/3, 1920/7500] Training Loss: 1.2757522344589234
[2/3, 1930/7500] Training Loss: 1.266666841506958
[2/3, 1940/7500] Training Loss: 1.2883014321327209
[2/3, 1950/7500] Training Loss: 1.2079741477966308
[2/3, 1960/7500] Training Loss: 1.2295835614204407
[2/3, 1970/7500] Training Loss: 1.2411662578582763
[2/3, 1980/7500] Training Loss: 1.2105977177619933
[2/3, 1990/7500] Training Loss: 1.3120319843292236
[2/3, 2000/7500] Training Loss: 1.2513377666473389
[2/3, 2010/7500] Training Loss: 1.2263250470161438
[2/3, 2020/7500] Training Loss: 1.26332288980484
[2/3, 2030/7500] Training Loss: 1.1964285314083098
[2/3, 2040/7500] Training Loss: 1.2435080766677857
[2/3, 2050/7500] Training Loss: 1.1872284710407257
[2/3, 2060/7500] Training Loss: 1.245543396472931
[2/3, 2070/7500] Training Loss: 1.2728583574295045
[2/3, 2080/7500] Training Loss: 1.2279753684997559
[2/3, 2090/7500] Training Loss: 1.2083252906799316
[2/3, 2100/7500] Training Loss: 1.2769277691841125
[2/3, 2110/7500] Training Loss: 1.3019222378730775
[2/3, 2120/7500] Training Loss: 1.2660092830657959
[2/3, 2130/7500] Training Loss: 1.2261419296264648
[2/3, 2140/7500] Training Loss: 1.2478906512260437
[2/3, 2150/7500] Training Loss: 1.233638334274292
[2/3, 2160/7500] Training Loss: 1.1855132222175597
[2/3, 2170/7500] Training Loss: 1.246204400062561
[2/3, 2180/7500] Training Loss: 1.2660636186599732
[2/3, 2190/7500] Training Loss: 1.169185757637024
[2/3, 2200/7500] Training Loss: 1.244619882106781
[2/3, 2210/7500] Training Loss: 1.223844975233078
[2/3, 2220/7500] Training Loss: 1.1662373185157775
[2/3, 2230/7500] Training Loss: 1.2304299414157867
[2/3, 2240/7500] Training Loss: 1.1715084552764892
[2/3, 2250/7500] Training Loss: 1.2914207935333253
[2/3, 2260/7500] Training Loss: 1.2594009637832642
[2/3, 2270/7500] Training Loss: 1.2181583523750306
[2/3, 2280/7500] Training Loss: 1.2335252523422242
[2/3, 2290/7500] Training Loss: 1.2344198346138
[2/3, 2300/7500] Training Loss: 1.2158301711082458
[2/3, 2310/7500] Training Loss: 1.2460733890533446
[2/3, 2320/7500] Training Loss: 1.2274987697601318
[2/3, 2330/7500] Training Loss: 1.2230114936828613
[2/3, 2340/7500] Training Loss: 1.1858221888542175
[2/3, 2350/7500] Training Loss: 1.2248731791973113
[2/3, 2360/7500] Training Loss: 1.2138922572135926
[2/3, 2370/7500] Training Loss: 1.270463329553604
[2/3, 2380/7500] Training Loss: 1.2159120202064515
[2/3, 2390/7500] Training Loss: 1.1503664135932923
[2/3, 2400/7500] Training Loss: 1.2546486258506775
[2/3, 2410/7500] Training Loss: 1.218113088607788
[2/3, 2420/7500] Training Loss: 1.2739124298095703
[2/3, 2430/7500] Training Loss: 1.258725368976593
[2/3, 2440/7500] Training Loss: 1.242271399497986
[2/3, 2450/7500] Training Loss: 1.2224080324172975
[2/3, 2460/7500] Training Loss: 1.1961908400058747
[2/3, 2470/7500] Training Loss: 1.2109040319919586
[2/3, 2480/7500] Training Loss: 1.2120786249637603
[2/3, 2490/7500] Training Loss: 1.2926089406013488
[2/3, 2500/7500] Training Loss: 1.1823741436004638
[2/3, 2510/7500] Training Loss: 1.2338774085044861
[2/3, 2520/7500] Training Loss: 1.221653163433075
[2/3, 2530/7500] Training Loss: 1.2542038083076477
[2/3, 2540/7500] Training Loss: 1.2507358908653259
[2/3, 2550/7500] Training Loss: 1.1824596881866456
[2/3, 2560/7500] Training Loss: 1.3135693430900575
[2/3, 2570/7500] Training Loss: 1.2440346240997315
[2/3, 2580/7500] Training Loss: 1.2114750385284423
[2/3, 2590/7500] Training Loss: 1.2070460438728332
[2/3, 2600/7500] Training Loss: 1.2688289403915405
[2/3, 2610/7500] Training Loss: 1.1835393190383912
[2/3, 2620/7500] Training Loss: 1.112435907125473
[2/3, 2630/7500] Training Loss: 1.2365639925003051
[2/3, 2640/7500] Training Loss: 1.1984404921531677
[2/3, 2650/7500] Training Loss: 1.1527079701423646
[2/3, 2660/7500] Training Loss: 1.2094195008277893
[2/3, 2670/7500] Training Loss: 1.2484837532043458
[2/3, 2680/7500] Training Loss: 1.217987072467804
[2/3, 2690/7500] Training Loss: 1.245532214641571
[2/3, 2700/7500] Training Loss: 1.1545621514320374
[2/3, 2710/7500] Training Loss: 1.2632834315299988
[2/3, 2720/7500] Training Loss: 1.1997477412223816
[2/3, 2730/7500] Training Loss: 1.268576455116272
[2/3, 2740/7500] Training Loss: 1.2274971842765807
[2/3, 2750/7500] Training Loss: 1.2399852275848389
[2/3, 2760/7500] Training Loss: 1.16644207239151
[2/3, 2770/7500] Training Loss: 1.2909518003463745
[2/3, 2780/7500] Training Loss: 1.1903732657432555
[2/3, 2790/7500] Training Loss: 1.2252682447433472
[2/3, 2800/7500] Training Loss: 1.2919434189796448
[2/3, 2810/7500] Training Loss: 1.1797908663749694
[2/3, 2820/7500] Training Loss: 1.1917898297309875
[2/3, 2830/7500] Training Loss: 1.2206019282341003
[2/3, 2840/7500] Training Loss: 1.2100745558738708
[2/3, 2850/7500] Training Loss: 1.2630985379219055
[2/3, 2860/7500] Training Loss: 1.2558675765991212
[2/3, 2870/7500] Training Loss: 1.247935938835144
[2/3, 2880/7500] Training Loss: 1.2461947560310365
[2/3, 2890/7500] Training Loss: 1.2525990962982179
[2/3, 2900/7500] Training Loss: 1.1786094665527345
[2/3, 2910/7500] Training Loss: 1.2276471734046936
[2/3, 2920/7500] Training Loss: 1.2576236724853516
[2/3, 2930/7500] Training Loss: 1.1725294530391692
[2/3, 2940/7500] Training Loss: 1.2268372893333435
[2/3, 2950/7500] Training Loss: 1.2351492822170258
[2/3, 2960/7500] Training Loss: 1.2983566045761108
[2/3, 2970/7500] Training Loss: 1.2366859912872314
[2/3, 2980/7500] Training Loss: 1.1976491928100585
[2/3, 2990/7500] Training Loss: 1.2343532919883728
[2/3, 3000/7500] Training Loss: 1.2581889986991883
[2/3, 3010/7500] Training Loss: 1.1953614592552184
[2/3, 3020/7500] Training Loss: 1.2420557379722594
[2/3, 3030/7500] Training Loss: 1.1889177083969116
[2/3, 3040/7500] Training Loss: 1.1419689536094666
[2/3, 3050/7500] Training Loss: 1.1981834888458252
[2/3, 3060/7500] Training Loss: 1.1842298567295075
[2/3, 3070/7500] Training Loss: 1.2094046831130982
[2/3, 3080/7500] Training Loss: 1.0876018106937408
[2/3, 3090/7500] Training Loss: 1.2697503685951232
[2/3, 3100/7500] Training Loss: 1.2248082160949707
[2/3, 3110/7500] Training Loss: 1.1736312508583069
[2/3, 3120/7500] Training Loss: 1.1897989213466644
[2/3, 3130/7500] Training Loss: 1.2242456793785095
[2/3, 3140/7500] Training Loss: 1.2719715237617493
[2/3, 3150/7500] Training Loss: 1.2064094066619873
[2/3, 3160/7500] Training Loss: 1.2689344882965088
[2/3, 3170/7500] Training Loss: 1.2630967617034912
[2/3, 3180/7500] Training Loss: 1.2626048028469086
[2/3, 3190/7500] Training Loss: 1.2864004373550415
[2/3, 3200/7500] Training Loss: 1.2330169081687927
[2/3, 3210/7500] Training Loss: 1.2714156627655029
[2/3, 3220/7500] Training Loss: 1.1583118438720703
[2/3, 3230/7500] Training Loss: 1.2494841456413268
[2/3, 3240/7500] Training Loss: 1.2616950631141663
[2/3, 3250/7500] Training Loss: 1.2538581967353821
[2/3, 3260/7500] Training Loss: 1.2213697910308838
[2/3, 3270/7500] Training Loss: 1.1963695645332337
[2/3, 3280/7500] Training Loss: 1.1924140214920045
[2/3, 3290/7500] Training Loss: 1.2626808404922485
[2/3, 3300/7500] Training Loss: 1.1946231186389924
[2/3, 3310/7500] Training Loss: 1.2493981719017029
[2/3, 3320/7500] Training Loss: 1.229399311542511
[2/3, 3330/7500] Training Loss: 1.1975243508815765
[2/3, 3340/7500] Training Loss: 1.2473154067993164
[2/3, 3350/7500] Training Loss: 1.2222140848636627
[2/3, 3360/7500] Training Loss: 1.1586085736751557
[2/3, 3370/7500] Training Loss: 1.2721380949020387
[2/3, 3380/7500] Training Loss: 1.131555938720703
[2/3, 3390/7500] Training Loss: 1.2097230076789856
[2/3, 3400/7500] Training Loss: 1.2385019481182098
[2/3, 3410/7500] Training Loss: 1.2431853532791137
[2/3, 3420/7500] Training Loss: 1.2400309681892394
[2/3, 3430/7500] Training Loss: 1.3166611671447754
[2/3, 3440/7500] Training Loss: 1.1863088965415955
[2/3, 3450/7500] Training Loss: 1.2216011047363282
[2/3, 3460/7500] Training Loss: 1.2331344306468963
[2/3, 3470/7500] Training Loss: 1.1937206268310547
[2/3, 3480/7500] Training Loss: 1.2928463101387024
[2/3, 3490/7500] Training Loss: 1.220275890827179
[2/3, 3500/7500] Training Loss: 1.2488664031028747
[2/3, 3510/7500] Training Loss: 1.2466960906982423
[2/3, 3520/7500] Training Loss: 1.2685920476913453
[2/3, 3530/7500] Training Loss: 1.1753888130187988
[2/3, 3540/7500] Training Loss: 1.2293897151947022
[2/3, 3550/7500] Training Loss: 1.188326644897461
[2/3, 3560/7500] Training Loss: 1.243771779537201
[2/3, 3570/7500] Training Loss: 1.23713880777359
[2/3, 3580/7500] Training Loss: 1.1679247975349427
[2/3, 3590/7500] Training Loss: 1.1699103474617005
[2/3, 3600/7500] Training Loss: 1.171256536245346
[2/3, 3610/7500] Training Loss: 1.1985880255699157
[2/3, 3620/7500] Training Loss: 1.190648627281189
[2/3, 3630/7500] Training Loss: 1.292764163017273
[2/3, 3640/7500] Training Loss: 1.1824192225933075
[2/3, 3650/7500] Training Loss: 1.240783876180649
[2/3, 3660/7500] Training Loss: 1.236894965171814
[2/3, 3670/7500] Training Loss: 1.1600778937339782
[2/3, 3680/7500] Training Loss: 1.2423884749412537
[2/3, 3690/7500] Training Loss: 1.2475133180618285
[2/3, 3700/7500] Training Loss: 1.2237851321697235
[2/3, 3710/7500] Training Loss: 1.2646453261375428
[2/3, 3720/7500] Training Loss: 1.2703023433685303
[2/3, 3730/7500] Training Loss: 1.2727179169654845
[2/3, 3740/7500] Training Loss: 1.2184270024299622
[2/3, 3750/7500] Training Loss: 1.2212436079978943
[2/3, 3760/7500] Training Loss: 1.2454481244087219
[2/3, 3770/7500] Training Loss: 1.2665860176086425
[2/3, 3780/7500] Training Loss: 1.2408193945884705
[2/3, 3790/7500] Training Loss: 1.158333432674408
[2/3, 3800/7500] Training Loss: 1.2582398653030396
[2/3, 3810/7500] Training Loss: 1.2145391941070556
[2/3, 3820/7500] Training Loss: 1.2231610596179963
[2/3, 3830/7500] Training Loss: 1.2184669971466064
[2/3, 3840/7500] Training Loss: 1.2043413877487184
[2/3, 3850/7500] Training Loss: 1.0854611814022064
[2/3, 3860/7500] Training Loss: 1.2144064426422119
[2/3, 3870/7500] Training Loss: 1.2488759636878968
[2/3, 3880/7500] Training Loss: 1.2198848128318787
[2/3, 3890/7500] Training Loss: 1.2462623715400696
[2/3, 3900/7500] Training Loss: 1.2180076122283936
[2/3, 3910/7500] Training Loss: 1.2236103415489197
[2/3, 3920/7500] Training Loss: 1.1805287539958953
[2/3, 3930/7500] Training Loss: 1.250942850112915
[2/3, 3940/7500] Training Loss: 1.2019741773605346
[2/3, 3950/7500] Training Loss: 1.1943308591842652
[2/3, 3960/7500] Training Loss: 1.2276673913002014
[2/3, 3970/7500] Training Loss: 1.1572879374027252
[2/3, 3980/7500] Training Loss: 1.2939374089241027
[2/3, 3990/7500] Training Loss: 1.1958601355552674
[2/3, 4000/7500] Training Loss: 1.2610476613044739
[2/3, 4010/7500] Training Loss: 1.1808708012104034
[2/3, 4020/7500] Training Loss: 1.14570814371109
[2/3, 4030/7500] Training Loss: 1.1833923757076263
[2/3, 4040/7500] Training Loss: 1.2420019030570983
[2/3, 4050/7500] Training Loss: 1.2877804279327392
[2/3, 4060/7500] Training Loss: 1.211379623413086
[2/3, 4070/7500] Training Loss: 1.250774371623993
[2/3, 4080/7500] Training Loss: 1.2247722446918488
[2/3, 4090/7500] Training Loss: 1.220948588848114
[2/3, 4100/7500] Training Loss: 1.1410457074642182
[2/3, 4110/7500] Training Loss: 1.2567121148109437
[2/3, 4120/7500] Training Loss: 1.2763627529144288
[2/3, 4130/7500] Training Loss: 1.1852194964885712
[2/3, 4140/7500] Training Loss: 1.2324693858623506
[2/3, 4150/7500] Training Loss: 1.169390892982483
[2/3, 4160/7500] Training Loss: 1.2496080636978149
[2/3, 4170/7500] Training Loss: 1.2103472590446471
[2/3, 4180/7500] Training Loss: 1.1862672567367554
[2/3, 4190/7500] Training Loss: 1.1787347316741943
[2/3, 4200/7500] Training Loss: 1.201919275522232
[2/3, 4210/7500] Training Loss: 1.165268647670746
[2/3, 4220/7500] Training Loss: 1.1896684646606446
[2/3, 4230/7500] Training Loss: 1.2295459508895874
[2/3, 4240/7500] Training Loss: 1.2166381657123566
[2/3, 4250/7500] Training Loss: 1.1719843685626983
[2/3, 4260/7500] Training Loss: 1.2368147492408752
[2/3, 4270/7500] Training Loss: 1.2654354095458984
[2/3, 4280/7500] Training Loss: 1.2619476437568664
[2/3, 4290/7500] Training Loss: 1.2082403302192688
[2/3, 4300/7500] Training Loss: 1.3259544491767883
[2/3, 4310/7500] Training Loss: 1.1556907713413238
[2/3, 4320/7500] Training Loss: 1.3540295124053956
[2/3, 4330/7500] Training Loss: 1.229521882534027
[2/3, 4340/7500] Training Loss: 1.1990505874156951
[2/3, 4350/7500] Training Loss: 1.2436941146850586
[2/3, 4360/7500] Training Loss: 1.1837472558021545
[2/3, 4370/7500] Training Loss: 1.2509124517440795
[2/3, 4380/7500] Training Loss: 1.1839698433876038
[2/3, 4390/7500] Training Loss: 1.2752664685249329
[2/3, 4400/7500] Training Loss: 1.2791326522827149
[2/3, 4410/7500] Training Loss: 1.2078145623207093
[2/3, 4420/7500] Training Loss: 1.2166955947875977
[2/3, 4430/7500] Training Loss: 1.2096351265907288
[2/3, 4440/7500] Training Loss: 1.191291046142578
[2/3, 4450/7500] Training Loss: 1.2063003778457642
[2/3, 4460/7500] Training Loss: 1.14364093542099
[2/3, 4470/7500] Training Loss: 1.2658812165260316
[2/3, 4480/7500] Training Loss: 1.284281587600708
[2/3, 4490/7500] Training Loss: 1.2847306966781615
[2/3, 4500/7500] Training Loss: 1.2533611536026001
[2/3, 4510/7500] Training Loss: 1.197712641954422
[2/3, 4520/7500] Training Loss: 1.1708476483821868
[2/3, 4530/7500] Training Loss: 1.23050799369812
[2/3, 4540/7500] Training Loss: 1.2786139249801636
[2/3, 4550/7500] Training Loss: 1.1945841670036317
[2/3, 4560/7500] Training Loss: 1.2351380825042724
[2/3, 4570/7500] Training Loss: 1.2970332622528076
[2/3, 4580/7500] Training Loss: 1.2526637673377992
[2/3, 4590/7500] Training Loss: 1.237698245048523
[2/3, 4600/7500] Training Loss: 1.1553477048873901
[2/3, 4610/7500] Training Loss: 1.171852421760559
[2/3, 4620/7500] Training Loss: 1.212169623374939
[2/3, 4630/7500] Training Loss: 1.1828105330467225
[2/3, 4640/7500] Training Loss: 1.2663423895835877
[2/3, 4650/7500] Training Loss: 1.2178405165672301
[2/3, 4660/7500] Training Loss: 1.2417749285697937
[2/3, 4670/7500] Training Loss: 1.2182438492774963
[2/3, 4680/7500] Training Loss: 1.2441414952278138
[2/3, 4690/7500] Training Loss: 1.2374988794326782
[2/3, 4700/7500] Training Loss: 1.2481392562389373
[2/3, 4710/7500] Training Loss: 1.2729975581169128
[2/3, 4720/7500] Training Loss: 1.2446023225784302
[2/3, 4730/7500] Training Loss: 1.200592577457428
[2/3, 4740/7500] Training Loss: 1.257176160812378
[2/3, 4750/7500] Training Loss: 1.2542099714279176
[2/3, 4760/7500] Training Loss: 1.1958814024925233
[2/3, 4770/7500] Training Loss: 1.2426196575164794
[2/3, 4780/7500] Training Loss: 1.155064332485199
[2/3, 4790/7500] Training Loss: 1.263233244419098
[2/3, 4800/7500] Training Loss: 1.1930655121803284
[2/3, 4810/7500] Training Loss: 1.2144366145133971
[2/3, 4820/7500] Training Loss: 1.1612839102745056
[2/3, 4830/7500] Training Loss: 1.2134361147880555
[2/3, 4840/7500] Training Loss: 1.1429957091808318
[2/3, 4850/7500] Training Loss: 1.174416369199753
[2/3, 4860/7500] Training Loss: 1.2067858755588532
[2/3, 4870/7500] Training Loss: 1.2499078035354614
[2/3, 4880/7500] Training Loss: 1.1602091670036316
[2/3, 4890/7500] Training Loss: 1.2828531980514526
[2/3, 4900/7500] Training Loss: 1.1925548195838929
[2/3, 4910/7500] Training Loss: 1.2286410927772522
[2/3, 4920/7500] Training Loss: 1.2840283274650575
[2/3, 4930/7500] Training Loss: 1.2046452164649963
[2/3, 4940/7500] Training Loss: 1.1312094569206237
[2/3, 4950/7500] Training Loss: 1.2232764959335327
[2/3, 4960/7500] Training Loss: 1.2106110811233521
[2/3, 4970/7500] Training Loss: 1.1740265130996703
[2/3, 4980/7500] Training Loss: 1.1566886603832245
[2/3, 4990/7500] Training Loss: 1.2179591059684753
[2/3, 5000/7500] Training Loss: 1.1986868619918822
[2/3, 5010/7500] Training Loss: 1.1564041435718537
[2/3, 5020/7500] Training Loss: 1.1713220059871674
[2/3, 5030/7500] Training Loss: 1.2633084535598755
[2/3, 5040/7500] Training Loss: 1.2296495199203492
[2/3, 5050/7500] Training Loss: 1.2061031699180602
[2/3, 5060/7500] Training Loss: 1.2303543925285338
[2/3, 5070/7500] Training Loss: 1.2316754639148713
[2/3, 5080/7500] Training Loss: 1.2897525072097777
[2/3, 5090/7500] Training Loss: 1.165841281414032
[2/3, 5100/7500] Training Loss: 1.1545103669166565
[2/3, 5110/7500] Training Loss: 1.2067289888858794
[2/3, 5120/7500] Training Loss: 1.1971355676651
[2/3, 5130/7500] Training Loss: 1.260802936553955
[2/3, 5140/7500] Training Loss: 1.1325045943260192
[2/3, 5150/7500] Training Loss: 1.241861915588379
[2/3, 5160/7500] Training Loss: 1.2397311925888062
[2/3, 5170/7500] Training Loss: 1.2361932635307311
[2/3, 5180/7500] Training Loss: 1.2516420602798461
[2/3, 5190/7500] Training Loss: 1.1310968279838562
[2/3, 5200/7500] Training Loss: 1.2154695332050323
[2/3, 5210/7500] Training Loss: 1.2474527955055237
[2/3, 5220/7500] Training Loss: 1.1868663370609283
[2/3, 5230/7500] Training Loss: 1.2008806347846985
[2/3, 5240/7500] Training Loss: 1.1951874852180482
[2/3, 5250/7500] Training Loss: 1.2527531266212464
[2/3, 5260/7500] Training Loss: 1.2682058930397033
[2/3, 5270/7500] Training Loss: 1.214224934577942
[2/3, 5280/7500] Training Loss: 1.2389196276664733
[2/3, 5290/7500] Training Loss: 1.2469170451164246
[2/3, 5300/7500] Training Loss: 1.2218833446502686
[2/3, 5310/7500] Training Loss: 1.1960119247436523
[2/3, 5320/7500] Training Loss: 1.2311564326286315
[2/3, 5330/7500] Training Loss: 1.2556281089782715
[2/3, 5340/7500] Training Loss: 1.1872537016868592
[2/3, 5350/7500] Training Loss: 1.2332655549049378
[2/3, 5360/7500] Training Loss: 1.3018999934196471
[2/3, 5370/7500] Training Loss: 1.2287592530250548
[2/3, 5380/7500] Training Loss: 1.2280660152435303
[2/3, 5390/7500] Training Loss: 1.3308674216270446
[2/3, 5400/7500] Training Loss: 1.2351555824279785
[2/3, 5410/7500] Training Loss: 1.2986738681793213
[2/3, 5420/7500] Training Loss: 1.2755589246749879
[2/3, 5430/7500] Training Loss: 1.2039361834526061
[2/3, 5440/7500] Training Loss: 1.1965121626853943
[2/3, 5450/7500] Training Loss: 1.204955816268921
[2/3, 5460/7500] Training Loss: 1.1521506309509277
[2/3, 5470/7500] Training Loss: 1.2242381751537323
[2/3, 5480/7500] Training Loss: 1.185513436794281
[2/3, 5490/7500] Training Loss: 1.1662723302841187
[2/3, 5500/7500] Training Loss: 1.1837060630321503
[2/3, 5510/7500] Training Loss: 1.188742446899414
[2/3, 5520/7500] Training Loss: 1.244468778371811
[2/3, 5530/7500] Training Loss: 1.264197587966919
[2/3, 5540/7500] Training Loss: 1.190899920463562
[2/3, 5550/7500] Training Loss: 1.2458501219749452
[2/3, 5560/7500] Training Loss: 1.19419686794281
[2/3, 5570/7500] Training Loss: 1.2867871880531312
[2/3, 5580/7500] Training Loss: 1.2348458766937256
[2/3, 5590/7500] Training Loss: 1.1932464241981506
[2/3, 5600/7500] Training Loss: 1.2113921403884889
[2/3, 5610/7500] Training Loss: 1.1747371912002564
[2/3, 5620/7500] Training Loss: 1.1450537741184235
[2/3, 5630/7500] Training Loss: 1.2420633137226105
[2/3, 5640/7500] Training Loss: 1.2270688652992248
[2/3, 5650/7500] Training Loss: 1.3277164816856384
[2/3, 5660/7500] Training Loss: 1.2022250771522522
[2/3, 5670/7500] Training Loss: 1.1843693017959596
[2/3, 5680/7500] Training Loss: 1.2059702157974244
[2/3, 5690/7500] Training Loss: 1.1979856133460998
[2/3, 5700/7500] Training Loss: 1.2384652018547058
[2/3, 5710/7500] Training Loss: 1.2758352875709533
[2/3, 5720/7500] Training Loss: 1.2180360078811645
[2/3, 5730/7500] Training Loss: 1.3220033943653107
[2/3, 5740/7500] Training Loss: 1.2340759992599488
[2/3, 5750/7500] Training Loss: 1.1641854166984558
[2/3, 5760/7500] Training Loss: 1.2196564674377441
[2/3, 5770/7500] Training Loss: 1.2354594826698304
[2/3, 5780/7500] Training Loss: 1.1914339065551758
[2/3, 5790/7500] Training Loss: 1.2843336701393127
[2/3, 5800/7500] Training Loss: 1.223177582025528
[2/3, 5810/7500] Training Loss: 1.310889232158661
[2/3, 5820/7500] Training Loss: 1.2140394568443298
[2/3, 5830/7500] Training Loss: 1.161239469051361
[2/3, 5840/7500] Training Loss: 1.2673001885414124
[2/3, 5850/7500] Training Loss: 1.2155935406684875
[2/3, 5860/7500] Training Loss: 1.1919965267181396
[2/3, 5870/7500] Training Loss: 1.2309817790985107
[2/3, 5880/7500] Training Loss: 1.177082324028015
[2/3, 5890/7500] Training Loss: 1.2998778223991394
[2/3, 5900/7500] Training Loss: 1.1834285318851472
[2/3, 5910/7500] Training Loss: 1.276797878742218
[2/3, 5920/7500] Training Loss: 1.2181441068649292
[2/3, 5930/7500] Training Loss: 1.2759086966514588
[2/3, 5940/7500] Training Loss: 1.1726253032684326
[2/3, 5950/7500] Training Loss: 1.2425024509429932
[2/3, 5960/7500] Training Loss: 1.253744864463806
[2/3, 5970/7500] Training Loss: 1.2196056962013244
[2/3, 5980/7500] Training Loss: 1.2120206356048584
[2/3, 5990/7500] Training Loss: 1.1957247793674468
[2/3, 6000/7500] Training Loss: 1.2432144105434417
[2/3, 6010/7500] Training Loss: 1.2615992426872253
[2/3, 6020/7500] Training Loss: 1.1972751140594482
[2/3, 6030/7500] Training Loss: 1.250287699699402
[2/3, 6040/7500] Training Loss: 1.2339874505996704
[2/3, 6050/7500] Training Loss: 1.2720446467399598
[2/3, 6060/7500] Training Loss: 1.211095929145813
[2/3, 6070/7500] Training Loss: 1.2422493696212769
[2/3, 6080/7500] Training Loss: 1.1883060336112976
[2/3, 6090/7500] Training Loss: 1.3043418645858764
[2/3, 6100/7500] Training Loss: 1.2147127032279967
[2/3, 6110/7500] Training Loss: 1.2264607906341554
[2/3, 6120/7500] Training Loss: 1.2459609627723693
[2/3, 6130/7500] Training Loss: 1.1711455643177033
[2/3, 6140/7500] Training Loss: 1.186757493019104
[2/3, 6150/7500] Training Loss: 1.294511616230011
[2/3, 6160/7500] Training Loss: 1.2665360808372497
[2/3, 6170/7500] Training Loss: 1.2073275685310363
[2/3, 6180/7500] Training Loss: 1.2077763676643372
[2/3, 6190/7500] Training Loss: 1.203451406955719
[2/3, 6200/7500] Training Loss: 1.205647611618042
[2/3, 6210/7500] Training Loss: 1.2103142738342285
[2/3, 6220/7500] Training Loss: 1.2787967443466186
[2/3, 6230/7500] Training Loss: 1.1953241527080536
[2/3, 6240/7500] Training Loss: 1.2969134628772736
[2/3, 6250/7500] Training Loss: 1.2026783585548402
[2/3, 6260/7500] Training Loss: 1.2085028409957885
[2/3, 6270/7500] Training Loss: 1.2058804988861085
[2/3, 6280/7500] Training Loss: 1.2407204151153564
[2/3, 6290/7500] Training Loss: 1.2033254384994507
[2/3, 6300/7500] Training Loss: 1.1821012854576112
[2/3, 6310/7500] Training Loss: 1.2221262216567994
[2/3, 6320/7500] Training Loss: 1.202063536643982
[2/3, 6330/7500] Training Loss: 1.1832054495811462
[2/3, 6340/7500] Training Loss: 1.3001061081886292
[2/3, 6350/7500] Training Loss: 1.2073489546775817
[2/3, 6360/7500] Training Loss: 1.2104735970497131
[2/3, 6370/7500] Training Loss: 1.2052895367145537
[2/3, 6380/7500] Training Loss: 1.2347434043884278
[2/3, 6390/7500] Training Loss: 1.2342706203460694
[2/3, 6400/7500] Training Loss: 1.240946489572525
[2/3, 6410/7500] Training Loss: 1.288432204723358
[2/3, 6420/7500] Training Loss: 1.1989856123924256
[2/3, 6430/7500] Training Loss: 1.2520264744758607
[2/3, 6440/7500] Training Loss: 1.2000527024269103
[2/3, 6450/7500] Training Loss: 1.2684175968170166
[2/3, 6460/7500] Training Loss: 1.239773118495941
[2/3, 6470/7500] Training Loss: 1.2553354024887085
[2/3, 6480/7500] Training Loss: 1.2049250960350038
[2/3, 6490/7500] Training Loss: 1.2150429487228394
[2/3, 6500/7500] Training Loss: 1.1929704010486604
[2/3, 6510/7500] Training Loss: 1.242057991027832
[2/3, 6520/7500] Training Loss: 1.2179922342300415
[2/3, 6530/7500] Training Loss: 1.1304404795169831
[2/3, 6540/7500] Training Loss: 1.272932517528534
[2/3, 6550/7500] Training Loss: 1.1668196737766265
[2/3, 6560/7500] Training Loss: 1.2620532989501954
[2/3, 6570/7500] Training Loss: 1.2524631142616272
[2/3, 6580/7500] Training Loss: 1.2553516745567321
[2/3, 6590/7500] Training Loss: 1.2380284368991852
[2/3, 6600/7500] Training Loss: 1.2907515168190002
[2/3, 6610/7500] Training Loss: 1.2441565155982972
[2/3, 6620/7500] Training Loss: 1.2480259001255036
[2/3, 6630/7500] Training Loss: 1.1850106120109558
[2/3, 6640/7500] Training Loss: 1.228929579257965
[2/3, 6650/7500] Training Loss: 1.2157810926437378
[2/3, 6660/7500] Training Loss: 1.153526282310486
[2/3, 6670/7500] Training Loss: 1.271129298210144
[2/3, 6680/7500] Training Loss: 1.1768107533454895
[2/3, 6690/7500] Training Loss: 1.168536865711212
[2/3, 6700/7500] Training Loss: 1.2333862483501434
[2/3, 6710/7500] Training Loss: 1.241113555431366
[2/3, 6720/7500] Training Loss: 1.2457027673721313
[2/3, 6730/7500] Training Loss: 1.2930733799934386
[2/3, 6740/7500] Training Loss: 1.2229772210121155
[2/3, 6750/7500] Training Loss: 1.1572591364383698
[2/3, 6760/7500] Training Loss: 1.2953551769256593
[2/3, 6770/7500] Training Loss: 1.2332075834274292
[2/3, 6780/7500] Training Loss: 1.1370034039020538
[2/3, 6790/7500] Training Loss: 1.2674506068229676
[2/3, 6800/7500] Training Loss: 1.277165985107422
[2/3, 6810/7500] Training Loss: 1.2277273535728455
[2/3, 6820/7500] Training Loss: 1.3232666492462157
[2/3, 6830/7500] Training Loss: 1.2634388506412506
[2/3, 6840/7500] Training Loss: 1.2089494347572327
[2/3, 6850/7500] Training Loss: 1.261480724811554
[2/3, 6860/7500] Training Loss: 1.1991225719451903
[2/3, 6870/7500] Training Loss: 1.2444963455200195
[2/3, 6880/7500] Training Loss: 1.218354094028473
[2/3, 6890/7500] Training Loss: 1.1770094633102417
[2/3, 6900/7500] Training Loss: 1.240564113855362
[2/3, 6910/7500] Training Loss: 1.2476617336273192
[2/3, 6920/7500] Training Loss: 1.18058443069458
[2/3, 6930/7500] Training Loss: 1.1970570087432861
[2/3, 6940/7500] Training Loss: 1.1842654764652252
[2/3, 6950/7500] Training Loss: 1.1696299314498901
[2/3, 6960/7500] Training Loss: 1.2437688946723937
[2/3, 6970/7500] Training Loss: 1.2253539323806764
[2/3, 6980/7500] Training Loss: 1.1832128584384918
[2/3, 6990/7500] Training Loss: 1.2956048965454101
[2/3, 7000/7500] Training Loss: 1.1336823940277099
[2/3, 7010/7500] Training Loss: 1.2367886543273925
[2/3, 7020/7500] Training Loss: 1.23489248752594
[2/3, 7030/7500] Training Loss: 1.168484205007553
[2/3, 7040/7500] Training Loss: 1.2065773546695708
[2/3, 7050/7500] Training Loss: 1.2260818660259247
[2/3, 7060/7500] Training Loss: 1.2188658356666564
[2/3, 7070/7500] Training Loss: 1.2830345511436463
[2/3, 7080/7500] Training Loss: 1.2194440960884094
[2/3, 7090/7500] Training Loss: 1.2277688026428222
[2/3, 7100/7500] Training Loss: 1.2039816737174989
[2/3, 7110/7500] Training Loss: 1.194122350215912
[2/3, 7120/7500] Training Loss: 1.263420569896698
[2/3, 7130/7500] Training Loss: 1.2445046126842498
[2/3, 7140/7500] Training Loss: 1.207476305961609
[2/3, 7150/7500] Training Loss: 1.2452780961990357
[2/3, 7160/7500] Training Loss: 1.2590466260910034
[2/3, 7170/7500] Training Loss: 1.3398351311683654
[2/3, 7180/7500] Training Loss: 1.2058045566082
[2/3, 7190/7500] Training Loss: 1.1859334468841554
[2/3, 7200/7500] Training Loss: 1.2681384265422821
[2/3, 7210/7500] Training Loss: 1.2191212832927705
[2/3, 7220/7500] Training Loss: 1.2599658131599427
[2/3, 7230/7500] Training Loss: 1.272456741333008
[2/3, 7240/7500] Training Loss: 1.1355533123016357
[2/3, 7250/7500] Training Loss: 1.2341365933418273
[2/3, 7260/7500] Training Loss: 1.1520525872707368
[2/3, 7270/7500] Training Loss: 1.2458469152450562
[2/3, 7280/7500] Training Loss: 1.2603426218032836
[2/3, 7290/7500] Training Loss: 1.2709969282150269
[2/3, 7300/7500] Training Loss: 1.2440748810768127
[2/3, 7310/7500] Training Loss: 1.2548161029815674
[2/3, 7320/7500] Training Loss: 1.2404531180858611
[2/3, 7330/7500] Training Loss: 1.1988393664360046
[2/3, 7340/7500] Training Loss: 1.1871055603027343
[2/3, 7350/7500] Training Loss: 1.0888575851917266
[2/3, 7360/7500] Training Loss: 1.2345552802085877
[2/3, 7370/7500] Training Loss: 1.2621260404586792
[2/3, 7380/7500] Training Loss: 1.2068317770957946
[2/3, 7390/7500] Training Loss: 1.1794476747512816
[2/3, 7400/7500] Training Loss: 1.156372106075287
[2/3, 7410/7500] Training Loss: 1.205410498380661
[2/3, 7420/7500] Training Loss: 1.221701967716217
[2/3, 7430/7500] Training Loss: 1.2461762189865113
[2/3, 7440/7500] Training Loss: 1.3031360983848572
[2/3, 7450/7500] Training Loss: 1.2216954469680785
[2/3, 7460/7500] Training Loss: 1.1793991923332214
[2/3, 7470/7500] Training Loss: 1.2760867595672607
[2/3, 7480/7500] Training Loss: 1.1538556516170502
[2/3, 7490/7500] Training Loss: 1.2757782459259033
[2/3, 7500/7500] Training Loss: 1.1904830574989318
5/5: Testing network...
Testing Loss: 1.2378233822584153
5/5: Training network...
[3/3, 10/7500] Training Loss: 1.2377041697502136
[3/3, 20/7500] Training Loss: 1.1947717726230622
[3/3, 30/7500] Training Loss: 1.25789954662323
[3/3, 40/7500] Training Loss: 1.2330438017845153
[3/3, 50/7500] Training Loss: 1.1725164890289306
[3/3, 60/7500] Training Loss: 1.1879260540008545
[3/3, 70/7500] Training Loss: 1.2426535665988923
[3/3, 80/7500] Training Loss: 1.3000812888145448
[3/3, 90/7500] Training Loss: 1.2854716300964355
[3/3, 100/7500] Training Loss: 1.1950154304504395
[3/3, 110/7500] Training Loss: 1.1743680775165557
[3/3, 120/7500] Training Loss: 1.219381332397461
[3/3, 130/7500] Training Loss: 1.1749870598316192
[3/3, 140/7500] Training Loss: 1.2153699100017548
[3/3, 150/7500] Training Loss: 1.2945976614952088
[3/3, 160/7500] Training Loss: 1.2382248282432555
[3/3, 170/7500] Training Loss: 1.2615959882736205
[3/3, 180/7500] Training Loss: 1.1650819182395935
[3/3, 190/7500] Training Loss: 1.1659285843372345
[3/3, 200/7500] Training Loss: 1.2715736865997314
[3/3, 210/7500] Training Loss: 1.2723974704742431
[3/3, 220/7500] Training Loss: 1.232016348838806
[3/3, 230/7500] Training Loss: 1.1149401009082793
[3/3, 240/7500] Training Loss: 1.1894482135772706
[3/3, 250/7500] Training Loss: 1.2326143503189086
[3/3, 260/7500] Training Loss: 1.1817265570163726
[3/3, 270/7500] Training Loss: 1.1922869443893434
[3/3, 280/7500] Training Loss: 1.269089412689209
[3/3, 290/7500] Training Loss: 1.2117469310760498
[3/3, 300/7500] Training Loss: 1.2809973180294036
[3/3, 310/7500] Training Loss: 1.2054895877838134
[3/3, 320/7500] Training Loss: 1.208557367324829
[3/3, 330/7500] Training Loss: 1.2111971139907838
[3/3, 340/7500] Training Loss: 1.2363535284996032
[3/3, 350/7500] Training Loss: 1.2058839797973633
[3/3, 360/7500] Training Loss: 1.2436188459396362
[3/3, 370/7500] Training Loss: 1.2592698574066161
[3/3, 380/7500] Training Loss: 1.1799694120883941
[3/3, 390/7500] Training Loss: 1.2363661289215089
[3/3, 400/7500] Training Loss: 1.167930507659912
[3/3, 410/7500] Training Loss: 1.2268481492996215
[3/3, 420/7500] Training Loss: 1.2952338337898255
[3/3, 430/7500] Training Loss: 1.225166380405426
[3/3, 440/7500] Training Loss: 1.216954880952835
[3/3, 450/7500] Training Loss: 1.2308741807937622
[3/3, 460/7500] Training Loss: 1.1913173854351045
[3/3, 470/7500] Training Loss: 1.3108363032341004
[3/3, 480/7500] Training Loss: 1.181846559047699
[3/3, 490/7500] Training Loss: 1.1997926950454711
[3/3, 500/7500] Training Loss: 1.1695976614952088
[3/3, 510/7500] Training Loss: 1.1963214159011841
[3/3, 520/7500] Training Loss: 1.208919060230255
[3/3, 530/7500] Training Loss: 1.2257113337516785
[3/3, 540/7500] Training Loss: 1.1850977540016174
[3/3, 550/7500] Training Loss: 1.2333224296569825
[3/3, 560/7500] Training Loss: 1.263029897212982
[3/3, 570/7500] Training Loss: 1.1809985280036925
[3/3, 580/7500] Training Loss: 1.1835877299308777
[3/3, 590/7500] Training Loss: 1.1413413882255554
[3/3, 600/7500] Training Loss: 1.243787306547165
[3/3, 610/7500] Training Loss: 1.2412230849266053
[3/3, 620/7500] Training Loss: 1.3127941608428955
[3/3, 630/7500] Training Loss: 1.2271321296691895
[3/3, 640/7500] Training Loss: 1.3220395207405091
[3/3, 650/7500] Training Loss: 1.2053759336471557
[3/3, 660/7500] Training Loss: 1.2551142513751983
[3/3, 670/7500] Training Loss: 1.2705823600292205
[3/3, 680/7500] Training Loss: 1.2417605519294739
[3/3, 690/7500] Training Loss: 1.2914489269256593
[3/3, 700/7500] Training Loss: 1.1964792490005494
[3/3, 710/7500] Training Loss: 1.1821404457092286
[3/3, 720/7500] Training Loss: 1.2192616641521454
[3/3, 730/7500] Training Loss: 1.2521034955978394
[3/3, 740/7500] Training Loss: 1.268829882144928
[3/3, 750/7500] Training Loss: 1.2143942356109618
[3/3, 760/7500] Training Loss: 1.2701976656913758
[3/3, 770/7500] Training Loss: 1.1905901253223419
[3/3, 780/7500] Training Loss: 1.2067800462245941
[3/3, 790/7500] Training Loss: 1.2438812851905823
[3/3, 800/7500] Training Loss: 1.2418844938278197
[3/3, 810/7500] Training Loss: 1.2472758531570434
[3/3, 820/7500] Training Loss: 1.1799320161342621
[3/3, 830/7500] Training Loss: 1.1768995523452759
[3/3, 840/7500] Training Loss: 1.1809722065925599
[3/3, 850/7500] Training Loss: 1.2634529888629913
[3/3, 860/7500] Training Loss: 1.1833099007606507
[3/3, 870/7500] Training Loss: 1.194377851486206
[3/3, 880/7500] Training Loss: 1.189933681488037
[3/3, 890/7500] Training Loss: 1.2014618277549745
[3/3, 900/7500] Training Loss: 1.187169885635376
[3/3, 910/7500] Training Loss: 1.281481146812439
[3/3, 920/7500] Training Loss: 1.2000584363937379
[3/3, 930/7500] Training Loss: 1.2797688364982605
[3/3, 940/7500] Training Loss: 1.2255811989307404
[3/3, 950/7500] Training Loss: 1.2032524228096009
[3/3, 960/7500] Training Loss: 1.1538342773914336
[3/3, 970/7500] Training Loss: 1.2660870671272277
[3/3, 980/7500] Training Loss: 1.2468510866165161
[3/3, 990/7500] Training Loss: 1.190865170955658
[3/3, 1000/7500] Training Loss: 1.2346810936927795
[3/3, 1010/7500] Training Loss: 1.2201776385307312
[3/3, 1020/7500] Training Loss: 1.2185183763504028
[3/3, 1030/7500] Training Loss: 1.2787933230400086
[3/3, 1040/7500] Training Loss: 1.1564733147621156
[3/3, 1050/7500] Training Loss: 1.1812909185886382
[3/3, 1060/7500] Training Loss: 1.2163895308971404
[3/3, 1070/7500] Training Loss: 1.1904148757457733
[3/3, 1080/7500] Training Loss: 1.2879537045955658
[3/3, 1090/7500] Training Loss: 1.1803274869918823
[3/3, 1100/7500] Training Loss: 1.144784390926361
[3/3, 1110/7500] Training Loss: 1.2527259290218353
[3/3, 1120/7500] Training Loss: 1.2443527102470398
[3/3, 1130/7500] Training Loss: 1.2321514248847962
[3/3, 1140/7500] Training Loss: 1.2331545948982239
[3/3, 1150/7500] Training Loss: 1.1931945025920867
[3/3, 1160/7500] Training Loss: 1.138452935218811
[3/3, 1170/7500] Training Loss: 1.2212916314601898
[3/3, 1180/7500] Training Loss: 1.1983216881752015
[3/3, 1190/7500] Training Loss: 1.258216631412506
[3/3, 1200/7500] Training Loss: 1.2586995720863343
[3/3, 1210/7500] Training Loss: 1.2016070067882538
[3/3, 1220/7500] Training Loss: 1.1913508594036102
[3/3, 1230/7500] Training Loss: 1.1650921165943147
[3/3, 1240/7500] Training Loss: 1.2148792266845703
[3/3, 1250/7500] Training Loss: 1.1995129942893983
[3/3, 1260/7500] Training Loss: 1.1566714346408844
[3/3, 1270/7500] Training Loss: 1.1761902928352357
[3/3, 1280/7500] Training Loss: 1.229919970035553
[3/3, 1290/7500] Training Loss: 1.177862536907196
[3/3, 1300/7500] Training Loss: 1.2724768817424774
[3/3, 1310/7500] Training Loss: 1.1846579253673553
[3/3, 1320/7500] Training Loss: 1.2868342518806457
[3/3, 1330/7500] Training Loss: 1.2151522159576416
[3/3, 1340/7500] Training Loss: 1.217110002040863
[3/3, 1350/7500] Training Loss: 1.1932202935218812
[3/3, 1360/7500] Training Loss: 1.2295267105102539
[3/3, 1370/7500] Training Loss: 1.1426343381404878
[3/3, 1380/7500] Training Loss: 1.2224790811538697
[3/3, 1390/7500] Training Loss: 1.2270880222320557
[3/3, 1400/7500] Training Loss: 1.2352699637413025
[3/3, 1410/7500] Training Loss: 1.2473761439323425
[3/3, 1420/7500] Training Loss: 1.1439689159393311
[3/3, 1430/7500] Training Loss: 1.2157980799674988
[3/3, 1440/7500] Training Loss: 1.1997974872589112
[3/3, 1450/7500] Training Loss: 1.2326053082942963
[3/3, 1460/7500] Training Loss: 1.2695376515388488
[3/3, 1470/7500] Training Loss: 1.2695675432682036
[3/3, 1480/7500] Training Loss: 1.155222088098526
[3/3, 1490/7500] Training Loss: 1.158903843164444
[3/3, 1500/7500] Training Loss: 1.2406474828720093
[3/3, 1510/7500] Training Loss: 1.2140231549739837
[3/3, 1520/7500] Training Loss: 1.2109921336174012
[3/3, 1530/7500] Training Loss: 1.1870388090610504
[3/3, 1540/7500] Training Loss: 1.2420914351940155
[3/3, 1550/7500] Training Loss: 1.202228271961212
[3/3, 1560/7500] Training Loss: 1.2147200107574463
[3/3, 1570/7500] Training Loss: 1.221642220020294
[3/3, 1580/7500] Training Loss: 1.178034394979477
[3/3, 1590/7500] Training Loss: 1.2121505260467529
[3/3, 1600/7500] Training Loss: 1.2007951378822326
[3/3, 1610/7500] Training Loss: 1.2403969526290894
[3/3, 1620/7500] Training Loss: 1.222034513950348
[3/3, 1630/7500] Training Loss: 1.2675079226493835
[3/3, 1640/7500] Training Loss: 1.2500273704528808
[3/3, 1650/7500] Training Loss: 1.245357382297516
[3/3, 1660/7500] Training Loss: 1.1865763068199158
[3/3, 1670/7500] Training Loss: 1.2501103699207305
[3/3, 1680/7500] Training Loss: 1.2697412848472596
[3/3, 1690/7500] Training Loss: 1.1623226821422576
[3/3, 1700/7500] Training Loss: 1.2469209551811218
[3/3, 1710/7500] Training Loss: 1.2102809190750121
[3/3, 1720/7500] Training Loss: 1.186204695701599
[3/3, 1730/7500] Training Loss: 1.1608247458934784
[3/3, 1740/7500] Training Loss: 1.2177558422088623
[3/3, 1750/7500] Training Loss: 1.2184540629386902
[3/3, 1760/7500] Training Loss: 1.2379709362983704
[3/3, 1770/7500] Training Loss: 1.2817430019378662
[3/3, 1780/7500] Training Loss: 1.227672016620636
[3/3, 1790/7500] Training Loss: 1.1743936896324159
[3/3, 1800/7500] Training Loss: 1.1837696313858033
[3/3, 1810/7500] Training Loss: 1.1884491801261903
[3/3, 1820/7500] Training Loss: 1.2264335036277771
[3/3, 1830/7500] Training Loss: 1.19757479429245
[3/3, 1840/7500] Training Loss: 1.1913950800895692
[3/3, 1850/7500] Training Loss: 1.1313404083251952
[3/3, 1860/7500] Training Loss: 1.2073217332363129
[3/3, 1870/7500] Training Loss: 1.2446596026420593
[3/3, 1880/7500] Training Loss: 1.1760209441184997
[3/3, 1890/7500] Training Loss: 1.2441056609153747
[3/3, 1900/7500] Training Loss: 1.2486526370048523
[3/3, 1910/7500] Training Loss: 1.2299777448177338
[3/3, 1920/7500] Training Loss: 1.21781005859375
[3/3, 1930/7500] Training Loss: 1.2148256301879883
[3/3, 1940/7500] Training Loss: 1.316447389125824
[3/3, 1950/7500] Training Loss: 1.1922689378261566
[3/3, 1960/7500] Training Loss: 1.2419852197170258
[3/3, 1970/7500] Training Loss: 1.1808449745178222
[3/3, 1980/7500] Training Loss: 1.2051775932312012
[3/3, 1990/7500] Training Loss: 1.19427889585495
[3/3, 2000/7500] Training Loss: 1.256213366985321
[3/3, 2010/7500] Training Loss: 1.216629958152771
[3/3, 2020/7500] Training Loss: 1.1998620867729186
[3/3, 2030/7500] Training Loss: 1.2646481454372407
[3/3, 2040/7500] Training Loss: 1.329019033908844
[3/3, 2050/7500] Training Loss: 1.2226229190826416
[3/3, 2060/7500] Training Loss: 1.1907656908035278
[3/3, 2070/7500] Training Loss: 1.2131041824817657
[3/3, 2080/7500] Training Loss: 1.1772100389003755
[3/3, 2090/7500] Training Loss: 1.2508007764816285
[3/3, 2100/7500] Training Loss: 1.2141342878341674
[3/3, 2110/7500] Training Loss: 1.2355416655540465
[3/3, 2120/7500] Training Loss: 1.2840429663658142
[3/3, 2130/7500] Training Loss: 1.1729014277458192
[3/3, 2140/7500] Training Loss: 1.2409279465675354
[3/3, 2150/7500] Training Loss: 1.2036985516548158
[3/3, 2160/7500] Training Loss: 1.1469988167285918
[3/3, 2170/7500] Training Loss: 1.2116031527519227
[3/3, 2180/7500] Training Loss: 1.2278075456619262
[3/3, 2190/7500] Training Loss: 1.216014552116394
[3/3, 2200/7500] Training Loss: 1.237067198753357
[3/3, 2210/7500] Training Loss: 1.2288664817810058
[3/3, 2220/7500] Training Loss: 1.1525855362415314
[3/3, 2230/7500] Training Loss: 1.2297556519508361
[3/3, 2240/7500] Training Loss: 1.1727052986621858
[3/3, 2250/7500] Training Loss: 1.2091385960578918
[3/3, 2260/7500] Training Loss: 1.2362537503242492
[3/3, 2270/7500] Training Loss: 1.1471931338310242
[3/3, 2280/7500] Training Loss: 1.2289672255516053
[3/3, 2290/7500] Training Loss: 1.1925759434700012
[3/3, 2300/7500] Training Loss: 1.0987451434135438
[3/3, 2310/7500] Training Loss: 1.2556387662887574
[3/3, 2320/7500] Training Loss: 1.2044935166835784
[3/3, 2330/7500] Training Loss: 1.224846315383911
[3/3, 2340/7500] Training Loss: 1.3000703930854798
[3/3, 2350/7500] Training Loss: 1.2460558414459229
[3/3, 2360/7500] Training Loss: 1.2249071955680848
[3/3, 2370/7500] Training Loss: 1.2106275260448456
[3/3, 2380/7500] Training Loss: 1.238566768169403
[3/3, 2390/7500] Training Loss: 1.1883142828941344
[3/3, 2400/7500] Training Loss: 1.2103704333305358
[3/3, 2410/7500] Training Loss: 1.2875276863574983
[3/3, 2420/7500] Training Loss: 1.1478821516036988
[3/3, 2430/7500] Training Loss: 1.278562033176422
[3/3, 2440/7500] Training Loss: 1.2618407726287841
[3/3, 2450/7500] Training Loss: 1.1758320569992065
[3/3, 2460/7500] Training Loss: 1.2045027911663055
[3/3, 2470/7500] Training Loss: 1.1826496958732604
[3/3, 2480/7500] Training Loss: 1.2740028023719787
[3/3, 2490/7500] Training Loss: 1.1681851685047149
[3/3, 2500/7500] Training Loss: 1.163671064376831
[3/3, 2510/7500] Training Loss: 1.2305322527885436
[3/3, 2520/7500] Training Loss: 1.178729808330536
[3/3, 2530/7500] Training Loss: 1.198447012901306
[3/3, 2540/7500] Training Loss: 1.1955757856369018
[3/3, 2550/7500] Training Loss: 1.2529500603675843
[3/3, 2560/7500] Training Loss: 1.270130181312561
[3/3, 2570/7500] Training Loss: 1.2594697594642639
[3/3, 2580/7500] Training Loss: 1.2642179608345032
[3/3, 2590/7500] Training Loss: 1.271222972869873
[3/3, 2600/7500] Training Loss: 1.207081425189972
[3/3, 2610/7500] Training Loss: 1.2331796050071717
[3/3, 2620/7500] Training Loss: 1.216258454322815
[3/3, 2630/7500] Training Loss: 1.2497165322303772
[3/3, 2640/7500] Training Loss: 1.1308976471424104
[3/3, 2650/7500] Training Loss: 1.2534877061843872
[3/3, 2660/7500] Training Loss: 1.1696987390518188
[3/3, 2670/7500] Training Loss: 1.1578014492988586
[3/3, 2680/7500] Training Loss: 1.240524172782898
[3/3, 2690/7500] Training Loss: 1.2837915301322937
[3/3, 2700/7500] Training Loss: 1.178936231136322
[3/3, 2710/7500] Training Loss: 1.2267120957374573
[3/3, 2720/7500] Training Loss: 1.2118581533432007
[3/3, 2730/7500] Training Loss: 1.3085830807685852
[3/3, 2740/7500] Training Loss: 1.1925463914871215
[3/3, 2750/7500] Training Loss: 1.1446851670742035
[3/3, 2760/7500] Training Loss: 1.3041156768798827
[3/3, 2770/7500] Training Loss: 1.1265795230865479
[3/3, 2780/7500] Training Loss: 1.192154598236084
[3/3, 2790/7500] Training Loss: 1.2074129939079286
[3/3, 2800/7500] Training Loss: 1.221587586402893
[3/3, 2810/7500] Training Loss: 1.2963519811630249
[3/3, 2820/7500] Training Loss: 1.1368266463279724
[3/3, 2830/7500] Training Loss: 1.2146866738796234
[3/3, 2840/7500] Training Loss: 1.108141428232193
[3/3, 2850/7500] Training Loss: 1.2430125236511231
[3/3, 2860/7500] Training Loss: 1.1116304755210877
[3/3, 2870/7500] Training Loss: 1.2244893193244935
[3/3, 2880/7500] Training Loss: 1.2585317015647888
[3/3, 2890/7500] Training Loss: 1.253394329547882
[3/3, 2900/7500] Training Loss: 1.2625629901885986
[3/3, 2910/7500] Training Loss: 1.2286264061927796
[3/3, 2920/7500] Training Loss: 1.2110549867153169
[3/3, 2930/7500] Training Loss: 1.1825869023799895
[3/3, 2940/7500] Training Loss: 1.1921008706092835
[3/3, 2950/7500] Training Loss: 1.1601707637310028
[3/3, 2960/7500] Training Loss: 1.177073895931244
[3/3, 2970/7500] Training Loss: 1.2232455730438232
[3/3, 2980/7500] Training Loss: 1.252093768119812
[3/3, 2990/7500] Training Loss: 1.3073988556861877
[3/3, 3000/7500] Training Loss: 1.200109350681305
[3/3, 3010/7500] Training Loss: 1.2517486691474915
[3/3, 3020/7500] Training Loss: 1.143598771095276
[3/3, 3030/7500] Training Loss: 1.2024591326713563
[3/3, 3040/7500] Training Loss: 1.2821618556976317
[3/3, 3050/7500] Training Loss: 1.294243848323822
[3/3, 3060/7500] Training Loss: 1.1616243541240692
[3/3, 3070/7500] Training Loss: 1.1921602070331574
[3/3, 3080/7500] Training Loss: 1.3222109436988831
[3/3, 3090/7500] Training Loss: 1.1504054367542267
[3/3, 3100/7500] Training Loss: 1.12880779504776
[3/3, 3110/7500] Training Loss: 1.2219643414020538
[3/3, 3120/7500] Training Loss: 1.2774504661560058
[3/3, 3130/7500] Training Loss: 1.2116415739059447
[3/3, 3140/7500] Training Loss: 1.1439811825752257
[3/3, 3150/7500] Training Loss: 1.2335184216499329
[3/3, 3160/7500] Training Loss: 1.243812620639801
[3/3, 3170/7500] Training Loss: 1.2147067546844483
[3/3, 3180/7500] Training Loss: 1.23811776638031
[3/3, 3190/7500] Training Loss: 1.2957443475723267
[3/3, 3200/7500] Training Loss: 1.2081018567085267
[3/3, 3210/7500] Training Loss: 1.2151704549789428
[3/3, 3220/7500] Training Loss: 1.1504694521427155
[3/3, 3230/7500] Training Loss: 1.2487720549106598
[3/3, 3240/7500] Training Loss: 1.1812930226325988
[3/3, 3250/7500] Training Loss: 1.203500247001648
[3/3, 3260/7500] Training Loss: 1.2563003242015838
[3/3, 3270/7500] Training Loss: 1.288788878917694
[3/3, 3280/7500] Training Loss: 1.1767109811306
[3/3, 3290/7500] Training Loss: 1.168718147277832
[3/3, 3300/7500] Training Loss: 1.2533372044563293
[3/3, 3310/7500] Training Loss: 1.2307782530784608
[3/3, 3320/7500] Training Loss: 1.2876977920532227
[3/3, 3330/7500] Training Loss: 1.2063167452812196
[3/3, 3340/7500] Training Loss: 1.2103648781776428
[3/3, 3350/7500] Training Loss: 1.1783377051353454
[3/3, 3360/7500] Training Loss: 1.218161904811859
[3/3, 3370/7500] Training Loss: 1.220833683013916
[3/3, 3380/7500] Training Loss: 1.2728328347206115
[3/3, 3390/7500] Training Loss: 1.1782927751541137
[3/3, 3400/7500] Training Loss: 1.2469833850860597
[3/3, 3410/7500] Training Loss: 1.1908464789390565
[3/3, 3420/7500] Training Loss: 1.2348995745182036
[3/3, 3430/7500] Training Loss: 1.2001870036125184
[3/3, 3440/7500] Training Loss: 1.2077109813690186
[3/3, 3450/7500] Training Loss: 1.2107099175453186
[3/3, 3460/7500] Training Loss: 1.2632740616798401
[3/3, 3470/7500] Training Loss: 1.2775516152381896
[3/3, 3480/7500] Training Loss: 1.2085852384567262
[3/3, 3490/7500] Training Loss: 1.2184367775917053
[3/3, 3500/7500] Training Loss: 1.2192932486534118
[3/3, 3510/7500] Training Loss: 1.21479674577713
[3/3, 3520/7500] Training Loss: 1.3111499786376952
[3/3, 3530/7500] Training Loss: 1.1578538298606873
[3/3, 3540/7500] Training Loss: 1.1835438847541808
[3/3, 3550/7500] Training Loss: 1.1383237361907959
[3/3, 3560/7500] Training Loss: 1.1555156767368318
[3/3, 3570/7500] Training Loss: 1.2022018134593964
[3/3, 3580/7500] Training Loss: 1.1253707647323608
[3/3, 3590/7500] Training Loss: 1.2510600328445434
[3/3, 3600/7500] Training Loss: 1.2598042964935303
[3/3, 3610/7500] Training Loss: 1.1515514135360718
[3/3, 3620/7500] Training Loss: 1.1687825918197632
[3/3, 3630/7500] Training Loss: 1.235255777835846
[3/3, 3640/7500] Training Loss: 1.2183902502059936
[3/3, 3650/7500] Training Loss: 1.205698585510254
[3/3, 3660/7500] Training Loss: 1.1850264906883239
[3/3, 3670/7500] Training Loss: 1.2758848547935486
[3/3, 3680/7500] Training Loss: 1.2879384279251098
[3/3, 3690/7500] Training Loss: 1.2557646870613097
[3/3, 3700/7500] Training Loss: 1.285444337129593
[3/3, 3710/7500] Training Loss: 1.2974541664123536
[3/3, 3720/7500] Training Loss: 1.2109200477600097
[3/3, 3730/7500] Training Loss: 1.2383386313915252
[3/3, 3740/7500] Training Loss: 1.2271690130233766
[3/3, 3750/7500] Training Loss: 1.2544744610786438
[3/3, 3760/7500] Training Loss: 1.2637705326080322
[3/3, 3770/7500] Training Loss: 1.2421828150749206
[3/3, 3780/7500] Training Loss: 1.2205501794815063
[3/3, 3790/7500] Training Loss: 1.1448624789714814
[3/3, 3800/7500] Training Loss: 1.3254119992256164
[3/3, 3810/7500] Training Loss: 1.1629987835884095
[3/3, 3820/7500] Training Loss: 1.2560599446296692
[3/3, 3830/7500] Training Loss: 1.2652776122093201
[3/3, 3840/7500] Training Loss: 1.2866943717002868
[3/3, 3850/7500] Training Loss: 1.231964910030365
[3/3, 3860/7500] Training Loss: 1.1683381259441377
[3/3, 3870/7500] Training Loss: 1.1800201296806336
[3/3, 3880/7500] Training Loss: 1.2038154006004333
[3/3, 3890/7500] Training Loss: 1.2736706733703613
[3/3, 3900/7500] Training Loss: 1.2295533657073974
[3/3, 3910/7500] Training Loss: 1.1919726729393005
[3/3, 3920/7500] Training Loss: 1.2710708975791931
[3/3, 3930/7500] Training Loss: 1.2116855919361114
[3/3, 3940/7500] Training Loss: 1.207915985584259
[3/3, 3950/7500] Training Loss: 1.2332609295845032
[3/3, 3960/7500] Training Loss: 1.2538561820983887
[3/3, 3970/7500] Training Loss: 1.2344361960887908
[3/3, 3980/7500] Training Loss: 1.1550636172294617
[3/3, 3990/7500] Training Loss: 1.2202332377433778
[3/3, 4000/7500] Training Loss: 1.2295812726020814
[3/3, 4010/7500] Training Loss: 1.2643854856491088
[3/3, 4020/7500] Training Loss: 1.223311460018158
[3/3, 4030/7500] Training Loss: 1.2301250100135803
[3/3, 4040/7500] Training Loss: 1.2635721206665038
[3/3, 4050/7500] Training Loss: 1.1906731367111205
[3/3, 4060/7500] Training Loss: 1.2666141867637635
[3/3, 4070/7500] Training Loss: 1.2019839763641358
[3/3, 4080/7500] Training Loss: 1.285809862613678
[3/3, 4090/7500] Training Loss: 1.252665823698044
[3/3, 4100/7500] Training Loss: 1.2290653467178345
[3/3, 4110/7500] Training Loss: 1.288051187992096
[3/3, 4120/7500] Training Loss: 1.2222447752952577
[3/3, 4130/7500] Training Loss: 1.2565310597419739
[3/3, 4140/7500] Training Loss: 1.2527908205986023
[3/3, 4150/7500] Training Loss: 1.251706612110138
[3/3, 4160/7500] Training Loss: 1.198454749584198
[3/3, 4170/7500] Training Loss: 1.2009593844413757
[3/3, 4180/7500] Training Loss: 1.2118112683296203
[3/3, 4190/7500] Training Loss: 1.2436973214149476
[3/3, 4200/7500] Training Loss: 1.2788627982139587
[3/3, 4210/7500] Training Loss: 1.1484994649887086
[3/3, 4220/7500] Training Loss: 1.2162781894207
[3/3, 4230/7500] Training Loss: 1.222135841846466
[3/3, 4240/7500] Training Loss: 1.2314067125320434
[3/3, 4250/7500] Training Loss: 1.2465913414955139
[3/3, 4260/7500] Training Loss: 1.2223845601081849
[3/3, 4270/7500] Training Loss: 1.179490727186203
[3/3, 4280/7500] Training Loss: 1.1893710017204284
[3/3, 4290/7500] Training Loss: 1.3215542554855346
[3/3, 4300/7500] Training Loss: 1.2240213632583619
[3/3, 4310/7500] Training Loss: 1.226236057281494
[3/3, 4320/7500] Training Loss: 1.222241187095642
[3/3, 4330/7500] Training Loss: 1.2290903687477113
[3/3, 4340/7500] Training Loss: 1.2567781567573548
[3/3, 4350/7500] Training Loss: 1.1957063615322112
[3/3, 4360/7500] Training Loss: 1.2309646785259247
[3/3, 4370/7500] Training Loss: 1.1376658499240875
[3/3, 4380/7500] Training Loss: 1.1649395108222962
[3/3, 4390/7500] Training Loss: 1.290651512145996
[3/3, 4400/7500] Training Loss: 1.2035052955150605
[3/3, 4410/7500] Training Loss: 1.240670132637024
[3/3, 4420/7500] Training Loss: 1.313683807849884
[3/3, 4430/7500] Training Loss: 1.1942769169807435
[3/3, 4440/7500] Training Loss: 1.3202590823173523
[3/3, 4450/7500] Training Loss: 1.2345184445381165
[3/3, 4460/7500] Training Loss: 1.2342850625514985
[3/3, 4470/7500] Training Loss: 1.2284643650054932
[3/3, 4480/7500] Training Loss: 1.2476626992225648
[3/3, 4490/7500] Training Loss: 1.2561791777610778
[3/3, 4500/7500] Training Loss: 1.249189853668213
[3/3, 4510/7500] Training Loss: 1.1810750484466552
[3/3, 4520/7500] Training Loss: 1.2309280276298522
[3/3, 4530/7500] Training Loss: 1.1845639765262603
[3/3, 4540/7500] Training Loss: 1.266768705844879
[3/3, 4550/7500] Training Loss: 1.1875170826911927
[3/3, 4560/7500] Training Loss: 1.176173460483551
[3/3, 4570/7500] Training Loss: 1.1823669731616975
[3/3, 4580/7500] Training Loss: 1.1563021421432496
[3/3, 4590/7500] Training Loss: 1.1813833236694335
[3/3, 4600/7500] Training Loss: 1.2294410109519958
[3/3, 4610/7500] Training Loss: 1.1593788683414459
[3/3, 4620/7500] Training Loss: 1.2602673053741456
[3/3, 4630/7500] Training Loss: 1.1396498024463653
[3/3, 4640/7500] Training Loss: 1.2150853872299194
[3/3, 4650/7500] Training Loss: 1.2408897161483765
[3/3, 4660/7500] Training Loss: 1.217768931388855
[3/3, 4670/7500] Training Loss: 1.1596883356571197
[3/3, 4680/7500] Training Loss: 1.2341510772705078
[3/3, 4690/7500] Training Loss: 1.2300018668174744
[3/3, 4700/7500] Training Loss: 1.2482991456985473
[3/3, 4710/7500] Training Loss: 1.2610046803951263
[3/3, 4720/7500] Training Loss: 1.1999852538108826
[3/3, 4730/7500] Training Loss: 1.1992927074432373
[3/3, 4740/7500] Training Loss: 1.227452576160431
[3/3, 4750/7500] Training Loss: 1.1753654837608338
[3/3, 4760/7500] Training Loss: 1.2338076055049896
[3/3, 4770/7500] Training Loss: 1.2062726259231566
[3/3, 4780/7500] Training Loss: 1.1856564164161683
[3/3, 4790/7500] Training Loss: 1.214274513721466
[3/3, 4800/7500] Training Loss: 1.248570454120636
[3/3, 4810/7500] Training Loss: 1.27194344997406
[3/3, 4820/7500] Training Loss: 1.2413779258728028
[3/3, 4830/7500] Training Loss: 1.1926709353923797
[3/3, 4840/7500] Training Loss: 1.3058080673217773
[3/3, 4850/7500] Training Loss: 1.1636320054531097
[3/3, 4860/7500] Training Loss: 1.179185140132904
[3/3, 4870/7500] Training Loss: 1.251920461654663
[3/3, 4880/7500] Training Loss: 1.2425649106502532
[3/3, 4890/7500] Training Loss: 1.204461807012558
[3/3, 4900/7500] Training Loss: 1.2605457067489625
[3/3, 4910/7500] Training Loss: 1.228649878501892
[3/3, 4920/7500] Training Loss: 1.1972234129905701
[3/3, 4930/7500] Training Loss: 1.2222860693931579
[3/3, 4940/7500] Training Loss: 1.1819016873836516
[3/3, 4950/7500] Training Loss: 1.2014081001281738
[3/3, 4960/7500] Training Loss: 1.3145220518112182
[3/3, 4970/7500] Training Loss: 1.2527841091156007
[3/3, 4980/7500] Training Loss: 1.2859009146690368
[3/3, 4990/7500] Training Loss: 1.188734805583954
[3/3, 5000/7500] Training Loss: 1.2916439294815063
[3/3, 5010/7500] Training Loss: 1.1957427859306335
[3/3, 5020/7500] Training Loss: 1.1737265825271606
[3/3, 5030/7500] Training Loss: 1.2205040216445924
[3/3, 5040/7500] Training Loss: 1.2391809701919556
[3/3, 5050/7500] Training Loss: 1.1832386195659637
[3/3, 5060/7500] Training Loss: 1.2054982662200928
[3/3, 5070/7500] Training Loss: 1.1970915079116822
[3/3, 5080/7500] Training Loss: 1.2056553602218627
[3/3, 5090/7500] Training Loss: 1.2758052706718446
[3/3, 5100/7500] Training Loss: 1.2853866457939147
[3/3, 5110/7500] Training Loss: 1.251023769378662
[3/3, 5120/7500] Training Loss: 1.2749289393424987
[3/3, 5130/7500] Training Loss: 1.2631943464279174
[3/3, 5140/7500] Training Loss: 1.2162534952163697
[3/3, 5150/7500] Training Loss: 1.2609023213386537
[3/3, 5160/7500] Training Loss: 1.2772984743118285
[3/3, 5170/7500] Training Loss: 1.2237003684043883
[3/3, 5180/7500] Training Loss: 1.26566823720932
[3/3, 5190/7500] Training Loss: 1.2601451873779297
[3/3, 5200/7500] Training Loss: 1.2248309552669525
[3/3, 5210/7500] Training Loss: 1.266979742050171
[3/3, 5220/7500] Training Loss: 1.2764200448989869
[3/3, 5230/7500] Training Loss: 1.2745789766311646
[3/3, 5240/7500] Training Loss: 1.2855259418487548
[3/3, 5250/7500] Training Loss: 1.1916483640670776
[3/3, 5260/7500] Training Loss: 1.1327584266662598
[3/3, 5270/7500] Training Loss: 1.2176785111427306
[3/3, 5280/7500] Training Loss: 1.1573141753673553
[3/3, 5290/7500] Training Loss: 1.2176940441131592
[3/3, 5300/7500] Training Loss: 1.1835276901721954
[3/3, 5310/7500] Training Loss: 1.2886425495147704
[3/3, 5320/7500] Training Loss: 1.1844453930854797
[3/3, 5330/7500] Training Loss: 1.2470920443534852
[3/3, 5340/7500] Training Loss: 1.234691071510315
[3/3, 5350/7500] Training Loss: 1.2709208011627198
[3/3, 5360/7500] Training Loss: 1.2171516418457031
[3/3, 5370/7500] Training Loss: 1.1034991800785066
[3/3, 5380/7500] Training Loss: 1.2524450540542602
[3/3, 5390/7500] Training Loss: 1.2189700424671173
[3/3, 5400/7500] Training Loss: 1.2016181588172912
[3/3, 5410/7500] Training Loss: 1.2156077742576599
[3/3, 5420/7500] Training Loss: 1.1761689066886902
[3/3, 5430/7500] Training Loss: 1.2793333411216736
[3/3, 5440/7500] Training Loss: 1.2126285672187804
[3/3, 5450/7500] Training Loss: 1.3028160095214845
[3/3, 5460/7500] Training Loss: 1.2509411096572876
[3/3, 5470/7500] Training Loss: 1.2519296765327455
[3/3, 5480/7500] Training Loss: 1.1886101901531219
[3/3, 5490/7500] Training Loss: 1.216915762424469
[3/3, 5500/7500] Training Loss: 1.2217090368270873
[3/3, 5510/7500] Training Loss: 1.1881632924079895
[3/3, 5520/7500] Training Loss: 1.2056828379631042
[3/3, 5530/7500] Training Loss: 1.2321483969688416
[3/3, 5540/7500] Training Loss: 1.3087252140045167
[3/3, 5550/7500] Training Loss: 1.2519268155097962
[3/3, 5560/7500] Training Loss: 1.1140590250492095
[3/3, 5570/7500] Training Loss: 1.2660261392593384
[3/3, 5580/7500] Training Loss: 1.2618727684020996
[3/3, 5590/7500] Training Loss: 1.2514686703681945
[3/3, 5600/7500] Training Loss: 1.2484377145767211
[3/3, 5610/7500] Training Loss: 1.192987471818924
[3/3, 5620/7500] Training Loss: 1.2508136749267578
[3/3, 5630/7500] Training Loss: 1.238735669851303
[3/3, 5640/7500] Training Loss: 1.2419552564620973
[3/3, 5650/7500] Training Loss: 1.1882079720497132
[3/3, 5660/7500] Training Loss: 1.2233086228370667
[3/3, 5670/7500] Training Loss: 1.1839554607868195
[3/3, 5680/7500] Training Loss: 1.2185355067253112
[3/3, 5690/7500] Training Loss: 1.207714855670929
[3/3, 5700/7500] Training Loss: 1.2594146728515625
[3/3, 5710/7500] Training Loss: 1.1606660604476928
[3/3, 5720/7500] Training Loss: 1.2316246151924133
[3/3, 5730/7500] Training Loss: 1.1616993367671966
[3/3, 5740/7500] Training Loss: 1.2431064903736115
[3/3, 5750/7500] Training Loss: 1.2833333492279053
[3/3, 5760/7500] Training Loss: 1.2262441039085388
[3/3, 5770/7500] Training Loss: 1.2245391964912415
[3/3, 5780/7500] Training Loss: 1.3050795197486877
[3/3, 5790/7500] Training Loss: 1.197378659248352
[3/3, 5800/7500] Training Loss: 1.2222049832344055
[3/3, 5810/7500] Training Loss: 1.166413378715515
[3/3, 5820/7500] Training Loss: 1.1477974712848664
[3/3, 5830/7500] Training Loss: 1.1778234124183655
[3/3, 5840/7500] Training Loss: 1.2598367571830749
[3/3, 5850/7500] Training Loss: 1.1835810780525207
[3/3, 5860/7500] Training Loss: 1.2212930500507355
[3/3, 5870/7500] Training Loss: 1.3031295776367187
[3/3, 5880/7500] Training Loss: 1.2004480421543122
[3/3, 5890/7500] Training Loss: 1.2682198286056519
[3/3, 5900/7500] Training Loss: 1.1824918806552887
[3/3, 5910/7500] Training Loss: 1.2587501406669617
[3/3, 5920/7500] Training Loss: 1.2171161770820618
[3/3, 5930/7500] Training Loss: 1.2318221926689148
[3/3, 5940/7500] Training Loss: 1.2216989278793335
[3/3, 5950/7500] Training Loss: 1.220459794998169
[3/3, 5960/7500] Training Loss: 1.238811433315277
[3/3, 5970/7500] Training Loss: 1.1502856731414794
[3/3, 5980/7500] Training Loss: 1.2631114959716796
[3/3, 5990/7500] Training Loss: 1.1961893439292908
[3/3, 6000/7500] Training Loss: 1.2179108023643495
[3/3, 6010/7500] Training Loss: 1.2777515530586243
[3/3, 6020/7500] Training Loss: 1.2216781854629517
[3/3, 6030/7500] Training Loss: 1.217374473810196
[3/3, 6040/7500] Training Loss: 1.1457636177539825
[3/3, 6050/7500] Training Loss: 1.2483407378196716
[3/3, 6060/7500] Training Loss: 1.1815598726272583
[3/3, 6070/7500] Training Loss: 1.204646933078766
[3/3, 6080/7500] Training Loss: 1.2508921384811402
[3/3, 6090/7500] Training Loss: 1.2634605407714843
[3/3, 6100/7500] Training Loss: 1.2465170264244079
[3/3, 6110/7500] Training Loss: 1.1671752691268922
[3/3, 6120/7500] Training Loss: 1.2156873941421509
[3/3, 6130/7500] Training Loss: 1.1927384376525878
[3/3, 6140/7500] Training Loss: 1.2208012223243714
[3/3, 6150/7500] Training Loss: 1.2407517552375793
[3/3, 6160/7500] Training Loss: 1.1768309116363525
[3/3, 6170/7500] Training Loss: 1.2291870951652526
[3/3, 6180/7500] Training Loss: 1.1820889711380005
[3/3, 6190/7500] Training Loss: 1.244888424873352
[3/3, 6200/7500] Training Loss: 1.2746977925300598
[3/3, 6210/7500] Training Loss: 1.1653026282787322
[3/3, 6220/7500] Training Loss: 1.2816755652427674
[3/3, 6230/7500] Training Loss: 1.2090660333633423
[3/3, 6240/7500] Training Loss: 1.225200641155243
[3/3, 6250/7500] Training Loss: 1.223874068260193
[3/3, 6260/7500] Training Loss: 1.2563687086105346
[3/3, 6270/7500] Training Loss: 1.218276858329773
[3/3, 6280/7500] Training Loss: 1.2105796813964844
[3/3, 6290/7500] Training Loss: 1.2460325360298157
[3/3, 6300/7500] Training Loss: 1.2122223854064942
[3/3, 6310/7500] Training Loss: 1.2677173256874084
[3/3, 6320/7500] Training Loss: 1.2178475916385652
[3/3, 6330/7500] Training Loss: 1.2678109645843505
[3/3, 6340/7500] Training Loss: 1.2375340819358827
[3/3, 6350/7500] Training Loss: 1.1924797415733337
[3/3, 6360/7500] Training Loss: 1.183246946334839
[3/3, 6370/7500] Training Loss: 1.1953401923179627
[3/3, 6380/7500] Training Loss: 1.2947516083717345
[3/3, 6390/7500] Training Loss: 1.2105145454406738
[3/3, 6400/7500] Training Loss: 1.210600531101227
[3/3, 6410/7500] Training Loss: 1.2755425095558166
[3/3, 6420/7500] Training Loss: 1.2332841157913208
[3/3, 6430/7500] Training Loss: 1.2120925426483153
[3/3, 6440/7500] Training Loss: 1.2613170623779297
[3/3, 6450/7500] Training Loss: 1.2527603030204773
[3/3, 6460/7500] Training Loss: 1.1629727244377137
[3/3, 6470/7500] Training Loss: 1.2627016663551331
[3/3, 6480/7500] Training Loss: 1.294873023033142
[3/3, 6490/7500] Training Loss: 1.1755783975124359
[3/3, 6500/7500] Training Loss: 1.2310015082359314
[3/3, 6510/7500] Training Loss: 1.2346036314964295
[3/3, 6520/7500] Training Loss: 1.2297245144844056
[3/3, 6530/7500] Training Loss: 1.2270823836326599
[3/3, 6540/7500] Training Loss: 1.2645730316638946
[3/3, 6550/7500] Training Loss: 1.2106152296066284
[3/3, 6560/7500] Training Loss: 1.198815929889679
[3/3, 6570/7500] Training Loss: 1.2799742639064788
[3/3, 6580/7500] Training Loss: 1.2167726457118988
[3/3, 6590/7500] Training Loss: 1.2440696239471436
[3/3, 6600/7500] Training Loss: 1.2236215412616729
[3/3, 6610/7500] Training Loss: 1.1694468677043914
[3/3, 6620/7500] Training Loss: 1.1882820010185242
[3/3, 6630/7500] Training Loss: 1.241204857826233
[3/3, 6640/7500] Training Loss: 1.1977205395698547
[3/3, 6650/7500] Training Loss: 1.2573931813240051
[3/3, 6660/7500] Training Loss: 1.1538034617900848
[3/3, 6670/7500] Training Loss: 1.3001855134963989
[3/3, 6680/7500] Training Loss: 1.303845965862274
[3/3, 6690/7500] Training Loss: 1.1726241171360017
[3/3, 6700/7500] Training Loss: 1.3101028680801392
[3/3, 6710/7500] Training Loss: 1.324619507789612
[3/3, 6720/7500] Training Loss: 1.1913663864135742
[3/3, 6730/7500] Training Loss: 1.2264025330543518
[3/3, 6740/7500] Training Loss: 1.1827605307102202
[3/3, 6750/7500] Training Loss: 1.1972655653953552
[3/3, 6760/7500] Training Loss: 1.2209859251976014
[3/3, 6770/7500] Training Loss: 1.1765625894069671
[3/3, 6780/7500] Training Loss: 1.180685716867447
[3/3, 6790/7500] Training Loss: 1.191767430305481
[3/3, 6800/7500] Training Loss: 1.2213126242160797
[3/3, 6810/7500] Training Loss: 1.223133885860443
[3/3, 6820/7500] Training Loss: 1.2050549387931824
[3/3, 6830/7500] Training Loss: 1.303428018093109
[3/3, 6840/7500] Training Loss: 1.2101176738739015
[3/3, 6850/7500] Training Loss: 1.1625518202781677
[3/3, 6860/7500] Training Loss: 1.188958179950714
[3/3, 6870/7500] Training Loss: 1.2976862072944642
[3/3, 6880/7500] Training Loss: 1.265259826183319
[3/3, 6890/7500] Training Loss: 1.2483214020729065
[3/3, 6900/7500] Training Loss: 1.201556384563446
[3/3, 6910/7500] Training Loss: 1.2059854507446288
[3/3, 6920/7500] Training Loss: 1.2852328777313233
[3/3, 6930/7500] Training Loss: 1.2210352301597596
[3/3, 6940/7500] Training Loss: 1.2242220342159271
[3/3, 6950/7500] Training Loss: 1.2264125347137451
[3/3, 6960/7500] Training Loss: 1.1625284790992736
[3/3, 6970/7500] Training Loss: 1.2299079537391662
[3/3, 6980/7500] Training Loss: 1.2037605702877046
[3/3, 6990/7500] Training Loss: 1.0769119024276734
[3/3, 7000/7500] Training Loss: 1.208341932296753
[3/3, 7010/7500] Training Loss: 1.2202115178108215
[3/3, 7020/7500] Training Loss: 1.1427297353744508
[3/3, 7030/7500] Training Loss: 1.285036063194275
[3/3, 7040/7500] Training Loss: 1.1939475655555725
[3/3, 7050/7500] Training Loss: 1.253767192363739
[3/3, 7060/7500] Training Loss: 1.288411557674408
[3/3, 7070/7500] Training Loss: 1.2188174247741699
[3/3, 7080/7500] Training Loss: 1.2956916689872742
[3/3, 7090/7500] Training Loss: 1.180412197113037
[3/3, 7100/7500] Training Loss: 1.2446577429771424
[3/3, 7110/7500] Training Loss: 1.258258581161499
[3/3, 7120/7500] Training Loss: 1.2512480735778808
[3/3, 7130/7500] Training Loss: 1.2028778791427612
[3/3, 7140/7500] Training Loss: 1.1728010296821594
[3/3, 7150/7500] Training Loss: 1.2634170770645141
[3/3, 7160/7500] Training Loss: 1.172281849384308
[3/3, 7170/7500] Training Loss: 1.2665493786334991
[3/3, 7180/7500] Training Loss: 1.217688012123108
[3/3, 7190/7500] Training Loss: 1.2342702150344849
[3/3, 7200/7500] Training Loss: 1.1645529866218567
[3/3, 7210/7500] Training Loss: 1.2277663588523864
[3/3, 7220/7500] Training Loss: 1.262247669696808
[3/3, 7230/7500] Training Loss: 1.2564977049827575
[3/3, 7240/7500] Training Loss: 1.203382658958435
[3/3, 7250/7500] Training Loss: 1.2010398626327514
[3/3, 7260/7500] Training Loss: 1.2411742210388184
[3/3, 7270/7500] Training Loss: 1.229277765750885
[3/3, 7280/7500] Training Loss: 1.2200811743736266
[3/3, 7290/7500] Training Loss: 1.2925775170326232
[3/3, 7300/7500] Training Loss: 1.248803400993347
[3/3, 7310/7500] Training Loss: 1.2360316276550294
[3/3, 7320/7500] Training Loss: 1.2854771494865418
[3/3, 7330/7500] Training Loss: 1.196100902557373
[3/3, 7340/7500] Training Loss: 1.2551255822181702
[3/3, 7350/7500] Training Loss: 1.2404343724250793
[3/3, 7360/7500] Training Loss: 1.278790545463562
[3/3, 7370/7500] Training Loss: 1.2499347805976868
[3/3, 7380/7500] Training Loss: 1.1777314603328706
[3/3, 7390/7500] Training Loss: 1.1938609659671784
[3/3, 7400/7500] Training Loss: 1.2094081401824952
[3/3, 7410/7500] Training Loss: 1.2994397282600403
[3/3, 7420/7500] Training Loss: 1.2653201937675476
[3/3, 7430/7500] Training Loss: 1.3133578538894652
[3/3, 7440/7500] Training Loss: 1.2178120374679566
[3/3, 7450/7500] Training Loss: 1.2653800010681153
[3/3, 7460/7500] Training Loss: 1.2189283132553101
[3/3, 7470/7500] Training Loss: 1.236028605699539
[3/3, 7480/7500] Training Loss: 1.2789971709251404
[3/3, 7490/7500] Training Loss: 1.2604136228561402
[3/3, 7500/7500] Training Loss: 1.2475827574729919
5/5: Testing network...
Testing Loss: 1.2369132282733917
Training and Testing Finished
Assembling test data for t-sne projection
tensor([0, 8, 0, 6, 6, 0, 3, 4])
Batch: 1/1250
torch.Size([8, 100])
tensor([5, 1, 1, 0, 1, 2, 5, 8])
Batch: 2/1250
torch.Size([8, 100])
tensor([3, 9, 4, 3, 2, 9, 9, 3])
Batch: 3/1250
torch.Size([8, 100])
tensor([2, 3, 8, 9, 4, 3, 4, 3])
Batch: 4/1250
torch.Size([8, 100])
tensor([2, 5, 1, 6, 3, 9, 3, 6])
Batch: 5/1250
torch.Size([8, 100])
tensor([3, 6, 3, 7, 1, 4, 5, 5])
Batch: 6/1250
torch.Size([8, 100])
tensor([1, 3, 9, 5, 6, 4, 2, 3])
Batch: 7/1250
torch.Size([8, 100])
tensor([9, 8, 5, 4, 1, 9, 9, 9])
Batch: 8/1250
torch.Size([8, 100])
tensor([8, 3, 8, 6, 4, 4, 7, 3])
Batch: 9/1250
torch.Size([8, 100])
tensor([4, 4, 1, 3, 2, 5, 5, 8])
Batch: 10/1250
torch.Size([8, 100])
tensor([4, 2, 6, 6, 8, 6, 4, 1])
Batch: 11/1250
torch.Size([8, 100])
tensor([8, 4, 1, 4, 9, 7, 0, 3])
Batch: 12/1250
torch.Size([8, 100])
tensor([5, 0, 9, 0, 6, 3, 8, 1])
Batch: 13/1250
torch.Size([8, 100])
tensor([1, 5, 9, 4, 3, 6, 7, 4])
Batch: 14/1250
torch.Size([8, 100])
tensor([9, 5, 9, 9, 2, 8, 2, 3])
Batch: 15/1250
torch.Size([8, 100])
tensor([3, 6, 2, 4, 0, 2, 7, 3])
Batch: 16/1250
torch.Size([8, 100])
tensor([2, 3, 5, 4, 1, 6, 4, 6])
Batch: 17/1250
torch.Size([8, 100])
tensor([5, 5, 3, 0, 3, 9, 7, 3])
Batch: 18/1250
torch.Size([8, 100])
tensor([8, 3, 5, 3, 2, 3, 4, 0])
Batch: 19/1250
torch.Size([8, 100])
tensor([4, 1, 5, 9, 1, 2, 5, 2])
Batch: 20/1250
torch.Size([8, 100])
tensor([9, 1, 8, 9, 8, 2, 8, 1])
Batch: 21/1250
torch.Size([8, 100])
tensor([3, 8, 1, 0, 0, 2, 5, 8])
Batch: 22/1250
torch.Size([8, 100])
tensor([4, 0, 7, 6, 8, 4, 8, 8])
Batch: 23/1250
torch.Size([8, 100])
tensor([1, 1, 9, 7, 6, 8, 1, 4])
Batch: 24/1250
torch.Size([8, 100])
tensor([6, 0, 7, 5, 4, 2, 0, 9])
Batch: 25/1250
torch.Size([8, 100])
tensor([0, 9, 4, 2, 0, 0, 5, 1])
Batch: 26/1250
torch.Size([8, 100])
tensor([0, 5, 1, 7, 4, 4, 8, 5])
Batch: 27/1250
torch.Size([8, 100])
tensor([3, 2, 1, 6, 0, 9, 3, 0])
Batch: 28/1250
torch.Size([8, 100])
tensor([8, 8, 2, 8, 4, 2, 7, 2])
Batch: 29/1250
torch.Size([8, 100])
tensor([2, 9, 2, 7, 9, 9, 5, 6])
Batch: 30/1250
torch.Size([8, 100])
tensor([6, 4, 0, 8, 5, 0, 1, 5])
Batch: 31/1250
torch.Size([8, 100])
tensor([6, 5, 9, 5, 9, 3, 9, 2])
Batch: 32/1250
torch.Size([8, 100])
tensor([6, 8, 6, 7, 6, 7, 2, 3])
Batch: 33/1250
torch.Size([8, 100])
tensor([2, 1, 3, 2, 4, 5, 9, 1])
Batch: 34/1250
torch.Size([8, 100])
tensor([8, 8, 0, 2, 8, 0, 4, 1])
Batch: 35/1250
torch.Size([8, 100])
tensor([3, 1, 0, 9, 9, 6, 7, 3])
Batch: 36/1250
torch.Size([8, 100])
tensor([3, 0, 5, 7, 8, 3, 5, 5])
Batch: 37/1250
torch.Size([8, 100])
tensor([6, 5, 2, 4, 4, 6, 1, 8])
Batch: 38/1250
torch.Size([8, 100])
tensor([8, 4, 9, 4, 7, 6, 7, 6])
Batch: 39/1250
torch.Size([8, 100])
tensor([5, 3, 7, 7, 1, 2, 4, 9])
Batch: 40/1250
torch.Size([8, 100])
tensor([8, 0, 3, 2, 8, 0, 1, 1])
Batch: 41/1250
torch.Size([8, 100])
tensor([9, 2, 1, 5, 8, 1, 3, 5])
Batch: 42/1250
torch.Size([8, 100])
tensor([4, 8, 8, 7, 4, 2, 8, 9])
Batch: 43/1250
torch.Size([8, 100])
tensor([1, 2, 2, 1, 6, 6, 7, 0])
Batch: 44/1250
torch.Size([8, 100])
tensor([5, 7, 6, 6, 4, 3, 9, 6])
Batch: 45/1250
torch.Size([8, 100])
tensor([2, 2, 2, 3, 6, 1, 1, 3])
Batch: 46/1250
torch.Size([8, 100])
tensor([9, 7, 2, 8, 4, 4, 9, 3])
Batch: 47/1250
torch.Size([8, 100])
tensor([7, 0, 0, 9, 4, 0, 2, 8])
Batch: 48/1250
torch.Size([8, 100])
tensor([5, 7, 5, 7, 9, 2, 3, 9])
Batch: 49/1250
torch.Size([8, 100])
tensor([0, 0, 9, 9, 5, 9, 7, 7])
Batch: 50/1250
torch.Size([8, 100])
tensor([8, 6, 3, 0, 0, 0, 2, 6])
Batch: 51/1250
torch.Size([8, 100])
tensor([2, 3, 1, 4, 7, 7, 6, 8])
Batch: 52/1250
torch.Size([8, 100])
tensor([9, 9, 9, 4, 2, 8, 1, 6])
Batch: 53/1250
torch.Size([8, 100])
tensor([1, 4, 7, 5, 1, 5, 0, 8])
Batch: 54/1250
torch.Size([8, 100])
tensor([3, 0, 7, 9, 3, 4, 3, 8])
Batch: 55/1250
torch.Size([8, 100])
tensor([7, 4, 7, 6, 8, 7, 2, 3])
Batch: 56/1250
torch.Size([8, 100])
tensor([6, 0, 0, 9, 5, 8, 5, 2])
Batch: 57/1250
torch.Size([8, 100])
tensor([9, 8, 2, 2, 3, 7, 4, 9])
Batch: 58/1250
torch.Size([8, 100])
tensor([3, 6, 6, 0, 5, 2, 4, 3])
Batch: 59/1250
torch.Size([8, 100])
tensor([0, 2, 9, 2, 6, 3, 9, 8])
Batch: 60/1250
torch.Size([8, 100])
tensor([7, 5, 6, 1, 9, 6, 6, 9])
Batch: 61/1250
torch.Size([8, 100])
tensor([4, 5, 4, 4, 8, 1, 5, 6])
Batch: 62/1250
torch.Size([8, 100])
tensor([0, 3, 2, 9, 6, 4, 6, 9])
Batch: 63/1250
torch.Size([8, 100])
tensor([8, 1, 0, 1, 7, 7, 9, 9])
Batch: 64/1250
torch.Size([8, 100])
tensor([4, 2, 4, 1, 2, 4, 5, 2])
Batch: 65/1250
torch.Size([8, 100])
tensor([9, 9, 2, 0, 7, 3, 6, 4])
Batch: 66/1250
torch.Size([8, 100])
tensor([0, 9, 5, 8, 1, 0, 4, 5])
Batch: 67/1250
torch.Size([8, 100])
tensor([1, 8, 8, 5, 6, 6, 9, 9])
Batch: 68/1250
torch.Size([8, 100])
tensor([1, 7, 5, 7, 7, 3, 3, 0])
Batch: 69/1250
torch.Size([8, 100])
tensor([3, 1, 7, 2, 4, 0, 8, 9])
Batch: 70/1250
torch.Size([8, 100])
tensor([3, 9, 5, 2, 5, 4, 8, 3])
Batch: 71/1250
torch.Size([8, 100])
tensor([6, 9, 0, 1, 4, 8, 5, 7])
Batch: 72/1250
torch.Size([8, 100])
tensor([0, 4, 3, 8, 5, 3, 3, 3])
Batch: 73/1250
torch.Size([8, 100])
tensor([2, 4, 4, 0, 0, 8, 8, 7])
Batch: 74/1250
torch.Size([8, 100])
tensor([7, 7, 1, 4, 6, 4, 0, 8])
Batch: 75/1250
torch.Size([8, 100])
tensor([1, 9, 4, 1, 6, 6, 0, 1])
Batch: 76/1250
torch.Size([8, 100])
tensor([7, 1, 9, 8, 7, 2, 1, 9])
Batch: 77/1250
torch.Size([8, 100])
tensor([3, 0, 8, 3, 1, 5, 8, 5])
Batch: 78/1250
torch.Size([8, 100])
tensor([8, 5, 3, 4, 9, 9, 0, 8])
Batch: 79/1250
torch.Size([8, 100])
tensor([6, 6, 8, 2, 2, 6, 9, 6])
Batch: 80/1250
torch.Size([8, 100])
tensor([3, 2, 0, 0, 4, 5, 9, 7])
Batch: 81/1250
torch.Size([8, 100])
tensor([9, 3, 1, 3, 6, 7, 2, 3])
Batch: 82/1250
torch.Size([8, 100])
tensor([7, 0, 0, 5, 3, 2, 2, 6])
Batch: 83/1250
torch.Size([8, 100])
tensor([9, 4, 4, 9, 5, 3, 8, 1])
Batch: 84/1250
torch.Size([8, 100])
tensor([8, 3, 5, 6, 1, 7, 3, 7])
Batch: 85/1250
torch.Size([8, 100])
tensor([5, 1, 9, 9, 6, 1, 2, 3])
Batch: 86/1250
torch.Size([8, 100])
tensor([8, 0, 4, 7, 2, 7, 6, 8])
Batch: 87/1250
torch.Size([8, 100])
tensor([0, 0, 0, 7, 6, 4, 7, 4])
Batch: 88/1250
torch.Size([8, 100])
tensor([4, 8, 1, 2, 6, 3, 4, 4])
Batch: 89/1250
torch.Size([8, 100])
tensor([2, 6, 8, 6, 4, 9, 1, 9])
Batch: 90/1250
torch.Size([8, 100])
tensor([2, 7, 7, 7, 0, 7, 9, 1])
Batch: 91/1250
torch.Size([8, 100])
tensor([1, 2, 9, 1, 6, 0, 6, 6])
Batch: 92/1250
torch.Size([8, 100])
tensor([0, 7, 3, 1, 9, 3, 0, 2])
Batch: 93/1250
torch.Size([8, 100])
tensor([0, 9, 4, 8, 9, 5, 9, 5])
Batch: 94/1250
torch.Size([8, 100])
tensor([0, 8, 7, 2, 8, 8, 0, 7])
Batch: 95/1250
torch.Size([8, 100])
tensor([8, 9, 6, 1, 2, 2, 2, 2])
Batch: 96/1250
torch.Size([8, 100])
tensor([8, 6, 3, 4, 2, 2, 7, 4])
Batch: 97/1250
torch.Size([8, 100])
tensor([3, 1, 6, 5, 5, 7, 7, 9])
Batch: 98/1250
torch.Size([8, 100])
tensor([4, 2, 7, 3, 6, 7, 1, 8])
Batch: 99/1250
torch.Size([8, 100])
tensor([2, 7, 9, 8, 1, 1, 2, 4])
Batch: 100/1250
torch.Size([8, 100])
tensor([7, 3, 2, 1, 1, 6, 4, 1])
Batch: 101/1250
torch.Size([8, 100])
tensor([8, 2, 3, 7, 3, 1, 8, 2])
Batch: 102/1250
torch.Size([8, 100])
tensor([0, 8, 0, 4, 9, 5, 3, 3])
Batch: 103/1250
torch.Size([8, 100])
tensor([8, 3, 8, 0, 1, 7, 9, 7])
Batch: 104/1250
torch.Size([8, 100])
tensor([8, 5, 7, 6, 4, 9, 5, 9])
Batch: 105/1250
torch.Size([8, 100])
tensor([4, 4, 4, 4, 3, 4, 6, 2])
Batch: 106/1250
torch.Size([8, 100])
tensor([8, 5, 9, 8, 8, 1, 2, 2])
Batch: 107/1250
torch.Size([8, 100])
tensor([1, 6, 8, 0, 6, 4, 6, 0])
Batch: 108/1250
torch.Size([8, 100])
tensor([6, 8, 2, 0, 5, 6, 8, 1])
Batch: 109/1250
torch.Size([8, 100])
tensor([0, 9, 9, 9, 3, 9, 4, 7])
Batch: 110/1250
torch.Size([8, 100])
tensor([4, 6, 9, 3, 2, 4, 2, 8])
Batch: 111/1250
torch.Size([8, 100])
tensor([7, 4, 4, 3, 5, 0, 8, 8])
Batch: 112/1250
torch.Size([8, 100])
tensor([2, 1, 3, 0, 4, 7, 1, 6])
Batch: 113/1250
torch.Size([8, 100])
tensor([0, 5, 2, 6, 2, 7, 3, 5])
Batch: 114/1250
torch.Size([8, 100])
tensor([6, 7, 8, 7, 0, 1, 5, 6])
Batch: 115/1250
torch.Size([8, 100])
tensor([0, 6, 4, 9, 3, 7, 2, 8])
Batch: 116/1250
torch.Size([8, 100])
tensor([3, 1, 7, 4, 2, 8, 5, 1])
Batch: 117/1250
torch.Size([8, 100])
tensor([9, 4, 3, 3, 4, 3, 6, 2])
Batch: 118/1250
torch.Size([8, 100])
tensor([2, 2, 0, 2, 8, 6, 0, 1])
Batch: 119/1250
torch.Size([8, 100])
tensor([6, 2, 0, 1, 1, 1, 9, 3])
Batch: 120/1250
torch.Size([8, 100])
tensor([5, 5, 2, 9, 5, 9, 6, 0])
Batch: 121/1250
torch.Size([8, 100])
tensor([5, 6, 8, 2, 7, 0, 2, 4])
Batch: 122/1250
torch.Size([8, 100])
tensor([6, 8, 1, 9, 4, 6, 2, 4])
Batch: 123/1250
torch.Size([8, 100])
tensor([6, 3, 3, 5, 3, 1, 5, 4])
Batch: 124/1250
torch.Size([8, 100])
tensor([6, 0, 1, 0, 0, 9, 4, 5])
Batch: 125/1250
torch.Size([8, 100])
tensor([3, 7, 0, 4, 0, 0, 7, 0])
Batch: 126/1250
torch.Size([8, 100])
tensor([8, 6, 7, 6, 5, 5, 5, 1])
Batch: 127/1250
torch.Size([8, 100])
tensor([2, 7, 7, 2, 6, 0, 4, 4])
Batch: 128/1250
torch.Size([8, 100])
tensor([1, 5, 1, 5, 6, 8, 6, 4])
Batch: 129/1250
torch.Size([8, 100])
tensor([2, 4, 4, 2, 8, 4, 3, 0])
Batch: 130/1250
torch.Size([8, 100])
tensor([4, 4, 6, 3, 8, 1, 0, 5])
Batch: 131/1250
torch.Size([8, 100])
tensor([2, 6, 3, 9, 7, 5, 8, 3])
Batch: 132/1250
torch.Size([8, 100])
tensor([5, 3, 8, 4, 8, 7, 9, 4])
Batch: 133/1250
torch.Size([8, 100])
tensor([6, 7, 5, 8, 0, 9, 6, 4])
Batch: 134/1250
torch.Size([8, 100])
tensor([1, 3, 6, 0, 7, 7, 8, 7])
Batch: 135/1250
torch.Size([8, 100])
tensor([8, 6, 0, 4, 0, 8, 2, 9])
Batch: 136/1250
torch.Size([8, 100])
tensor([4, 9, 1, 4, 5, 0, 5, 0])
Batch: 137/1250
torch.Size([8, 100])
tensor([6, 3, 1, 1, 4, 0, 3, 1])
Batch: 138/1250
torch.Size([8, 100])
tensor([4, 2, 7, 7, 5, 7, 6, 0])
Batch: 139/1250
torch.Size([8, 100])
tensor([7, 9, 9, 4, 1, 1, 2, 1])
Batch: 140/1250
torch.Size([8, 100])
tensor([3, 2, 7, 3, 8, 3, 0, 7])
Batch: 141/1250
torch.Size([8, 100])
tensor([9, 8, 3, 9, 3, 6, 5, 2])
Batch: 142/1250
torch.Size([8, 100])
tensor([7, 2, 3, 3, 0, 6, 4, 6])
Batch: 143/1250
torch.Size([8, 100])
tensor([3, 2, 8, 6, 2, 9, 8, 4])
Batch: 144/1250
torch.Size([8, 100])
tensor([7, 1, 8, 6, 2, 1, 4, 4])
Batch: 145/1250
torch.Size([8, 100])
tensor([0, 2, 5, 3, 4, 8, 5, 0])
Batch: 146/1250
torch.Size([8, 100])
tensor([0, 2, 8, 2, 5, 6, 9, 1])
Batch: 147/1250
torch.Size([8, 100])
tensor([4, 4, 4, 0, 3, 4, 4, 2])
Batch: 148/1250
torch.Size([8, 100])
tensor([2, 3, 0, 2, 3, 9, 2, 6])
Batch: 149/1250
torch.Size([8, 100])
tensor([7, 1, 6, 2, 7, 3, 7, 7])
Batch: 150/1250
torch.Size([8, 100])
tensor([9, 3, 3, 1, 0, 3, 9, 0])
Batch: 151/1250
torch.Size([8, 100])
tensor([1, 4, 2, 8, 6, 7, 4, 2])
Batch: 152/1250
torch.Size([8, 100])
tensor([2, 5, 8, 3, 2, 3, 1, 0])
Batch: 153/1250
torch.Size([8, 100])
tensor([4, 7, 2, 8, 1, 6, 0, 4])
Batch: 154/1250
torch.Size([8, 100])
tensor([9, 4, 1, 1, 8, 4, 8, 7])
Batch: 155/1250
torch.Size([8, 100])
tensor([6, 3, 1, 8, 3, 8, 3, 5])
Batch: 156/1250
torch.Size([8, 100])
tensor([2, 2, 3, 1, 6, 3, 9, 4])
Batch: 157/1250
torch.Size([8, 100])
tensor([0, 7, 1, 9, 0, 7, 8, 8])
Batch: 158/1250
torch.Size([8, 100])
tensor([2, 6, 6, 2, 0, 0, 0, 1])
Batch: 159/1250
torch.Size([8, 100])
tensor([0, 1, 7, 1, 3, 2, 4, 6])
Batch: 160/1250
torch.Size([8, 100])
tensor([2, 8, 8, 8, 3, 9, 0, 4])
Batch: 161/1250
torch.Size([8, 100])
tensor([7, 0, 8, 3, 4, 6, 3, 1])
Batch: 162/1250
torch.Size([8, 100])
tensor([5, 1, 8, 5, 9, 9, 3, 0])
Batch: 163/1250
torch.Size([8, 100])
tensor([2, 8, 0, 3, 8, 0, 8, 7])
Batch: 164/1250
torch.Size([8, 100])
tensor([0, 9, 3, 2, 1, 7, 4, 9])
Batch: 165/1250
torch.Size([8, 100])
tensor([1, 1, 0, 6, 8, 1, 8, 2])
Batch: 166/1250
torch.Size([8, 100])
tensor([1, 6, 8, 3, 1, 8, 1, 9])
Batch: 167/1250
torch.Size([8, 100])
tensor([2, 3, 1, 0, 6, 1, 3, 2])
Batch: 168/1250
torch.Size([8, 100])
tensor([7, 5, 7, 0, 3, 6, 2, 4])
Batch: 169/1250
torch.Size([8, 100])
tensor([6, 9, 5, 0, 2, 3, 7, 6])
Batch: 170/1250
torch.Size([8, 100])
tensor([2, 5, 1, 0, 1, 7, 6, 9])
Batch: 171/1250
torch.Size([8, 100])
tensor([4, 1, 5, 9, 6, 7, 4, 3])
Batch: 172/1250
torch.Size([8, 100])
tensor([5, 5, 2, 1, 9, 6, 0, 3])
Batch: 173/1250
torch.Size([8, 100])
tensor([3, 0, 0, 8, 9, 1, 8, 4])
Batch: 174/1250
torch.Size([8, 100])
tensor([2, 3, 9, 5, 0, 3, 4, 4])
Batch: 175/1250
torch.Size([8, 100])
tensor([6, 2, 4, 7, 8, 6, 1, 2])
Batch: 176/1250
torch.Size([8, 100])
tensor([9, 9, 4, 8, 0, 9, 6, 7])
Batch: 177/1250
torch.Size([8, 100])
tensor([5, 3, 7, 3, 8, 1, 2, 0])
Batch: 178/1250
torch.Size([8, 100])
tensor([0, 8, 0, 3, 2, 5, 1, 7])
Batch: 179/1250
torch.Size([8, 100])
tensor([5, 6, 7, 4, 1, 2, 2, 0])
Batch: 180/1250
torch.Size([8, 100])
tensor([9, 9, 0, 7, 2, 0, 6, 1])
Batch: 181/1250
torch.Size([8, 100])
tensor([0, 5, 7, 4, 8, 5, 8, 4])
Batch: 182/1250
torch.Size([8, 100])
tensor([5, 1, 6, 1, 1, 8, 2, 4])
Batch: 183/1250
torch.Size([8, 100])
tensor([4, 2, 1, 2, 2, 9, 9, 0])
Batch: 184/1250
torch.Size([8, 100])
tensor([2, 3, 2, 7, 8, 4, 6, 0])
Batch: 185/1250
torch.Size([8, 100])
tensor([6, 2, 6, 6, 7, 1, 8, 7])
Batch: 186/1250
torch.Size([8, 100])
tensor([1, 9, 3, 5, 6, 4, 0, 3])
Batch: 187/1250
torch.Size([8, 100])
tensor([9, 1, 0, 0, 2, 6, 1, 5])
Batch: 188/1250
torch.Size([8, 100])
tensor([9, 9, 0, 3, 0, 1, 7, 9])
Batch: 189/1250
torch.Size([8, 100])
tensor([2, 3, 2, 3, 6, 7, 9, 3])
Batch: 190/1250
torch.Size([8, 100])
tensor([5, 3, 1, 6, 5, 3, 8, 0])
Batch: 191/1250
torch.Size([8, 100])
tensor([9, 4, 3, 9, 5, 4, 1, 8])
Batch: 192/1250
torch.Size([8, 100])
tensor([1, 3, 5, 8, 2, 1, 9, 1])
Batch: 193/1250
torch.Size([8, 100])
tensor([1, 2, 9, 3, 4, 1, 1, 8])
Batch: 194/1250
torch.Size([8, 100])
tensor([3, 9, 1, 4, 0, 3, 4, 3])
Batch: 195/1250
torch.Size([8, 100])
tensor([1, 0, 2, 8, 7, 7, 3, 2])
Batch: 196/1250
torch.Size([8, 100])
tensor([0, 3, 2, 9, 2, 0, 4, 2])
Batch: 197/1250
torch.Size([8, 100])
tensor([1, 2, 0, 8, 6, 8, 7, 2])
Batch: 198/1250
torch.Size([8, 100])
tensor([9, 0, 9, 2, 5, 4, 7, 9])
Batch: 199/1250
torch.Size([8, 100])
tensor([4, 7, 0, 1, 3, 8, 4, 1])
Batch: 200/1250
torch.Size([8, 100])
tensor([1, 2, 4, 5, 8, 1, 7, 3])
Batch: 201/1250
torch.Size([8, 100])
tensor([7, 1, 8, 5, 8, 7, 4, 3])
Batch: 202/1250
torch.Size([8, 100])
tensor([4, 1, 4, 2, 3, 1, 1, 5])
Batch: 203/1250
torch.Size([8, 100])
tensor([7, 1, 3, 1, 4, 8, 6, 1])
Batch: 204/1250
torch.Size([8, 100])
tensor([0, 8, 6, 1, 8, 9, 3, 2])
Batch: 205/1250
torch.Size([8, 100])
tensor([1, 0, 0, 8, 8, 2, 2, 7])
Batch: 206/1250
torch.Size([8, 100])
tensor([0, 4, 5, 4, 2, 1, 5, 0])
Batch: 207/1250
torch.Size([8, 100])
tensor([5, 2, 9, 4, 1, 3, 9, 3])
Batch: 208/1250
torch.Size([8, 100])
tensor([7, 6, 7, 8, 4, 7, 2, 1])
Batch: 209/1250
torch.Size([8, 100])
tensor([0, 1, 8, 7, 7, 9, 4, 1])
Batch: 210/1250
torch.Size([8, 100])
tensor([8, 9, 7, 6, 6, 9, 2, 3])
Batch: 211/1250
torch.Size([8, 100])
tensor([6, 6, 8, 2, 4, 2, 0, 8])
Batch: 212/1250
torch.Size([8, 100])
tensor([3, 2, 3, 5, 1, 8, 6, 5])
Batch: 213/1250
torch.Size([8, 100])
tensor([1, 8, 5, 5, 6, 6, 8, 3])
Batch: 214/1250
torch.Size([8, 100])
tensor([9, 6, 3, 9, 7, 3, 4, 1])
Batch: 215/1250
torch.Size([8, 100])
tensor([1, 8, 9, 4, 8, 2, 1, 7])
Batch: 216/1250
torch.Size([8, 100])
tensor([2, 9, 2, 9, 0, 6, 0, 8])
Batch: 217/1250
torch.Size([8, 100])
tensor([7, 7, 3, 2, 2, 3, 9, 1])
Batch: 218/1250
torch.Size([8, 100])
tensor([5, 1, 1, 3, 0, 1, 1, 2])
Batch: 219/1250
torch.Size([8, 100])
tensor([5, 9, 5, 6, 8, 5, 6, 4])
Batch: 220/1250
torch.Size([8, 100])
tensor([0, 3, 0, 9, 5, 2, 5, 0])
Batch: 221/1250
torch.Size([8, 100])
tensor([2, 6, 0, 3, 4, 6, 4, 4])
Batch: 222/1250
torch.Size([8, 100])
tensor([2, 2, 8, 7, 9, 3, 4, 1])
Batch: 223/1250
torch.Size([8, 100])
tensor([0, 5, 8, 8, 7, 4, 1, 4])
Batch: 224/1250
torch.Size([8, 100])
tensor([1, 8, 4, 6, 6, 1, 4, 1])
Batch: 225/1250
torch.Size([8, 100])
tensor([2, 3, 3, 0, 4, 2, 3, 3])
Batch: 226/1250
torch.Size([8, 100])
tensor([2, 7, 8, 2, 3, 0, 3, 3])
Batch: 227/1250
torch.Size([8, 100])
tensor([9, 1, 7, 2, 2, 5, 1, 3])
Batch: 228/1250
torch.Size([8, 100])
tensor([1, 4, 2, 7, 4, 6, 4, 2])
Batch: 229/1250
torch.Size([8, 100])
tensor([2, 8, 5, 9, 1, 4, 8, 4])
Batch: 230/1250
torch.Size([8, 100])
tensor([4, 4, 2, 2, 2, 7, 8, 7])
Batch: 231/1250
torch.Size([8, 100])
tensor([0, 5, 5, 0, 9, 8, 1, 8])
Batch: 232/1250
torch.Size([8, 100])
tensor([6, 1, 9, 6, 6, 3, 7, 3])
Batch: 233/1250
torch.Size([8, 100])
tensor([1, 3, 7, 1, 6, 7, 3, 7])
Batch: 234/1250
torch.Size([8, 100])
tensor([9, 8, 0, 8, 0, 1, 9, 0])
Batch: 235/1250
torch.Size([8, 100])
tensor([2, 1, 1, 2, 7, 0, 0, 6])
Batch: 236/1250
torch.Size([8, 100])
tensor([9, 5, 8, 0, 4, 6, 3, 4])
Batch: 237/1250
torch.Size([8, 100])
tensor([2, 6, 0, 1, 2, 0, 2, 9])
Batch: 238/1250
torch.Size([8, 100])
tensor([1, 7, 4, 9, 3, 8, 6, 1])
Batch: 239/1250
torch.Size([8, 100])
tensor([1, 4, 3, 7, 2, 9, 7, 7])
Batch: 240/1250
torch.Size([8, 100])
tensor([3, 9, 1, 7, 8, 6, 5, 7])
Batch: 241/1250
torch.Size([8, 100])
tensor([2, 3, 4, 0, 9, 6, 2, 7])
Batch: 242/1250
torch.Size([8, 100])
tensor([7, 6, 5, 7, 1, 4, 1, 3])
Batch: 243/1250
torch.Size([8, 100])
tensor([6, 6, 1, 9, 7, 8, 3, 4])
Batch: 244/1250
torch.Size([8, 100])
tensor([4, 2, 3, 2, 8, 1, 5, 7])
Batch: 245/1250
torch.Size([8, 100])
tensor([0, 7, 2, 7, 5, 0, 5, 9])
Batch: 246/1250
torch.Size([8, 100])
tensor([3, 7, 1, 7, 0, 2, 6, 4])
Batch: 247/1250
torch.Size([8, 100])
tensor([8, 1, 9, 5, 4, 8, 2, 4])
Batch: 248/1250
torch.Size([8, 100])
tensor([1, 9, 8, 9, 9, 7, 7, 9])
Batch: 249/1250
torch.Size([8, 100])
tensor([8, 8, 1, 4, 6, 9, 9, 0])
Batch: 250/1250
torch.Size([8, 100])
tensor([4, 5, 1, 7, 8, 1, 8, 8])
Batch: 251/1250
torch.Size([8, 100])
tensor([8, 3, 9, 8, 8, 4, 6, 8])
Batch: 252/1250
torch.Size([8, 100])
tensor([9, 4, 8, 3, 8, 9, 2, 9])
Batch: 253/1250
torch.Size([8, 100])
tensor([4, 1, 0, 4, 5, 4, 9, 7])
Batch: 254/1250
torch.Size([8, 100])
tensor([9, 8, 6, 9, 3, 0, 9, 2])
Batch: 255/1250
torch.Size([8, 100])
tensor([1, 8, 6, 5, 2, 5, 5, 1])
Batch: 256/1250
torch.Size([8, 100])
tensor([6, 8, 0, 7, 1, 2, 4, 8])
Batch: 257/1250
torch.Size([8, 100])
tensor([6, 8, 7, 8, 3, 6, 9, 3])
Batch: 258/1250
torch.Size([8, 100])
tensor([5, 8, 7, 8, 5, 2, 4, 4])
Batch: 259/1250
torch.Size([8, 100])
tensor([0, 0, 0, 5, 0, 4, 6, 0])
Batch: 260/1250
torch.Size([8, 100])
tensor([8, 6, 8, 3, 3, 8, 7, 3])
Batch: 261/1250
torch.Size([8, 100])
tensor([6, 0, 3, 7, 8, 4, 8, 3])
Batch: 262/1250
torch.Size([8, 100])
tensor([2, 0, 6, 4, 0, 3, 5, 5])
Batch: 263/1250
torch.Size([8, 100])
tensor([9, 0, 8, 4, 1, 4, 7, 1])
Batch: 264/1250
torch.Size([8, 100])
tensor([3, 2, 0, 7, 2, 8, 1, 6])
Batch: 265/1250
torch.Size([8, 100])
tensor([6, 1, 7, 8, 7, 0, 7, 1])
Batch: 266/1250
torch.Size([8, 100])
tensor([9, 6, 9, 1, 8, 9, 5, 5])
Batch: 267/1250
torch.Size([8, 100])
tensor([9, 7, 4, 5, 0, 1, 8, 5])
Batch: 268/1250
torch.Size([8, 100])
tensor([0, 8, 3, 7, 1, 7, 8, 5])
Batch: 269/1250
torch.Size([8, 100])
tensor([1, 1, 9, 3, 7, 4, 0, 3])
Batch: 270/1250
torch.Size([8, 100])
tensor([6, 8, 6, 8, 2, 2, 4, 7])
Batch: 271/1250
torch.Size([8, 100])
tensor([5, 2, 3, 3, 6, 6, 0, 7])
Batch: 272/1250
torch.Size([8, 100])
tensor([4, 4, 1, 0, 2, 6, 1, 3])
Batch: 273/1250
torch.Size([8, 100])
tensor([0, 5, 4, 4, 5, 5, 7, 1])
Batch: 274/1250
torch.Size([8, 100])
tensor([9, 2, 7, 7, 0, 0, 1, 3])
Batch: 275/1250
torch.Size([8, 100])
tensor([0, 4, 0, 4, 8, 9, 8, 7])
Batch: 276/1250
torch.Size([8, 100])
tensor([9, 3, 8, 5, 5, 7, 3, 0])
Batch: 277/1250
torch.Size([8, 100])
tensor([6, 4, 1, 9, 0, 2, 3, 3])
Batch: 278/1250
torch.Size([8, 100])
tensor([9, 9, 5, 0, 5, 6, 8, 3])
Batch: 279/1250
torch.Size([8, 100])
tensor([8, 0, 8, 6, 0, 8, 1, 6])
Batch: 280/1250
torch.Size([8, 100])
tensor([3, 3, 8, 5, 2, 9, 1, 9])
Batch: 281/1250
torch.Size([8, 100])
tensor([7, 7, 5, 6, 5, 2, 4, 8])
Batch: 282/1250
torch.Size([8, 100])
tensor([9, 2, 3, 4, 1, 6, 6, 4])
Batch: 283/1250
torch.Size([8, 100])
tensor([4, 4, 7, 6, 2, 6, 6, 6])
Batch: 284/1250
torch.Size([8, 100])
tensor([3, 6, 4, 5, 1, 7, 8, 3])
Batch: 285/1250
torch.Size([8, 100])
tensor([1, 3, 4, 5, 6, 1, 4, 0])
Batch: 286/1250
torch.Size([8, 100])
tensor([3, 2, 6, 2, 9, 2, 2, 2])
Batch: 287/1250
torch.Size([8, 100])
tensor([2, 7, 3, 1, 2, 5, 1, 0])
Batch: 288/1250
torch.Size([8, 100])
tensor([9, 2, 9, 5, 5, 2, 7, 0])
Batch: 289/1250
torch.Size([8, 100])
tensor([7, 2, 1, 5, 0, 7, 2, 6])
Batch: 290/1250
torch.Size([8, 100])
tensor([8, 1, 7, 0, 7, 8, 4, 3])
Batch: 291/1250
torch.Size([8, 100])
tensor([1, 1, 2, 2, 1, 3, 9, 1])
Batch: 292/1250
torch.Size([8, 100])
tensor([0, 9, 3, 2, 1, 9, 4, 9])
Batch: 293/1250
torch.Size([8, 100])
tensor([1, 4, 7, 7, 4, 7, 1, 7])
Batch: 294/1250
torch.Size([8, 100])
tensor([4, 6, 0, 3, 3, 0, 6, 0])
Batch: 295/1250
torch.Size([8, 100])
tensor([5, 7, 2, 4, 3, 2, 7, 2])
Batch: 296/1250
torch.Size([8, 100])
tensor([8, 1, 8, 2, 9, 2, 1, 5])
Batch: 297/1250
torch.Size([8, 100])
tensor([5, 2, 8, 3, 9, 7, 5, 8])
Batch: 298/1250
torch.Size([8, 100])
tensor([5, 4, 1, 3, 4, 6, 8, 4])
Batch: 299/1250
torch.Size([8, 100])
tensor([0, 2, 4, 0, 7, 7, 8, 7])
Batch: 300/1250
torch.Size([8, 100])
tensor([1, 0, 9, 4, 6, 3, 6, 8])
Batch: 301/1250
torch.Size([8, 100])
tensor([8, 3, 6, 6, 4, 3, 7, 8])
Batch: 302/1250
torch.Size([8, 100])
tensor([3, 4, 9, 1, 3, 2, 0, 7])
Batch: 303/1250
torch.Size([8, 100])
tensor([9, 4, 7, 8, 6, 2, 2, 9])
Batch: 304/1250
torch.Size([8, 100])
tensor([6, 0, 7, 9, 1, 6, 3, 1])
Batch: 305/1250
torch.Size([8, 100])
tensor([6, 0, 1, 5, 9, 5, 5, 5])
Batch: 306/1250
torch.Size([8, 100])
tensor([4, 0, 7, 4, 0, 6, 8, 8])
Batch: 307/1250
torch.Size([8, 100])
tensor([9, 8, 3, 9, 5, 4, 5, 2])
Batch: 308/1250
torch.Size([8, 100])
tensor([6, 4, 5, 2, 7, 2, 7, 1])
Batch: 309/1250
torch.Size([8, 100])
tensor([0, 0, 9, 8, 8, 0, 7, 6])
Batch: 310/1250
torch.Size([8, 100])
tensor([0, 7, 5, 8, 9, 1, 5, 1])
Batch: 311/1250
torch.Size([8, 100])
tensor([3, 9, 3, 9, 3, 2, 9, 6])
Batch: 312/1250
torch.Size([8, 100])
tensor([2, 0, 1, 6, 1, 5, 9, 2])
Batch: 313/1250
torch.Size([8, 100])
tensor([1, 4, 1, 4, 1, 5, 6, 1])
Batch: 314/1250
torch.Size([8, 100])
tensor([6, 2, 7, 7, 2, 1, 3, 4])
Batch: 315/1250
torch.Size([8, 100])
tensor([5, 7, 5, 1, 7, 8, 6, 8])
Batch: 316/1250
torch.Size([8, 100])
tensor([5, 1, 5, 9, 3, 7, 0, 9])
Batch: 317/1250
torch.Size([8, 100])
tensor([5, 5, 8, 6, 6, 2, 6, 4])
Batch: 318/1250
torch.Size([8, 100])
tensor([3, 3, 8, 3, 0, 8, 5, 4])
Batch: 319/1250
torch.Size([8, 100])
tensor([7, 0, 7, 1, 3, 8, 6, 7])
Batch: 320/1250
torch.Size([8, 100])
tensor([9, 9, 2, 3, 1, 4, 7, 2])
Batch: 321/1250
torch.Size([8, 100])
tensor([5, 8, 3, 4, 3, 8, 6, 9])
Batch: 322/1250
torch.Size([8, 100])
tensor([8, 8, 5, 0, 5, 1, 5, 3])
Batch: 323/1250
torch.Size([8, 100])
tensor([1, 6, 5, 9, 6, 9, 7, 1])
Batch: 324/1250
torch.Size([8, 100])
tensor([4, 5, 7, 2, 4, 3, 8, 6])
Batch: 325/1250
torch.Size([8, 100])
tensor([2, 6, 6, 7, 6, 4, 4, 4])
Batch: 326/1250
torch.Size([8, 100])
tensor([7, 3, 4, 5, 9, 6, 1, 6])
Batch: 327/1250
torch.Size([8, 100])
tensor([3, 2, 8, 2, 7, 1, 8, 4])
Batch: 328/1250
torch.Size([8, 100])
tensor([5, 0, 4, 6, 6, 2, 8, 1])
Batch: 329/1250
torch.Size([8, 100])
tensor([7, 5, 2, 8, 2, 5, 0, 3])
Batch: 330/1250
torch.Size([8, 100])
tensor([2, 0, 3, 4, 5, 4, 3, 1])
Batch: 331/1250
torch.Size([8, 100])
tensor([0, 5, 0, 1, 8, 8, 2, 6])
Batch: 332/1250
torch.Size([8, 100])
tensor([6, 7, 7, 9, 0, 2, 3, 1])
Batch: 333/1250
torch.Size([8, 100])
tensor([8, 8, 6, 3, 9, 9, 4, 3])
Batch: 334/1250
torch.Size([8, 100])
tensor([4, 9, 5, 3, 2, 0, 0, 8])
Batch: 335/1250
torch.Size([8, 100])
tensor([1, 1, 7, 5, 9, 2, 6, 9])
Batch: 336/1250
torch.Size([8, 100])
tensor([6, 5, 8, 8, 8, 0, 3, 1])
Batch: 337/1250
torch.Size([8, 100])
tensor([7, 2, 3, 2, 3, 2, 1, 9])
Batch: 338/1250
torch.Size([8, 100])
tensor([6, 8, 3, 3, 4, 1, 3, 7])
Batch: 339/1250
torch.Size([8, 100])
tensor([1, 0, 1, 3, 8, 1, 7, 2])
Batch: 340/1250
torch.Size([8, 100])
tensor([6, 6, 6, 2, 6, 1, 9, 4])
Batch: 341/1250
torch.Size([8, 100])
tensor([5, 1, 1, 1, 8, 1, 4, 6])
Batch: 342/1250
torch.Size([8, 100])
tensor([2, 1, 3, 6, 7, 8, 9, 6])
Batch: 343/1250
torch.Size([8, 100])
tensor([6, 1, 9, 7, 6, 4, 7, 5])
Batch: 344/1250
torch.Size([8, 100])
tensor([5, 1, 8, 5, 5, 2, 6, 5])
Batch: 345/1250
torch.Size([8, 100])
tensor([0, 0, 8, 0, 8, 7, 4, 4])
Batch: 346/1250
torch.Size([8, 100])
tensor([7, 5, 0, 6, 4, 3, 3, 7])
Batch: 347/1250
torch.Size([8, 100])
tensor([1, 7, 8, 6, 8, 6, 7, 0])
Batch: 348/1250
torch.Size([8, 100])
tensor([2, 3, 9, 5, 9, 1, 0, 8])
Batch: 349/1250
torch.Size([8, 100])
tensor([3, 0, 1, 6, 4, 2, 0, 2])
Batch: 350/1250
torch.Size([8, 100])
tensor([5, 7, 3, 7, 9, 5, 4, 7])
Batch: 351/1250
torch.Size([8, 100])
tensor([8, 8, 0, 3, 0, 1, 4, 2])
Batch: 352/1250
torch.Size([8, 100])
tensor([0, 7, 6, 3, 6, 3, 0, 9])
Batch: 353/1250
torch.Size([8, 100])
tensor([1, 9, 2, 0, 9, 7, 5, 8])
Batch: 354/1250
torch.Size([8, 100])
tensor([6, 4, 3, 3, 8, 9, 7, 7])
Batch: 355/1250
torch.Size([8, 100])
tensor([3, 5, 3, 4, 5, 9, 5, 4])
Batch: 356/1250
torch.Size([8, 100])
tensor([9, 4, 3, 1, 6, 4, 6, 4])
Batch: 357/1250
torch.Size([8, 100])
tensor([8, 9, 6, 4, 7, 2, 0, 8])
Batch: 358/1250
torch.Size([8, 100])
tensor([9, 7, 9, 0, 2, 1, 9, 3])
Batch: 359/1250
torch.Size([8, 100])
tensor([1, 9, 1, 8, 1, 6, 8, 2])
Batch: 360/1250
torch.Size([8, 100])
tensor([2, 1, 1, 5, 2, 8, 7, 2])
Batch: 361/1250
torch.Size([8, 100])
tensor([9, 9, 2, 6, 1, 3, 1, 7])
Batch: 362/1250
torch.Size([8, 100])
tensor([9, 1, 0, 7, 6, 3, 4, 9])
Batch: 363/1250
torch.Size([8, 100])
tensor([2, 3, 0, 5, 7, 5, 4, 0])
Batch: 364/1250
torch.Size([8, 100])
tensor([6, 0, 7, 0, 7, 4, 2, 4])
Batch: 365/1250
torch.Size([8, 100])
tensor([1, 8, 1, 6, 5, 0, 1, 6])
Batch: 366/1250
torch.Size([8, 100])
tensor([3, 0, 3, 8, 2, 2, 3, 7])
Batch: 367/1250
torch.Size([8, 100])
tensor([9, 1, 4, 4, 1, 6, 5, 2])
Batch: 368/1250
torch.Size([8, 100])
tensor([2, 7, 9, 4, 2, 8, 2, 6])
Batch: 369/1250
torch.Size([8, 100])
tensor([2, 6, 7, 1, 0, 1, 8, 2])
Batch: 370/1250
torch.Size([8, 100])
tensor([2, 1, 3, 4, 2, 4, 4, 0])
Batch: 371/1250
torch.Size([8, 100])
tensor([9, 7, 4, 7, 3, 3, 1, 4])
Batch: 372/1250
torch.Size([8, 100])
tensor([5, 3, 7, 7, 0, 8, 8, 2])
Batch: 373/1250
torch.Size([8, 100])
tensor([2, 0, 8, 3, 9, 8, 1, 1])
Batch: 374/1250
torch.Size([8, 100])
tensor([2, 1, 2, 3, 6, 9, 1, 7])
Batch: 375/1250
torch.Size([8, 100])
tensor([5, 9, 3, 5, 5, 0, 9, 3])
Batch: 376/1250
torch.Size([8, 100])
tensor([1, 1, 2, 6, 2, 3, 0, 9])
Batch: 377/1250
torch.Size([8, 100])
tensor([3, 0, 6, 1, 3, 7, 0, 9])
Batch: 378/1250
torch.Size([8, 100])
tensor([3, 8, 2, 4, 6, 9, 2, 1])
Batch: 379/1250
torch.Size([8, 100])
tensor([3, 6, 8, 6, 2, 6, 1, 6])
Batch: 380/1250
torch.Size([8, 100])
tensor([4, 7, 0, 6, 7, 7, 7, 3])
Batch: 381/1250
torch.Size([8, 100])
tensor([0, 1, 1, 1, 1, 5, 1, 3])
Batch: 382/1250
torch.Size([8, 100])
tensor([4, 4, 8, 7, 7, 8, 3, 0])
Batch: 383/1250
torch.Size([8, 100])
tensor([2, 7, 7, 0, 8, 3, 0, 4])
Batch: 384/1250
torch.Size([8, 100])
tensor([8, 9, 7, 7, 8, 2, 1, 6])
Batch: 385/1250
torch.Size([8, 100])
tensor([5, 6, 1, 3, 4, 4, 1, 7])
Batch: 386/1250
torch.Size([8, 100])
tensor([3, 1, 2, 4, 4, 4, 8, 5])
Batch: 387/1250
torch.Size([8, 100])
tensor([2, 4, 6, 7, 0, 6, 5, 2])
Batch: 388/1250
torch.Size([8, 100])
tensor([9, 8, 7, 1, 5, 7, 2, 7])
Batch: 389/1250
torch.Size([8, 100])
tensor([1, 0, 8, 1, 0, 1, 8, 8])
Batch: 390/1250
torch.Size([8, 100])
tensor([9, 3, 5, 3, 2, 9, 7, 0])
Batch: 391/1250
torch.Size([8, 100])
tensor([0, 7, 7, 3, 3, 9, 8, 4])
Batch: 392/1250
torch.Size([8, 100])
tensor([9, 0, 9, 8, 8, 4, 3, 7])
Batch: 393/1250
torch.Size([8, 100])
tensor([3, 2, 4, 1, 9, 8, 0, 9])
Batch: 394/1250
torch.Size([8, 100])
tensor([5, 2, 4, 6, 2, 7, 9, 3])
Batch: 395/1250
torch.Size([8, 100])
tensor([7, 8, 6, 4, 1, 7, 5, 3])
Batch: 396/1250
torch.Size([8, 100])
tensor([7, 5, 0, 9, 3, 5, 0, 6])
Batch: 397/1250
torch.Size([8, 100])
tensor([4, 6, 3, 4, 9, 8, 8, 6])
Batch: 398/1250
torch.Size([8, 100])
tensor([0, 4, 1, 2, 6, 8, 1, 7])
Batch: 399/1250
torch.Size([8, 100])
tensor([1, 7, 4, 0, 9, 1, 4, 3])
Batch: 400/1250
torch.Size([8, 100])
tensor([3, 9, 5, 0, 8, 1, 3, 8])
Batch: 401/1250
torch.Size([8, 100])
tensor([5, 5, 3, 2, 3, 4, 1, 0])
Batch: 402/1250
torch.Size([8, 100])
tensor([1, 3, 2, 1, 9, 0, 9, 3])
Batch: 403/1250
torch.Size([8, 100])
tensor([7, 3, 4, 1, 3, 3, 1, 7])
Batch: 404/1250
torch.Size([8, 100])
tensor([9, 9, 5, 2, 1, 0, 3, 7])
Batch: 405/1250
torch.Size([8, 100])
tensor([0, 2, 1, 4, 5, 2, 2, 5])
Batch: 406/1250
torch.Size([8, 100])
tensor([9, 8, 5, 8, 1, 2, 1, 4])
Batch: 407/1250
torch.Size([8, 100])
tensor([0, 3, 6, 1, 2, 2, 7, 0])
Batch: 408/1250
torch.Size([8, 100])
tensor([4, 2, 5, 8, 5, 2, 7, 2])
Batch: 409/1250
torch.Size([8, 100])
tensor([3, 4, 3, 3, 7, 6, 3, 5])
Batch: 410/1250
torch.Size([8, 100])
tensor([0, 1, 9, 0, 1, 5, 8, 1])
Batch: 411/1250
torch.Size([8, 100])
tensor([5, 5, 3, 6, 6, 3, 1, 7])
Batch: 412/1250
torch.Size([8, 100])
tensor([2, 3, 4, 8, 9, 5, 7, 0])
Batch: 413/1250
torch.Size([8, 100])
tensor([1, 4, 4, 8, 5, 3, 6, 0])
Batch: 414/1250
torch.Size([8, 100])
tensor([9, 5, 1, 8, 9, 3, 1, 8])
Batch: 415/1250
torch.Size([8, 100])
tensor([5, 4, 7, 6, 8, 9, 9, 3])
Batch: 416/1250
torch.Size([8, 100])
tensor([2, 9, 7, 2, 0, 6, 7, 5])
Batch: 417/1250
torch.Size([8, 100])
tensor([2, 5, 8, 8, 7, 9, 2, 1])
Batch: 418/1250
torch.Size([8, 100])
tensor([7, 9, 6, 0, 5, 0, 0, 1])
Batch: 419/1250
torch.Size([8, 100])
tensor([8, 2, 1, 2, 9, 8, 6, 2])
Batch: 420/1250
torch.Size([8, 100])
tensor([6, 3, 6, 9, 3, 1, 5, 0])
Batch: 421/1250
torch.Size([8, 100])
tensor([0, 5, 1, 8, 7, 3, 7, 1])
Batch: 422/1250
torch.Size([8, 100])
tensor([7, 3, 9, 2, 9, 6, 4, 6])
Batch: 423/1250
torch.Size([8, 100])
tensor([6, 4, 4, 3, 9, 7, 6, 9])
Batch: 424/1250
torch.Size([8, 100])
tensor([5, 6, 2, 3, 9, 1, 1, 7])
Batch: 425/1250
torch.Size([8, 100])
tensor([0, 7, 6, 0, 2, 1, 4, 1])
Batch: 426/1250
torch.Size([8, 100])
tensor([0, 2, 6, 8, 7, 3, 1, 5])
Batch: 427/1250
torch.Size([8, 100])
tensor([0, 3, 0, 0, 0, 9, 8, 4])
Batch: 428/1250
torch.Size([8, 100])
tensor([2, 3, 2, 7, 1, 4, 7, 1])
Batch: 429/1250
torch.Size([8, 100])
tensor([6, 8, 3, 7, 4, 7, 8, 3])
Batch: 430/1250
torch.Size([8, 100])
tensor([4, 0, 1, 8, 9, 9, 4, 5])
Batch: 431/1250
torch.Size([8, 100])
tensor([5, 4, 1, 6, 5, 7, 7, 2])
Batch: 432/1250
torch.Size([8, 100])
tensor([8, 9, 4, 9, 4, 4, 0, 6])
Batch: 433/1250
torch.Size([8, 100])
tensor([4, 8, 1, 7, 6, 4, 9, 6])
Batch: 434/1250
torch.Size([8, 100])
tensor([2, 5, 2, 2, 1, 1, 3, 3])
Batch: 435/1250
torch.Size([8, 100])
tensor([0, 9, 4, 8, 0, 4, 9, 1])
Batch: 436/1250
torch.Size([8, 100])
tensor([4, 2, 4, 6, 5, 1, 7, 9])
Batch: 437/1250
torch.Size([8, 100])
tensor([7, 8, 6, 3, 1, 4, 6, 2])
Batch: 438/1250
torch.Size([8, 100])
tensor([5, 1, 9, 6, 7, 8, 9, 7])
Batch: 439/1250
torch.Size([8, 100])
tensor([5, 1, 0, 0, 8, 8, 4, 7])
Batch: 440/1250
torch.Size([8, 100])
tensor([0, 2, 2, 2, 9, 2, 3, 9])
Batch: 441/1250
torch.Size([8, 100])
tensor([3, 7, 1, 8, 0, 3, 2, 1])
Batch: 442/1250
torch.Size([8, 100])
tensor([8, 6, 7, 8, 4, 9, 5, 9])
Batch: 443/1250
torch.Size([8, 100])
tensor([3, 2, 1, 4, 7, 4, 2, 9])
Batch: 444/1250
torch.Size([8, 100])
tensor([7, 2, 5, 6, 9, 4, 8, 8])
Batch: 445/1250
torch.Size([8, 100])
tensor([4, 0, 0, 3, 9, 3, 5, 3])
Batch: 446/1250
torch.Size([8, 100])
tensor([8, 2, 5, 7, 2, 7, 6, 4])
Batch: 447/1250
torch.Size([8, 100])
tensor([2, 4, 5, 6, 2, 6, 9, 9])
Batch: 448/1250
torch.Size([8, 100])
tensor([7, 3, 8, 0, 1, 6, 0, 5])
Batch: 449/1250
torch.Size([8, 100])
tensor([2, 0, 8, 9, 6, 6, 6, 7])
Batch: 450/1250
torch.Size([8, 100])
tensor([5, 2, 4, 4, 0, 6, 6, 9])
Batch: 451/1250
torch.Size([8, 100])
tensor([6, 5, 5, 4, 7, 8, 7, 1])
Batch: 452/1250
torch.Size([8, 100])
tensor([4, 5, 7, 0, 3, 4, 9, 5])
Batch: 453/1250
torch.Size([8, 100])
tensor([3, 5, 4, 0, 0, 5, 5, 8])
Batch: 454/1250
torch.Size([8, 100])
tensor([2, 4, 0, 6, 0, 3, 5, 9])
Batch: 455/1250
torch.Size([8, 100])
tensor([7, 6, 3, 3, 4, 1, 2, 4])
Batch: 456/1250
torch.Size([8, 100])
tensor([8, 0, 5, 1, 2, 5, 2, 9])
Batch: 457/1250
torch.Size([8, 100])
tensor([9, 3, 4, 4, 6, 8, 3, 8])
Batch: 458/1250
torch.Size([8, 100])
tensor([6, 6, 8, 8, 0, 0, 6, 6])
Batch: 459/1250
torch.Size([8, 100])
tensor([1, 6, 8, 7, 8, 6, 1, 7])
Batch: 460/1250
torch.Size([8, 100])
tensor([4, 4, 0, 9, 1, 0, 7, 3])
Batch: 461/1250
torch.Size([8, 100])
tensor([9, 8, 3, 0, 7, 7, 6, 5])
Batch: 462/1250
torch.Size([8, 100])
tensor([6, 3, 9, 4, 6, 7, 3, 3])
Batch: 463/1250
torch.Size([8, 100])
tensor([5, 5, 1, 0, 1, 3, 9, 8])
Batch: 464/1250
torch.Size([8, 100])
tensor([6, 0, 4, 1, 1, 6, 9, 5])
Batch: 465/1250
torch.Size([8, 100])
tensor([1, 9, 0, 7, 3, 8, 7, 8])
Batch: 466/1250
torch.Size([8, 100])
tensor([5, 1, 9, 8, 9, 8, 7, 5])
Batch: 467/1250
torch.Size([8, 100])
tensor([9, 9, 1, 0, 0, 8, 6, 5])
Batch: 468/1250
torch.Size([8, 100])
tensor([3, 9, 1, 6, 0, 3, 6, 6])
Batch: 469/1250
torch.Size([8, 100])
tensor([5, 9, 7, 0, 8, 2, 8, 3])
Batch: 470/1250
torch.Size([8, 100])
tensor([4, 6, 1, 5, 5, 0, 3, 5])
Batch: 471/1250
torch.Size([8, 100])
tensor([2, 0, 3, 2, 1, 1, 3, 7])
Batch: 472/1250
torch.Size([8, 100])
tensor([3, 3, 6, 7, 1, 5, 2, 8])
Batch: 473/1250
torch.Size([8, 100])
tensor([6, 5, 9, 9, 7, 9, 4, 4])
Batch: 474/1250
torch.Size([8, 100])
tensor([5, 8, 2, 3, 9, 2, 4, 6])
Batch: 475/1250
torch.Size([8, 100])
tensor([0, 6, 2, 7, 3, 9, 8, 4])
Batch: 476/1250
torch.Size([8, 100])
tensor([0, 3, 5, 6, 7, 2, 7, 7])
Batch: 477/1250
torch.Size([8, 100])
tensor([1, 5, 3, 5, 6, 6, 3, 3])
Batch: 478/1250
torch.Size([8, 100])
tensor([0, 8, 3, 9, 0, 0, 1, 0])
Batch: 479/1250
torch.Size([8, 100])
tensor([1, 2, 5, 3, 8, 7, 4, 3])
Batch: 480/1250
torch.Size([8, 100])
tensor([2, 8, 4, 5, 4, 2, 2, 4])
Batch: 481/1250
torch.Size([8, 100])
tensor([6, 9, 8, 0, 2, 7, 4, 7])
Batch: 482/1250
torch.Size([8, 100])
tensor([1, 9, 0, 3, 9, 5, 4, 1])
Batch: 483/1250
torch.Size([8, 100])
tensor([9, 9, 2, 6, 5, 8, 0, 3])
Batch: 484/1250
torch.Size([8, 100])
tensor([3, 0, 3, 6, 1, 2, 5, 4])
Batch: 485/1250
torch.Size([8, 100])
tensor([9, 0, 2, 7, 2, 9, 7, 3])
Batch: 486/1250
torch.Size([8, 100])
tensor([8, 7, 8, 4, 1, 3, 9, 1])
Batch: 487/1250
torch.Size([8, 100])
tensor([8, 1, 7, 8, 0, 1, 1, 5])
Batch: 488/1250
torch.Size([8, 100])
tensor([9, 8, 8, 4, 8, 5, 2, 1])
Batch: 489/1250
torch.Size([8, 100])
tensor([6, 4, 5, 4, 9, 4, 3, 2])
Batch: 490/1250
torch.Size([8, 100])
tensor([1, 4, 2, 3, 4, 9, 9, 0])
Batch: 491/1250
torch.Size([8, 100])
tensor([4, 6, 8, 7, 2, 0, 1, 8])
Batch: 492/1250
torch.Size([8, 100])
tensor([0, 3, 3, 2, 7, 0, 7, 8])
Batch: 493/1250
torch.Size([8, 100])
tensor([4, 2, 7, 8, 0, 7, 0, 2])
Batch: 494/1250
torch.Size([8, 100])
tensor([3, 0, 3, 0, 7, 4, 6, 6])
Batch: 495/1250
torch.Size([8, 100])
tensor([8, 8, 0, 9, 1, 4, 3, 2])
Batch: 496/1250
torch.Size([8, 100])
tensor([0, 9, 4, 8, 7, 7, 6, 9])
Batch: 497/1250
torch.Size([8, 100])
tensor([4, 8, 7, 1, 7, 8, 8, 2])
Batch: 498/1250
torch.Size([8, 100])
tensor([6, 7, 1, 6, 5, 8, 0, 9])
Batch: 499/1250
torch.Size([8, 100])
tensor([7, 7, 6, 2, 6, 0, 5, 9])
Batch: 500/1250
torch.Size([8, 100])
tensor([9, 7, 5, 6, 0, 1, 1, 8])
Batch: 501/1250
torch.Size([8, 100])
tensor([0, 9, 1, 7, 8, 9, 7, 5])
Batch: 502/1250
torch.Size([8, 100])
tensor([8, 4, 2, 4, 0, 9, 2, 2])
Batch: 503/1250
torch.Size([8, 100])
tensor([0, 3, 1, 8, 2, 9, 6, 8])
Batch: 504/1250
torch.Size([8, 100])
tensor([3, 4, 0, 0, 3, 7, 1, 4])
Batch: 505/1250
torch.Size([8, 100])
tensor([3, 0, 6, 8, 7, 5, 9, 5])
Batch: 506/1250
torch.Size([8, 100])
tensor([2, 5, 1, 0, 0, 2, 4, 9])
Batch: 507/1250
torch.Size([8, 100])
tensor([1, 9, 1, 7, 5, 8, 6, 1])
Batch: 508/1250
torch.Size([8, 100])
tensor([8, 6, 5, 5, 6, 5, 2, 2])
Batch: 509/1250
torch.Size([8, 100])
tensor([3, 6, 9, 4, 3, 1, 7, 8])
Batch: 510/1250
torch.Size([8, 100])
tensor([9, 3, 2, 5, 2, 8, 3, 4])
Batch: 511/1250
torch.Size([8, 100])
tensor([2, 5, 2, 5, 3, 0, 2, 9])
Batch: 512/1250
torch.Size([8, 100])
tensor([5, 7, 9, 0, 5, 4, 0, 9])
Batch: 513/1250
torch.Size([8, 100])
tensor([7, 5, 0, 3, 4, 1, 5, 7])
Batch: 514/1250
torch.Size([8, 100])
tensor([1, 1, 9, 1, 8, 5, 5, 9])
Batch: 515/1250
torch.Size([8, 100])
tensor([1, 8, 2, 2, 1, 5, 9, 2])
Batch: 516/1250
torch.Size([8, 100])
tensor([4, 0, 7, 2, 6, 4, 2, 3])
Batch: 517/1250
torch.Size([8, 100])
tensor([4, 2, 7, 0, 9, 4, 1, 0])
Batch: 518/1250
torch.Size([8, 100])
tensor([6, 8, 1, 4, 3, 1, 7, 5])
Batch: 519/1250
torch.Size([8, 100])
tensor([1, 6, 8, 7, 8, 5, 5, 8])
Batch: 520/1250
torch.Size([8, 100])
tensor([1, 5, 8, 0, 6, 5, 1, 0])
Batch: 521/1250
torch.Size([8, 100])
tensor([8, 2, 5, 6, 8, 4, 6, 0])
Batch: 522/1250
torch.Size([8, 100])
tensor([2, 7, 8, 1, 1, 6, 7, 5])
Batch: 523/1250
torch.Size([8, 100])
tensor([3, 9, 6, 6, 8, 6, 9, 6])
Batch: 524/1250
torch.Size([8, 100])
tensor([6, 0, 5, 7, 8, 5, 8, 9])
Batch: 525/1250
torch.Size([8, 100])
tensor([3, 1, 7, 4, 0, 6, 6, 7])
Batch: 526/1250
torch.Size([8, 100])
tensor([1, 2, 3, 9, 1, 8, 4, 2])
Batch: 527/1250
torch.Size([8, 100])
tensor([9, 7, 5, 2, 0, 0, 2, 2])
Batch: 528/1250
torch.Size([8, 100])
tensor([7, 1, 6, 9, 0, 8, 3, 3])
Batch: 529/1250
torch.Size([8, 100])
tensor([9, 6, 9, 4, 2, 0, 7, 2])
Batch: 530/1250
torch.Size([8, 100])
tensor([2, 7, 1, 1, 1, 1, 5, 7])
Batch: 531/1250
torch.Size([8, 100])
tensor([1, 1, 3, 9, 8, 1, 6, 7])
Batch: 532/1250
torch.Size([8, 100])
tensor([7, 4, 1, 1, 8, 3, 4, 0])
Batch: 533/1250
torch.Size([8, 100])
tensor([5, 2, 5, 1, 6, 4, 8, 5])
Batch: 534/1250
torch.Size([8, 100])
tensor([5, 1, 0, 8, 2, 3, 0, 4])
Batch: 535/1250
torch.Size([8, 100])
tensor([4, 8, 9, 0, 5, 5, 4, 2])
Batch: 536/1250
torch.Size([8, 100])
tensor([4, 2, 4, 4, 7, 8, 4, 5])
Batch: 537/1250
torch.Size([8, 100])
tensor([4, 9, 4, 6, 2, 4, 1, 2])
Batch: 538/1250
torch.Size([8, 100])
tensor([6, 8, 0, 1, 1, 2, 3, 7])
Batch: 539/1250
torch.Size([8, 100])
tensor([5, 9, 2, 3, 1, 9, 6, 8])
Batch: 540/1250
torch.Size([8, 100])
tensor([4, 4, 3, 9, 1, 8, 0, 3])
Batch: 541/1250
torch.Size([8, 100])
tensor([7, 4, 9, 1, 7, 0, 3, 6])
Batch: 542/1250
torch.Size([8, 100])
tensor([1, 1, 0, 9, 4, 5, 1, 1])
Batch: 543/1250
torch.Size([8, 100])
tensor([4, 2, 9, 3, 5, 4, 6, 3])
Batch: 544/1250
torch.Size([8, 100])
tensor([3, 4, 0, 5, 1, 9, 1, 1])
Batch: 545/1250
torch.Size([8, 100])
tensor([8, 4, 0, 6, 0, 8, 4, 4])
Batch: 546/1250
torch.Size([8, 100])
tensor([0, 0, 6, 8, 1, 8, 8, 6])
Batch: 547/1250
torch.Size([8, 100])
tensor([7, 9, 5, 0, 5, 6, 9, 9])
Batch: 548/1250
torch.Size([8, 100])
tensor([3, 6, 9, 5, 9, 0, 8, 4])
Batch: 549/1250
torch.Size([8, 100])
tensor([5, 8, 0, 1, 2, 8, 0, 7])
Batch: 550/1250
torch.Size([8, 100])
tensor([0, 1, 5, 9, 2, 4, 4, 8])
Batch: 551/1250
torch.Size([8, 100])
tensor([4, 6, 2, 8, 4, 6, 2, 6])
Batch: 552/1250
torch.Size([8, 100])
tensor([2, 4, 7, 7, 6, 1, 9, 2])
Batch: 553/1250
torch.Size([8, 100])
tensor([7, 9, 5, 2, 7, 3, 3, 6])
Batch: 554/1250
torch.Size([8, 100])
tensor([2, 9, 7, 9, 8, 2, 6, 0])
Batch: 555/1250
torch.Size([8, 100])
tensor([1, 9, 6, 5, 9, 1, 9, 4])
Batch: 556/1250
torch.Size([8, 100])
tensor([0, 6, 7, 8, 3, 4, 3, 2])
Batch: 557/1250
torch.Size([8, 100])
tensor([9, 2, 5, 6, 3, 2, 1, 4])
Batch: 558/1250
torch.Size([8, 100])
tensor([3, 7, 1, 7, 1, 9, 6, 6])
Batch: 559/1250
torch.Size([8, 100])
tensor([2, 1, 0, 3, 8, 4, 2, 4])
Batch: 560/1250
torch.Size([8, 100])
tensor([8, 9, 2, 9, 2, 8, 4, 7])
Batch: 561/1250
torch.Size([8, 100])
tensor([8, 1, 8, 1, 6, 4, 5, 3])
Batch: 562/1250
torch.Size([8, 100])
tensor([8, 4, 7, 1, 0, 5, 0, 2])
Batch: 563/1250
torch.Size([8, 100])
tensor([0, 8, 6, 8, 9, 0, 8, 2])
Batch: 564/1250
torch.Size([8, 100])
tensor([7, 2, 1, 9, 3, 6, 9, 6])
Batch: 565/1250
torch.Size([8, 100])
tensor([6, 6, 2, 1, 7, 3, 9, 7])
Batch: 566/1250
torch.Size([8, 100])
tensor([0, 6, 7, 4, 5, 3, 7, 4])
Batch: 567/1250
torch.Size([8, 100])
tensor([2, 1, 2, 0, 1, 2, 8, 5])
Batch: 568/1250
torch.Size([8, 100])
tensor([0, 5, 7, 2, 2, 8, 5, 6])
Batch: 569/1250
torch.Size([8, 100])
tensor([6, 7, 4, 7, 0, 5, 3, 6])
Batch: 570/1250
torch.Size([8, 100])
tensor([2, 9, 1, 9, 4, 4, 6, 5])
Batch: 571/1250
torch.Size([8, 100])
tensor([4, 6, 0, 5, 4, 8, 4, 1])
Batch: 572/1250
torch.Size([8, 100])
tensor([9, 6, 1, 8, 5, 8, 9, 1])
Batch: 573/1250
torch.Size([8, 100])
tensor([7, 5, 0, 4, 0, 3, 3, 2])
Batch: 574/1250
torch.Size([8, 100])
tensor([3, 0, 2, 4, 1, 7, 8, 9])
Batch: 575/1250
torch.Size([8, 100])
tensor([1, 9, 8, 9, 8, 7, 3, 8])
Batch: 576/1250
torch.Size([8, 100])
tensor([0, 1, 4, 7, 0, 8, 3, 8])
Batch: 577/1250
torch.Size([8, 100])
tensor([6, 4, 8, 7, 7, 2, 3, 2])
Batch: 578/1250
torch.Size([8, 100])
tensor([4, 5, 6, 8, 4, 0, 1, 7])
Batch: 579/1250
torch.Size([8, 100])
tensor([3, 5, 6, 2, 2, 1, 8, 7])
Batch: 580/1250
torch.Size([8, 100])
tensor([2, 1, 1, 3, 7, 1, 5, 6])
Batch: 581/1250
torch.Size([8, 100])
tensor([4, 6, 5, 1, 8, 9, 7, 8])
Batch: 582/1250
torch.Size([8, 100])
tensor([2, 6, 2, 3, 9, 6, 8, 7])
Batch: 583/1250
torch.Size([8, 100])
tensor([0, 0, 1, 9, 7, 1, 8, 1])
Batch: 584/1250
torch.Size([8, 100])
tensor([9, 9, 9, 9, 1, 0, 3, 3])
Batch: 585/1250
torch.Size([8, 100])
tensor([5, 3, 3, 9, 2, 9, 8, 7])
Batch: 586/1250
torch.Size([8, 100])
tensor([4, 1, 3, 8, 9, 5, 5, 3])
Batch: 587/1250
torch.Size([8, 100])
tensor([1, 1, 3, 7, 6, 1, 4, 3])
Batch: 588/1250
torch.Size([8, 100])
tensor([3, 6, 3, 4, 5, 8, 2, 1])
Batch: 589/1250
torch.Size([8, 100])
tensor([1, 1, 1, 6, 6, 1, 6, 1])
Batch: 590/1250
torch.Size([8, 100])
tensor([9, 2, 7, 3, 6, 7, 5, 1])
Batch: 591/1250
torch.Size([8, 100])
tensor([9, 4, 2, 2, 1, 1, 6, 4])
Batch: 592/1250
torch.Size([8, 100])
tensor([8, 0, 2, 0, 1, 9, 4, 5])
Batch: 593/1250
torch.Size([8, 100])
tensor([0, 0, 4, 2, 9, 9, 2, 2])
Batch: 594/1250
torch.Size([8, 100])
tensor([7, 8, 4, 1, 3, 1, 8, 5])
Batch: 595/1250
torch.Size([8, 100])
tensor([2, 3, 9, 9, 8, 2, 0, 8])
Batch: 596/1250
torch.Size([8, 100])
tensor([2, 5, 7, 1, 6, 1, 5, 4])
Batch: 597/1250
torch.Size([8, 100])
tensor([1, 7, 3, 6, 4, 1, 6, 7])
Batch: 598/1250
torch.Size([8, 100])
tensor([0, 6, 7, 3, 7, 3, 6, 8])
Batch: 599/1250
torch.Size([8, 100])
tensor([0, 2, 2, 9, 5, 7, 6, 4])
Batch: 600/1250
torch.Size([8, 100])
tensor([2, 5, 8, 6, 2, 5, 7, 5])
Batch: 601/1250
torch.Size([8, 100])
tensor([0, 1, 1, 0, 1, 3, 9, 1])
Batch: 602/1250
torch.Size([8, 100])
tensor([3, 6, 1, 6, 6, 7, 3, 8])
Batch: 603/1250
torch.Size([8, 100])
tensor([4, 9, 3, 3, 5, 0, 1, 8])
Batch: 604/1250
torch.Size([8, 100])
tensor([4, 8, 8, 9, 8, 7, 4, 3])
Batch: 605/1250
torch.Size([8, 100])
tensor([9, 8, 7, 7, 0, 9, 2, 6])
Batch: 606/1250
torch.Size([8, 100])
tensor([1, 6, 8, 2, 0, 8, 6, 1])
Batch: 607/1250
torch.Size([8, 100])
tensor([4, 2, 0, 3, 4, 3, 4, 2])
Batch: 608/1250
torch.Size([8, 100])
tensor([3, 6, 0, 9, 1, 9, 8, 8])
Batch: 609/1250
torch.Size([8, 100])
tensor([9, 5, 3, 9, 8, 0, 3, 1])
Batch: 610/1250
torch.Size([8, 100])
tensor([8, 7, 4, 1, 3, 8, 2, 7])
Batch: 611/1250
torch.Size([8, 100])
tensor([3, 2, 8, 3, 8, 1, 6, 4])
Batch: 612/1250
torch.Size([8, 100])
tensor([8, 7, 4, 0, 7, 4, 1, 0])
Batch: 613/1250
torch.Size([8, 100])
tensor([7, 7, 6, 5, 8, 3, 9, 3])
Batch: 614/1250
torch.Size([8, 100])
tensor([2, 6, 3, 1, 1, 8, 1, 9])
Batch: 615/1250
torch.Size([8, 100])
tensor([9, 8, 0, 8, 9, 8, 5, 8])
Batch: 616/1250
torch.Size([8, 100])
tensor([8, 8, 8, 2, 5, 1, 2, 8])
Batch: 617/1250
torch.Size([8, 100])
tensor([3, 5, 9, 6, 3, 4, 1, 9])
Batch: 618/1250
torch.Size([8, 100])
tensor([2, 9, 7, 7, 3, 6, 5, 0])
Batch: 619/1250
torch.Size([8, 100])
tensor([1, 3, 1, 9, 0, 3, 9, 2])
Batch: 620/1250
torch.Size([8, 100])
tensor([9, 7, 9, 4, 9, 3, 1, 2])
Batch: 621/1250
torch.Size([8, 100])
tensor([6, 3, 7, 8, 6, 7, 4, 3])
Batch: 622/1250
torch.Size([8, 100])
tensor([8, 0, 9, 5, 3, 3, 1, 5])
Batch: 623/1250
torch.Size([8, 100])
tensor([7, 1, 7, 5, 7, 0, 8, 5])
Batch: 624/1250
torch.Size([8, 100])
tensor([9, 0, 8, 1, 9, 8, 7, 3])
Batch: 625/1250
torch.Size([8, 100])
tensor([1, 7, 1, 2, 2, 0, 0, 0])
Batch: 626/1250
torch.Size([8, 100])
tensor([2, 4, 0, 2, 9, 1, 0, 9])
Batch: 627/1250
torch.Size([8, 100])
tensor([9, 3, 9, 2, 6, 5, 8, 6])
Batch: 628/1250
torch.Size([8, 100])
tensor([7, 8, 0, 0, 0, 7, 5, 0])
Batch: 629/1250
torch.Size([8, 100])
tensor([4, 4, 2, 8, 2, 3, 0, 6])
Batch: 630/1250
torch.Size([8, 100])
tensor([0, 1, 5, 0, 3, 3, 4, 1])
Batch: 631/1250
torch.Size([8, 100])
tensor([9, 9, 1, 4, 2, 3, 5, 2])
Batch: 632/1250
torch.Size([8, 100])
tensor([6, 5, 4, 9, 8, 1, 9, 0])
Batch: 633/1250
torch.Size([8, 100])
tensor([7, 0, 1, 1, 7, 8, 3, 2])
Batch: 634/1250
torch.Size([8, 100])
tensor([6, 3, 4, 3, 7, 7, 5, 4])
Batch: 635/1250
torch.Size([8, 100])
tensor([2, 3, 7, 6, 9, 6, 0, 1])
Batch: 636/1250
torch.Size([8, 100])
tensor([3, 7, 0, 4, 7, 6, 5, 8])
Batch: 637/1250
torch.Size([8, 100])
tensor([6, 5, 9, 1, 3, 3, 1, 3])
Batch: 638/1250
torch.Size([8, 100])
tensor([0, 8, 3, 1, 3, 0, 1, 3])
Batch: 639/1250
torch.Size([8, 100])
tensor([9, 0, 0, 2, 4, 9, 5, 8])
Batch: 640/1250
torch.Size([8, 100])
tensor([2, 4, 5, 5, 3, 7, 7, 3])
Batch: 641/1250
torch.Size([8, 100])
tensor([0, 5, 3, 6, 6, 2, 3, 0])
Batch: 642/1250
torch.Size([8, 100])
tensor([2, 5, 0, 9, 5, 8, 7, 9])
Batch: 643/1250
torch.Size([8, 100])
tensor([2, 1, 9, 7, 1, 7, 7, 1])
Batch: 644/1250
torch.Size([8, 100])
tensor([3, 1, 7, 0, 2, 3, 7, 1])
Batch: 645/1250
torch.Size([8, 100])
tensor([1, 3, 4, 9, 3, 2, 7, 3])
Batch: 646/1250
torch.Size([8, 100])
tensor([3, 9, 3, 0, 3, 0, 5, 8])
Batch: 647/1250
torch.Size([8, 100])
tensor([1, 4, 2, 7, 5, 2, 0, 8])
Batch: 648/1250
torch.Size([8, 100])
tensor([4, 2, 6, 2, 4, 7, 4, 0])
Batch: 649/1250
torch.Size([8, 100])
tensor([2, 7, 1, 4, 7, 2, 5, 3])
Batch: 650/1250
torch.Size([8, 100])
tensor([5, 3, 5, 9, 9, 4, 2, 2])
Batch: 651/1250
torch.Size([8, 100])
tensor([7, 7, 6, 5, 0, 5, 0, 7])
Batch: 652/1250
torch.Size([8, 100])
tensor([0, 9, 5, 2, 2, 0, 2, 7])
Batch: 653/1250
torch.Size([8, 100])
tensor([0, 7, 2, 9, 7, 1, 9, 1])
Batch: 654/1250
torch.Size([8, 100])
tensor([7, 2, 7, 1, 1, 9, 1, 7])
Batch: 655/1250
torch.Size([8, 100])
tensor([1, 5, 0, 7, 0, 0, 9, 3])
Batch: 656/1250
torch.Size([8, 100])
tensor([6, 9, 4, 6, 5, 4, 8, 6])
Batch: 657/1250
torch.Size([8, 100])
tensor([1, 2, 7, 9, 0, 5, 7, 7])
Batch: 658/1250
torch.Size([8, 100])
tensor([4, 2, 1, 6, 7, 1, 5, 5])
Batch: 659/1250
torch.Size([8, 100])
tensor([1, 1, 3, 9, 2, 8, 9, 6])
Batch: 660/1250
torch.Size([8, 100])
tensor([0, 0, 6, 4, 9, 4, 2, 6])
Batch: 661/1250
torch.Size([8, 100])
tensor([9, 8, 8, 3, 2, 3, 7, 7])
Batch: 662/1250
torch.Size([8, 100])
tensor([5, 4, 0, 4, 9, 1, 2, 6])
Batch: 663/1250
torch.Size([8, 100])
tensor([6, 0, 8, 1, 0, 5, 2, 6])
Batch: 664/1250
torch.Size([8, 100])
tensor([2, 6, 6, 8, 3, 0, 2, 0])
Batch: 665/1250
torch.Size([8, 100])
tensor([4, 3, 4, 2, 7, 6, 9, 7])
Batch: 666/1250
torch.Size([8, 100])
tensor([0, 6, 3, 9, 4, 1, 6, 0])
Batch: 667/1250
torch.Size([8, 100])
tensor([4, 5, 6, 7, 7, 9, 9, 1])
Batch: 668/1250
torch.Size([8, 100])
tensor([3, 5, 6, 1, 5, 0, 9, 3])
Batch: 669/1250
torch.Size([8, 100])
tensor([8, 3, 9, 0, 8, 0, 2, 3])
Batch: 670/1250
torch.Size([8, 100])
tensor([9, 4, 2, 5, 5, 9, 0, 5])
Batch: 671/1250
torch.Size([8, 100])
tensor([0, 4, 8, 5, 9, 6, 6, 9])
Batch: 672/1250
torch.Size([8, 100])
tensor([2, 0, 6, 1, 0, 7, 2, 9])
Batch: 673/1250
torch.Size([8, 100])
tensor([1, 1, 7, 6, 0, 0, 8, 3])
Batch: 674/1250
torch.Size([8, 100])
tensor([0, 0, 0, 0, 1, 1, 6, 7])
Batch: 675/1250
torch.Size([8, 100])
tensor([9, 5, 8, 8, 8, 7, 3, 4])
Batch: 676/1250
torch.Size([8, 100])
tensor([1, 8, 4, 4, 3, 2, 6, 4])
Batch: 677/1250
torch.Size([8, 100])
tensor([9, 0, 2, 5, 8, 6, 8, 9])
Batch: 678/1250
torch.Size([8, 100])
tensor([8, 8, 1, 0, 9, 9, 1, 9])
Batch: 679/1250
torch.Size([8, 100])
tensor([1, 0, 6, 6, 8, 9, 2, 8])
Batch: 680/1250
torch.Size([8, 100])
tensor([5, 1, 0, 3, 5, 8, 0, 4])
Batch: 681/1250
torch.Size([8, 100])
tensor([7, 2, 9, 2, 4, 0, 9, 5])
Batch: 682/1250
torch.Size([8, 100])
tensor([5, 3, 4, 0, 3, 8, 2, 1])
Batch: 683/1250
torch.Size([8, 100])
tensor([5, 1, 6, 5, 3, 5, 4, 3])
Batch: 684/1250
torch.Size([8, 100])
tensor([3, 7, 4, 1, 8, 5, 4, 8])
Batch: 685/1250
torch.Size([8, 100])
tensor([5, 0, 3, 0, 8, 5, 2, 2])
Batch: 686/1250
torch.Size([8, 100])
tensor([4, 3, 1, 6, 5, 8, 9, 3])
Batch: 687/1250
torch.Size([8, 100])
tensor([7, 3, 1, 6, 3, 8, 3, 2])
Batch: 688/1250
torch.Size([8, 100])
tensor([0, 4, 1, 9, 0, 9, 1, 2])
Batch: 689/1250
torch.Size([8, 100])
tensor([8, 7, 8, 3, 4, 1, 9, 4])
Batch: 690/1250
torch.Size([8, 100])
tensor([0, 6, 9, 4, 5, 8, 5, 4])
Batch: 691/1250
torch.Size([8, 100])
tensor([5, 7, 8, 4, 9, 7, 8, 4])
Batch: 692/1250
torch.Size([8, 100])
tensor([9, 2, 5, 1, 9, 2, 2, 1])
Batch: 693/1250
torch.Size([8, 100])
tensor([0, 4, 6, 4, 9, 7, 5, 9])
Batch: 694/1250
torch.Size([8, 100])
tensor([3, 5, 4, 4, 7, 0, 5, 1])
Batch: 695/1250
torch.Size([8, 100])
tensor([1, 6, 4, 5, 5, 6, 3, 5])
Batch: 696/1250
torch.Size([8, 100])
tensor([0, 7, 6, 2, 9, 0, 0, 5])
Batch: 697/1250
torch.Size([8, 100])
tensor([9, 1, 8, 4, 3, 6, 8, 2])
Batch: 698/1250
torch.Size([8, 100])
tensor([7, 2, 0, 2, 6, 9, 9, 2])
Batch: 699/1250
torch.Size([8, 100])
tensor([6, 4, 0, 9, 3, 5, 9, 1])
Batch: 700/1250
torch.Size([8, 100])
tensor([9, 8, 9, 7, 3, 3, 0, 2])
Batch: 701/1250
torch.Size([8, 100])
tensor([2, 7, 9, 3, 2, 4, 8, 7])
Batch: 702/1250
torch.Size([8, 100])
tensor([2, 0, 6, 7, 7, 7, 4, 3])
Batch: 703/1250
torch.Size([8, 100])
tensor([6, 9, 5, 5, 7, 1, 2, 9])
Batch: 704/1250
torch.Size([8, 100])
tensor([7, 7, 0, 0, 1, 0, 1, 0])
Batch: 705/1250
torch.Size([8, 100])
tensor([2, 2, 9, 5, 8, 1, 8, 4])
Batch: 706/1250
torch.Size([8, 100])
tensor([9, 3, 9, 1, 0, 3, 2, 7])
Batch: 707/1250
torch.Size([8, 100])
tensor([6, 0, 0, 7, 8, 9, 5, 0])
Batch: 708/1250
torch.Size([8, 100])
tensor([3, 3, 1, 9, 8, 8, 9, 7])
Batch: 709/1250
torch.Size([8, 100])
tensor([1, 7, 6, 1, 6, 7, 6, 1])
Batch: 710/1250
torch.Size([8, 100])
tensor([0, 2, 9, 1, 8, 7, 5, 3])
Batch: 711/1250
torch.Size([8, 100])
tensor([7, 2, 1, 0, 5, 1, 0, 2])
Batch: 712/1250
torch.Size([8, 100])
tensor([6, 5, 6, 0, 9, 4, 3, 2])
Batch: 713/1250
torch.Size([8, 100])
tensor([7, 9, 9, 7, 3, 7, 4, 1])
Batch: 714/1250
torch.Size([8, 100])
tensor([9, 5, 4, 7, 6, 9, 0, 7])
Batch: 715/1250
torch.Size([8, 100])
tensor([1, 8, 2, 3, 9, 1, 6, 5])
Batch: 716/1250
torch.Size([8, 100])
tensor([9, 2, 3, 2, 8, 2, 8, 9])
Batch: 717/1250
torch.Size([8, 100])
tensor([4, 9, 5, 4, 7, 2, 4, 4])
Batch: 718/1250
torch.Size([8, 100])
tensor([9, 1, 6, 3, 0, 3, 7, 2])
Batch: 719/1250
torch.Size([8, 100])
tensor([1, 3, 3, 8, 9, 4, 5, 6])
Batch: 720/1250
torch.Size([8, 100])
tensor([1, 8, 2, 3, 4, 8, 7, 0])
Batch: 721/1250
torch.Size([8, 100])
tensor([4, 2, 4, 7, 6, 8, 0, 0])
Batch: 722/1250
torch.Size([8, 100])
tensor([2, 7, 7, 3, 1, 3, 4, 7])
Batch: 723/1250
torch.Size([8, 100])
tensor([1, 5, 0, 6, 5, 9, 9, 1])
Batch: 724/1250
torch.Size([8, 100])
tensor([4, 5, 3, 6, 2, 9, 2, 9])
Batch: 725/1250
torch.Size([8, 100])
tensor([0, 2, 6, 8, 5, 0, 4, 4])
Batch: 726/1250
torch.Size([8, 100])
tensor([5, 3, 1, 5, 9, 3, 7, 0])
Batch: 727/1250
torch.Size([8, 100])
tensor([2, 1, 8, 5, 3, 6, 0, 4])
Batch: 728/1250
torch.Size([8, 100])
tensor([8, 6, 8, 9, 9, 7, 4, 8])
Batch: 729/1250
torch.Size([8, 100])
tensor([0, 4, 1, 2, 1, 7, 6, 3])
Batch: 730/1250
torch.Size([8, 100])
tensor([8, 8, 5, 3, 8, 7, 1, 2])
Batch: 731/1250
torch.Size([8, 100])
tensor([7, 0, 1, 4, 6, 7, 3, 5])
Batch: 732/1250
torch.Size([8, 100])
tensor([9, 3, 9, 3, 8, 4, 3, 3])
Batch: 733/1250
torch.Size([8, 100])
tensor([4, 2, 0, 6, 1, 8, 5, 6])
Batch: 734/1250
torch.Size([8, 100])
tensor([6, 0, 5, 1, 5, 1, 1, 9])
Batch: 735/1250
torch.Size([8, 100])
tensor([9, 4, 1, 4, 4, 5, 2, 3])
Batch: 736/1250
torch.Size([8, 100])
tensor([6, 3, 1, 8, 2, 2, 6, 3])
Batch: 737/1250
torch.Size([8, 100])
tensor([8, 3, 5, 0, 7, 1, 3, 3])
Batch: 738/1250
torch.Size([8, 100])
tensor([1, 1, 6, 7, 7, 1, 0, 2])
Batch: 739/1250
torch.Size([8, 100])
tensor([3, 9, 2, 2, 1, 3, 9, 6])
Batch: 740/1250
torch.Size([8, 100])
tensor([5, 1, 5, 8, 9, 7, 9, 1])
Batch: 741/1250
torch.Size([8, 100])
tensor([3, 0, 5, 0, 3, 2, 9, 1])
Batch: 742/1250
torch.Size([8, 100])
tensor([7, 3, 8, 8, 8, 7, 3, 3])
Batch: 743/1250
torch.Size([8, 100])
tensor([1, 1, 1, 2, 4, 0, 3, 3])
Batch: 744/1250
torch.Size([8, 100])
tensor([1, 3, 1, 1, 5, 1, 9, 1])
Batch: 745/1250
torch.Size([8, 100])
tensor([8, 2, 3, 9, 4, 4, 7, 0])
Batch: 746/1250
torch.Size([8, 100])
tensor([7, 8, 6, 9, 1, 5, 0, 2])
Batch: 747/1250
torch.Size([8, 100])
tensor([7, 6, 2, 3, 8, 7, 9, 1])
Batch: 748/1250
torch.Size([8, 100])
tensor([8, 6, 1, 4, 1, 8, 8, 6])
Batch: 749/1250
torch.Size([8, 100])
tensor([6, 1, 2, 8, 6, 0, 4, 3])
Batch: 750/1250
torch.Size([8, 100])
tensor([2, 1, 4, 7, 6, 2, 3, 9])
Batch: 751/1250
torch.Size([8, 100])
tensor([7, 9, 2, 6, 2, 4, 9, 4])
Batch: 752/1250
torch.Size([8, 100])
tensor([8, 2, 5, 0, 2, 0, 1, 0])
Batch: 753/1250
torch.Size([8, 100])
tensor([2, 6, 3, 4, 7, 3, 1, 5])
Batch: 754/1250
torch.Size([8, 100])
tensor([9, 4, 9, 2, 8, 4, 3, 7])
Batch: 755/1250
torch.Size([8, 100])
tensor([4, 5, 9, 6, 2, 1, 3, 8])
Batch: 756/1250
torch.Size([8, 100])
tensor([8, 1, 8, 8, 2, 9, 7, 6])
Batch: 757/1250
torch.Size([8, 100])
tensor([7, 0, 0, 5, 1, 7, 7, 1])
Batch: 758/1250
torch.Size([8, 100])
tensor([6, 5, 5, 4, 6, 2, 6, 3])
Batch: 759/1250
torch.Size([8, 100])
tensor([1, 9, 5, 1, 1, 4, 8, 3])
Batch: 760/1250
torch.Size([8, 100])
tensor([4, 7, 5, 8, 0, 4, 2, 9])
Batch: 761/1250
torch.Size([8, 100])
tensor([7, 2, 7, 4, 1, 5, 7, 9])
Batch: 762/1250
torch.Size([8, 100])
tensor([6, 0, 2, 4, 2, 2, 1, 9])
Batch: 763/1250
torch.Size([8, 100])
tensor([0, 4, 0, 1, 9, 6, 6, 1])
Batch: 764/1250
torch.Size([8, 100])
tensor([7, 3, 8, 1, 5, 5, 6, 7])
Batch: 765/1250
torch.Size([8, 100])
tensor([4, 4, 0, 8, 0, 4, 4, 3])
Batch: 766/1250
torch.Size([8, 100])
tensor([9, 5, 7, 6, 4, 5, 2, 5])
Batch: 767/1250
torch.Size([8, 100])
tensor([8, 1, 8, 0, 4, 7, 0, 5])
Batch: 768/1250
torch.Size([8, 100])
tensor([3, 4, 0, 8, 5, 4, 3, 3])
Batch: 769/1250
torch.Size([8, 100])
tensor([5, 3, 4, 1, 3, 2, 8, 3])
Batch: 770/1250
torch.Size([8, 100])
tensor([0, 1, 8, 9, 7, 0, 9, 4])
Batch: 771/1250
torch.Size([8, 100])
tensor([2, 1, 8, 4, 6, 8, 6, 3])
Batch: 772/1250
torch.Size([8, 100])
tensor([6, 1, 1, 8, 0, 7, 9, 2])
Batch: 773/1250
torch.Size([8, 100])
tensor([4, 1, 3, 5, 8, 3, 0, 4])
Batch: 774/1250
torch.Size([8, 100])
tensor([7, 2, 0, 5, 6, 1, 7, 2])
Batch: 775/1250
torch.Size([8, 100])
tensor([7, 0, 2, 6, 4, 3, 8, 7])
Batch: 776/1250
torch.Size([8, 100])
tensor([3, 4, 1, 0, 9, 7, 4, 8])
Batch: 777/1250
torch.Size([8, 100])
tensor([8, 3, 0, 3, 1, 5, 5, 7])
Batch: 778/1250
torch.Size([8, 100])
tensor([7, 9, 1, 9, 1, 4, 1, 2])
Batch: 779/1250
torch.Size([8, 100])
tensor([3, 6, 1, 6, 0, 6, 9, 5])
Batch: 780/1250
torch.Size([8, 100])
tensor([7, 5, 6, 6, 5, 9, 6, 8])
Batch: 781/1250
torch.Size([8, 100])
tensor([7, 1, 3, 4, 3, 9, 5, 1])
Batch: 782/1250
torch.Size([8, 100])
tensor([7, 4, 7, 7, 7, 5, 2, 7])
Batch: 783/1250
torch.Size([8, 100])
tensor([9, 2, 9, 5, 0, 0, 4, 6])
Batch: 784/1250
torch.Size([8, 100])
tensor([9, 4, 9, 4, 0, 0, 5, 3])
Batch: 785/1250
torch.Size([8, 100])
tensor([8, 6, 0, 3, 0, 4, 7, 8])
Batch: 786/1250
torch.Size([8, 100])
tensor([2, 1, 7, 1, 7, 4, 7, 7])
Batch: 787/1250
torch.Size([8, 100])
tensor([1, 9, 7, 2, 3, 6, 0, 1])
Batch: 788/1250
torch.Size([8, 100])
tensor([8, 5, 5, 3, 3, 5, 4, 0])
Batch: 789/1250
torch.Size([8, 100])
tensor([2, 2, 1, 2, 4, 2, 2, 5])
Batch: 790/1250
torch.Size([8, 100])
tensor([2, 1, 6, 4, 6, 6, 5, 1])
Batch: 791/1250
torch.Size([8, 100])
tensor([1, 6, 3, 5, 3, 7, 1, 6])
Batch: 792/1250
torch.Size([8, 100])
tensor([3, 3, 6, 9, 3, 6, 1, 7])
Batch: 793/1250
torch.Size([8, 100])
tensor([0, 5, 4, 7, 6, 2, 3, 8])
Batch: 794/1250
torch.Size([8, 100])
tensor([7, 2, 0, 7, 0, 2, 3, 9])
Batch: 795/1250
torch.Size([8, 100])
tensor([9, 3, 9, 9, 7, 7, 2, 9])
Batch: 796/1250
torch.Size([8, 100])
tensor([8, 1, 9, 8, 9, 0, 2, 5])
Batch: 797/1250
torch.Size([8, 100])
tensor([9, 0, 5, 0, 1, 2, 1, 6])
Batch: 798/1250
torch.Size([8, 100])
tensor([3, 4, 4, 9, 7, 3, 9, 0])
Batch: 799/1250
torch.Size([8, 100])
tensor([5, 2, 3, 6, 1, 6, 9, 5])
Batch: 800/1250
torch.Size([8, 100])
tensor([2, 0, 5, 7, 0, 5, 8, 7])
Batch: 801/1250
torch.Size([8, 100])
tensor([9, 2, 2, 0, 4, 0, 0, 1])
Batch: 802/1250
torch.Size([8, 100])
tensor([1, 5, 5, 1, 5, 9, 2, 2])
Batch: 803/1250
torch.Size([8, 100])
tensor([6, 0, 2, 3, 5, 9, 1, 7])
Batch: 804/1250
torch.Size([8, 100])
tensor([2, 7, 6, 0, 6, 8, 9, 1])
Batch: 805/1250
torch.Size([8, 100])
tensor([4, 6, 2, 3, 0, 2, 2, 3])
Batch: 806/1250
torch.Size([8, 100])
tensor([7, 6, 2, 5, 5, 5, 2, 7])
Batch: 807/1250
torch.Size([8, 100])
tensor([5, 2, 5, 8, 7, 6, 0, 3])
Batch: 808/1250
torch.Size([8, 100])
tensor([1, 4, 7, 7, 0, 6, 8, 8])
Batch: 809/1250
torch.Size([8, 100])
tensor([8, 2, 7, 7, 1, 1, 6, 4])
Batch: 810/1250
torch.Size([8, 100])
tensor([2, 9, 8, 9, 5, 3, 8, 9])
Batch: 811/1250
torch.Size([8, 100])
tensor([2, 0, 8, 0, 7, 8, 1, 7])
Batch: 812/1250
torch.Size([8, 100])
tensor([7, 0, 5, 3, 6, 4, 0, 0])
Batch: 813/1250
torch.Size([8, 100])
tensor([3, 2, 3, 9, 7, 7, 4, 1])
Batch: 814/1250
torch.Size([8, 100])
tensor([6, 6, 7, 1, 5, 2, 0, 7])
Batch: 815/1250
torch.Size([8, 100])
tensor([2, 9, 8, 9, 7, 0, 1, 5])
Batch: 816/1250
torch.Size([8, 100])
tensor([7, 7, 4, 0, 0, 7, 0, 9])
Batch: 817/1250
torch.Size([8, 100])
tensor([6, 1, 0, 8, 4, 1, 5, 8])
Batch: 818/1250
torch.Size([8, 100])
tensor([6, 8, 2, 5, 6, 7, 5, 4])
Batch: 819/1250
torch.Size([8, 100])
tensor([4, 6, 1, 9, 3, 8, 2, 0])
Batch: 820/1250
torch.Size([8, 100])
tensor([8, 1, 9, 4, 4, 5, 7, 8])
Batch: 821/1250
torch.Size([8, 100])
tensor([5, 0, 5, 7, 0, 2, 3, 6])
Batch: 822/1250
torch.Size([8, 100])
tensor([9, 1, 0, 8, 7, 5, 1, 9])
Batch: 823/1250
torch.Size([8, 100])
tensor([7, 3, 9, 7, 2, 9, 6, 6])
Batch: 824/1250
torch.Size([8, 100])
tensor([4, 3, 5, 3, 8, 3, 2, 7])
Batch: 825/1250
torch.Size([8, 100])
tensor([0, 4, 2, 0, 3, 6, 8, 1])
Batch: 826/1250
torch.Size([8, 100])
tensor([5, 3, 4, 2, 6, 2, 6, 1])
Batch: 827/1250
torch.Size([8, 100])
tensor([9, 0, 8, 8, 3, 9, 3, 1])
Batch: 828/1250
torch.Size([8, 100])
tensor([1, 2, 7, 3, 9, 1, 6, 3])
Batch: 829/1250
torch.Size([8, 100])
tensor([1, 6, 8, 8, 6, 6, 1, 8])
Batch: 830/1250
torch.Size([8, 100])
tensor([4, 5, 6, 1, 5, 7, 8, 9])
Batch: 831/1250
torch.Size([8, 100])
tensor([8, 0, 6, 1, 8, 1, 3, 1])
Batch: 832/1250
torch.Size([8, 100])
tensor([2, 2, 7, 8, 3, 1, 5, 2])
Batch: 833/1250
torch.Size([8, 100])
tensor([1, 4, 6, 0, 8, 1, 3, 7])
Batch: 834/1250
torch.Size([8, 100])
tensor([9, 5, 4, 6, 5, 6, 8, 8])
Batch: 835/1250
torch.Size([8, 100])
tensor([6, 1, 7, 1, 2, 3, 8, 6])
Batch: 836/1250
torch.Size([8, 100])
tensor([9, 9, 5, 2, 5, 3, 8, 4])
Batch: 837/1250
torch.Size([8, 100])
tensor([4, 0, 1, 3, 2, 0, 1, 6])
Batch: 838/1250
torch.Size([8, 100])
tensor([2, 6, 8, 2, 1, 6, 1, 8])
Batch: 839/1250
torch.Size([8, 100])
tensor([4, 9, 3, 9, 5, 9, 9, 0])
Batch: 840/1250
torch.Size([8, 100])
tensor([8, 1, 1, 8, 5, 4, 3, 0])
Batch: 841/1250
torch.Size([8, 100])
tensor([7, 6, 0, 0, 5, 1, 0, 9])
Batch: 842/1250
torch.Size([8, 100])
tensor([0, 6, 9, 5, 0, 4, 9, 9])
Batch: 843/1250
torch.Size([8, 100])
tensor([9, 9, 6, 2, 3, 0, 7, 7])
Batch: 844/1250
torch.Size([8, 100])
tensor([9, 0, 7, 6, 7, 8, 7, 8])
Batch: 845/1250
torch.Size([8, 100])
tensor([3, 9, 0, 0, 9, 4, 7, 5])
Batch: 846/1250
torch.Size([8, 100])
tensor([5, 1, 6, 1, 7, 7, 2, 5])
Batch: 847/1250
torch.Size([8, 100])
tensor([4, 9, 8, 4, 9, 9, 3, 1])
Batch: 848/1250
torch.Size([8, 100])
tensor([9, 8, 2, 5, 3, 2, 4, 3])
Batch: 849/1250
torch.Size([8, 100])
tensor([9, 7, 1, 1, 1, 6, 7, 9])
Batch: 850/1250
torch.Size([8, 100])
tensor([2, 6, 3, 3, 9, 7, 3, 8])
Batch: 851/1250
torch.Size([8, 100])
tensor([8, 4, 1, 9, 7, 7, 3, 6])
Batch: 852/1250
torch.Size([8, 100])
tensor([7, 7, 7, 3, 1, 5, 4, 8])
Batch: 853/1250
torch.Size([8, 100])
tensor([0, 0, 0, 5, 9, 0, 4, 2])
Batch: 854/1250
torch.Size([8, 100])
tensor([8, 8, 9, 5, 3, 4, 1, 2])
Batch: 855/1250
torch.Size([8, 100])
tensor([5, 1, 8, 1, 1, 4, 8, 1])
Batch: 856/1250
torch.Size([8, 100])
tensor([7, 6, 9, 2, 8, 1, 8, 5])
Batch: 857/1250
torch.Size([8, 100])
tensor([5, 9, 0, 2, 0, 0, 7, 2])
Batch: 858/1250
torch.Size([8, 100])
tensor([0, 0, 2, 5, 7, 1, 0, 9])
Batch: 859/1250
torch.Size([8, 100])
tensor([8, 0, 7, 3, 6, 4, 2, 6])
Batch: 860/1250
torch.Size([8, 100])
tensor([1, 6, 5, 0, 7, 3, 0, 2])
Batch: 861/1250
torch.Size([8, 100])
tensor([8, 7, 2, 4, 2, 9, 6, 7])
Batch: 862/1250
torch.Size([8, 100])
tensor([3, 6, 3, 1, 7, 3, 6, 0])
Batch: 863/1250
torch.Size([8, 100])
tensor([4, 2, 8, 1, 2, 1, 1, 2])
Batch: 864/1250
torch.Size([8, 100])
tensor([5, 4, 3, 4, 2, 1, 5, 2])
Batch: 865/1250
torch.Size([8, 100])
tensor([6, 7, 1, 0, 3, 0, 5, 1])
Batch: 866/1250
torch.Size([8, 100])
tensor([0, 3, 6, 2, 7, 0, 0, 8])
Batch: 867/1250
torch.Size([8, 100])
tensor([7, 5, 8, 5, 4, 1, 3, 4])
Batch: 868/1250
torch.Size([8, 100])
tensor([0, 6, 5, 1, 1, 5, 5, 4])
Batch: 869/1250
torch.Size([8, 100])
tensor([7, 7, 4, 4, 1, 4, 8, 3])
Batch: 870/1250
torch.Size([8, 100])
tensor([6, 3, 4, 1, 1, 4, 5, 1])
Batch: 871/1250
torch.Size([8, 100])
tensor([4, 3, 6, 0, 5, 6, 7, 6])
Batch: 872/1250
torch.Size([8, 100])
tensor([0, 2, 4, 7, 1, 3, 0, 2])
Batch: 873/1250
torch.Size([8, 100])
tensor([4, 4, 4, 7, 9, 9, 7, 6])
Batch: 874/1250
torch.Size([8, 100])
tensor([0, 1, 8, 2, 1, 3, 8, 8])
Batch: 875/1250
torch.Size([8, 100])
tensor([5, 2, 4, 8, 1, 9, 1, 0])
Batch: 876/1250
torch.Size([8, 100])
tensor([0, 1, 0, 1, 9, 3, 3, 5])
Batch: 877/1250
torch.Size([8, 100])
tensor([8, 9, 6, 1, 1, 1, 4, 9])
Batch: 878/1250
torch.Size([8, 100])
tensor([6, 8, 8, 9, 7, 4, 2, 4])
Batch: 879/1250
torch.Size([8, 100])
tensor([1, 5, 4, 2, 4, 9, 8, 6])
Batch: 880/1250
torch.Size([8, 100])
tensor([8, 1, 1, 4, 2, 4, 3, 5])
Batch: 881/1250
torch.Size([8, 100])
tensor([4, 2, 5, 2, 1, 3, 1, 5])
Batch: 882/1250
torch.Size([8, 100])
tensor([3, 6, 9, 5, 1, 6, 6, 9])
Batch: 883/1250
torch.Size([8, 100])
tensor([2, 0, 3, 6, 4, 7, 0, 6])
Batch: 884/1250
torch.Size([8, 100])
tensor([7, 8, 8, 0, 3, 5, 6, 9])
Batch: 885/1250
torch.Size([8, 100])
tensor([5, 1, 1, 3, 7, 6, 3, 6])
Batch: 886/1250
torch.Size([8, 100])
tensor([9, 3, 9, 8, 7, 8, 1, 3])
Batch: 887/1250
torch.Size([8, 100])
tensor([4, 3, 2, 2, 2, 9, 9, 8])
Batch: 888/1250
torch.Size([8, 100])
tensor([7, 3, 8, 5, 3, 6, 5, 1])
Batch: 889/1250
torch.Size([8, 100])
tensor([1, 1, 0, 0, 9, 5, 3, 0])
Batch: 890/1250
torch.Size([8, 100])
tensor([7, 4, 4, 9, 7, 0, 4, 9])
Batch: 891/1250
torch.Size([8, 100])
tensor([1, 0, 0, 5, 2, 0, 5, 3])
Batch: 892/1250
torch.Size([8, 100])
tensor([3, 6, 0, 8, 1, 9, 5, 2])
Batch: 893/1250
torch.Size([8, 100])
tensor([1, 4, 9, 9, 8, 3, 2, 3])
Batch: 894/1250
torch.Size([8, 100])
tensor([9, 6, 9, 7, 8, 3, 2, 6])
Batch: 895/1250
torch.Size([8, 100])
tensor([5, 4, 6, 9, 2, 4, 2, 9])
Batch: 896/1250
torch.Size([8, 100])
tensor([1, 3, 7, 9, 4, 6, 9, 4])
Batch: 897/1250
torch.Size([8, 100])
tensor([2, 8, 4, 4, 4, 8, 2, 0])
Batch: 898/1250
torch.Size([8, 100])
tensor([9, 0, 6, 8, 5, 8, 4, 7])
Batch: 899/1250
torch.Size([8, 100])
tensor([5, 6, 7, 5, 8, 4, 8, 4])
Batch: 900/1250
torch.Size([8, 100])
tensor([2, 7, 0, 1, 8, 8, 1, 4])
Batch: 901/1250
torch.Size([8, 100])
tensor([1, 2, 5, 6, 9, 9, 3, 3])
Batch: 902/1250
torch.Size([8, 100])
tensor([5, 8, 7, 2, 2, 4, 2, 8])
Batch: 903/1250
torch.Size([8, 100])
tensor([0, 9, 6, 0, 0, 2, 8, 7])
Batch: 904/1250
torch.Size([8, 100])
tensor([7, 4, 6, 0, 1, 2, 6, 2])
Batch: 905/1250
torch.Size([8, 100])
tensor([8, 8, 2, 9, 4, 4, 9, 5])
Batch: 906/1250
torch.Size([8, 100])
tensor([3, 6, 7, 3, 1, 4, 0, 2])
Batch: 907/1250
torch.Size([8, 100])
tensor([9, 9, 7, 3, 8, 1, 9, 6])
Batch: 908/1250
torch.Size([8, 100])
tensor([1, 3, 7, 3, 2, 9, 1, 3])
Batch: 909/1250
torch.Size([8, 100])
tensor([8, 9, 2, 6, 7, 7, 0, 5])
Batch: 910/1250
torch.Size([8, 100])
tensor([5, 8, 3, 9, 8, 7, 5, 4])
Batch: 911/1250
torch.Size([8, 100])
tensor([9, 6, 1, 2, 5, 5, 6, 9])
Batch: 912/1250
torch.Size([8, 100])
tensor([7, 3, 5, 8, 5, 5, 2, 9])
Batch: 913/1250
torch.Size([8, 100])
tensor([1, 9, 4, 2, 4, 5, 2, 5])
Batch: 914/1250
torch.Size([8, 100])
tensor([7, 6, 3, 2, 1, 3, 5, 9])
Batch: 915/1250
torch.Size([8, 100])
tensor([7, 4, 0, 4, 2, 0, 1, 7])
Batch: 916/1250
torch.Size([8, 100])
tensor([5, 0, 7, 2, 1, 7, 4, 0])
Batch: 917/1250
torch.Size([8, 100])
tensor([7, 0, 1, 1, 9, 2, 7, 1])
Batch: 918/1250
torch.Size([8, 100])
tensor([9, 4, 8, 8, 7, 4, 3, 7])
Batch: 919/1250
torch.Size([8, 100])
tensor([5, 8, 2, 7, 2, 0, 7, 5])
Batch: 920/1250
torch.Size([8, 100])
tensor([2, 5, 1, 8, 5, 3, 3, 2])
Batch: 921/1250
torch.Size([8, 100])
tensor([4, 8, 7, 3, 0, 1, 7, 4])
Batch: 922/1250
torch.Size([8, 100])
tensor([4, 2, 4, 4, 1, 8, 4, 5])
Batch: 923/1250
torch.Size([8, 100])
tensor([0, 5, 4, 2, 3, 1, 1, 9])
Batch: 924/1250
torch.Size([8, 100])
tensor([7, 3, 2, 7, 2, 8, 4, 9])
Batch: 925/1250
torch.Size([8, 100])
tensor([8, 4, 9, 1, 5, 1, 9, 9])
Batch: 926/1250
torch.Size([8, 100])
tensor([9, 3, 3, 7, 5, 0, 4, 0])
Batch: 927/1250
torch.Size([8, 100])
tensor([3, 8, 2, 0, 9, 1, 4, 1])
Batch: 928/1250
torch.Size([8, 100])
tensor([7, 3, 2, 3, 0, 5, 2, 0])
Batch: 929/1250
torch.Size([8, 100])
tensor([0, 9, 1, 6, 0, 6, 8, 8])
Batch: 930/1250
torch.Size([8, 100])
tensor([0, 8, 4, 1, 8, 7, 1, 1])
Batch: 931/1250
torch.Size([8, 100])
tensor([8, 5, 8, 2, 1, 8, 5, 9])
Batch: 932/1250
torch.Size([8, 100])
tensor([3, 5, 2, 8, 1, 1, 2, 9])
Batch: 933/1250
torch.Size([8, 100])
tensor([1, 0, 5, 9, 4, 3, 0, 8])
Batch: 934/1250
torch.Size([8, 100])
tensor([6, 4, 9, 9, 9, 6, 7, 1])
Batch: 935/1250
torch.Size([8, 100])
tensor([4, 1, 2, 4, 8, 2, 7, 1])
Batch: 936/1250
torch.Size([8, 100])
tensor([4, 7, 8, 0, 7, 1, 1, 5])
Batch: 937/1250
torch.Size([8, 100])
tensor([7, 6, 9, 2, 9, 8, 9, 8])
Batch: 938/1250
torch.Size([8, 100])
tensor([7, 9, 8, 0, 9, 5, 7, 7])
Batch: 939/1250
torch.Size([8, 100])
tensor([2, 5, 5, 0, 5, 5, 6, 7])
Batch: 940/1250
torch.Size([8, 100])
tensor([1, 6, 9, 0, 9, 6, 7, 7])
Batch: 941/1250
torch.Size([8, 100])
tensor([2, 2, 4, 6, 5, 8, 1, 6])
Batch: 942/1250
torch.Size([8, 100])
tensor([3, 9, 7, 5, 6, 3, 8, 6])
Batch: 943/1250
torch.Size([8, 100])
tensor([1, 8, 7, 9, 1, 9, 0, 3])
Batch: 944/1250
torch.Size([8, 100])
tensor([5, 8, 6, 3, 7, 3, 0, 1])
Batch: 945/1250
torch.Size([8, 100])
tensor([8, 3, 8, 6, 8, 9, 8, 7])
Batch: 946/1250
torch.Size([8, 100])
tensor([1, 3, 2, 6, 7, 7, 5, 1])
Batch: 947/1250
torch.Size([8, 100])
tensor([8, 8, 9, 7, 7, 3, 4, 6])
Batch: 948/1250
torch.Size([8, 100])
tensor([8, 8, 7, 6, 7, 8, 0, 1])
Batch: 949/1250
torch.Size([8, 100])
tensor([1, 4, 5, 7, 5, 4, 7, 0])
Batch: 950/1250
torch.Size([8, 100])
tensor([1, 1, 5, 3, 6, 9, 9, 5])
Batch: 951/1250
torch.Size([8, 100])
tensor([8, 4, 6, 0, 3, 0, 1, 7])
Batch: 952/1250
torch.Size([8, 100])
tensor([8, 5, 1, 5, 5, 5, 2, 3])
Batch: 953/1250
torch.Size([8, 100])
tensor([3, 4, 0, 9, 2, 0, 6, 0])
Batch: 954/1250
torch.Size([8, 100])
tensor([0, 2, 5, 9, 9, 6, 2, 4])
Batch: 955/1250
torch.Size([8, 100])
tensor([4, 0, 5, 3, 6, 3, 9, 6])
Batch: 956/1250
torch.Size([8, 100])
tensor([3, 2, 7, 9, 9, 7, 6, 7])
Batch: 957/1250
torch.Size([8, 100])
tensor([3, 1, 9, 3, 2, 7, 2, 2])
Batch: 958/1250
torch.Size([8, 100])
tensor([8, 0, 3, 6, 2, 8, 7, 7])
Batch: 959/1250
torch.Size([8, 100])
tensor([6, 7, 3, 8, 3, 7, 0, 7])
Batch: 960/1250
torch.Size([8, 100])
tensor([9, 1, 7, 5, 6, 3, 1, 3])
Batch: 961/1250
torch.Size([8, 100])
tensor([5, 5, 1, 0, 1, 2, 7, 4])
Batch: 962/1250
torch.Size([8, 100])
tensor([8, 9, 8, 0, 5, 4, 3, 0])
Batch: 963/1250
torch.Size([8, 100])
tensor([1, 9, 1, 7, 3, 1, 4, 2])
Batch: 964/1250
torch.Size([8, 100])
tensor([6, 6, 2, 8, 5, 2, 9, 7])
Batch: 965/1250
torch.Size([8, 100])
tensor([1, 9, 8, 0, 7, 6, 8, 5])
Batch: 966/1250
torch.Size([8, 100])
tensor([2, 0, 5, 9, 7, 1, 1, 1])
Batch: 967/1250
torch.Size([8, 100])
tensor([8, 3, 3, 3, 2, 6, 9, 2])
Batch: 968/1250
torch.Size([8, 100])
tensor([9, 7, 8, 2, 7, 5, 1, 4])
Batch: 969/1250
torch.Size([8, 100])
tensor([1, 1, 3, 1, 4, 0, 5, 4])
Batch: 970/1250
torch.Size([8, 100])
tensor([0, 6, 3, 1, 7, 1, 1, 0])
Batch: 971/1250
torch.Size([8, 100])
tensor([1, 8, 9, 9, 4, 2, 9, 5])
Batch: 972/1250
torch.Size([8, 100])
tensor([5, 5, 1, 6, 0, 2, 4, 4])
Batch: 973/1250
torch.Size([8, 100])
tensor([0, 6, 0, 8, 9, 2, 7, 1])
Batch: 974/1250
torch.Size([8, 100])
tensor([7, 4, 9, 9, 3, 3, 0, 4])
Batch: 975/1250
torch.Size([8, 100])
tensor([5, 9, 0, 3, 0, 7, 8, 0])
Batch: 976/1250
torch.Size([8, 100])
tensor([7, 0, 0, 7, 1, 3, 2, 2])
Batch: 977/1250
torch.Size([8, 100])
tensor([9, 1, 4, 4, 6, 7, 4, 2])
Batch: 978/1250
torch.Size([8, 100])
tensor([5, 6, 1, 1, 9, 5, 0, 2])
Batch: 979/1250
torch.Size([8, 100])
tensor([3, 8, 3, 8, 3, 4, 9, 6])
Batch: 980/1250
torch.Size([8, 100])
tensor([5, 2, 5, 4, 8, 5, 9, 4])
Batch: 981/1250
torch.Size([8, 100])
tensor([5, 9, 0, 5, 3, 4, 6, 1])
Batch: 982/1250
torch.Size([8, 100])
tensor([8, 3, 8, 0, 7, 2, 4, 1])
Batch: 983/1250
torch.Size([8, 100])
tensor([4, 5, 9, 1, 4, 0, 8, 6])
Batch: 984/1250
torch.Size([8, 100])
tensor([9, 2, 8, 1, 3, 6, 2, 2])
Batch: 985/1250
torch.Size([8, 100])
tensor([6, 3, 6, 2, 7, 8, 4, 3])
Batch: 986/1250
torch.Size([8, 100])
tensor([9, 1, 8, 0, 9, 3, 7, 7])
Batch: 987/1250
torch.Size([8, 100])
tensor([8, 1, 5, 3, 6, 7, 0, 2])
Batch: 988/1250
torch.Size([8, 100])
tensor([6, 6, 6, 6, 4, 5, 9, 3])
Batch: 989/1250
torch.Size([8, 100])
tensor([7, 4, 1, 6, 1, 5, 4, 2])
Batch: 990/1250
torch.Size([8, 100])
tensor([8, 7, 3, 3, 8, 4, 3, 1])
Batch: 991/1250
torch.Size([8, 100])
tensor([3, 9, 4, 0, 2, 1, 8, 8])
Batch: 992/1250
torch.Size([8, 100])
tensor([3, 8, 8, 5, 2, 7, 9, 9])
Batch: 993/1250
torch.Size([8, 100])
tensor([7, 3, 2, 3, 7, 5, 7, 2])
Batch: 994/1250
torch.Size([8, 100])
tensor([5, 7, 6, 9, 1, 0, 6, 1])
Batch: 995/1250
torch.Size([8, 100])
tensor([0, 7, 4, 4, 3, 6, 0, 7])
Batch: 996/1250
torch.Size([8, 100])
tensor([2, 9, 1, 9, 5, 3, 8, 3])
Batch: 997/1250
torch.Size([8, 100])
tensor([1, 0, 2, 6, 7, 5, 6, 4])
Batch: 998/1250
torch.Size([8, 100])
tensor([1, 4, 0, 7, 8, 2, 7, 8])
Batch: 999/1250
torch.Size([8, 100])
tensor([7, 6, 9, 3, 0, 7, 7, 3])
Batch: 1000/1250
torch.Size([8, 100])
tensor([6, 7, 1, 3, 1, 0, 1, 1])
Batch: 1001/1250
torch.Size([8, 100])
tensor([6, 4, 5, 9, 4, 7, 2, 9])
Batch: 1002/1250
torch.Size([8, 100])
tensor([6, 3, 8, 6, 9, 9, 4, 1])
Batch: 1003/1250
torch.Size([8, 100])
tensor([3, 1, 4, 7, 8, 6, 6, 6])
Batch: 1004/1250
torch.Size([8, 100])
tensor([7, 0, 6, 7, 8, 0, 4, 1])
Batch: 1005/1250
torch.Size([8, 100])
tensor([7, 6, 0, 7, 3, 2, 4, 3])
Batch: 1006/1250
torch.Size([8, 100])
tensor([3, 6, 1, 0, 6, 1, 8, 3])
Batch: 1007/1250
torch.Size([8, 100])
tensor([4, 9, 3, 2, 5, 7, 7, 5])
Batch: 1008/1250
torch.Size([8, 100])
tensor([6, 8, 8, 0, 7, 9, 0, 2])
Batch: 1009/1250
torch.Size([8, 100])
tensor([1, 7, 6, 6, 1, 3, 1, 3])
Batch: 1010/1250
torch.Size([8, 100])
tensor([7, 0, 0, 6, 9, 7, 4, 6])
Batch: 1011/1250
torch.Size([8, 100])
tensor([4, 4, 7, 9, 6, 1, 6, 2])
Batch: 1012/1250
torch.Size([8, 100])
tensor([7, 4, 5, 1, 7, 0, 7, 4])
Batch: 1013/1250
torch.Size([8, 100])
tensor([6, 4, 6, 8, 2, 2, 8, 0])
Batch: 1014/1250
torch.Size([8, 100])
tensor([3, 2, 3, 2, 2, 3, 0, 3])
Batch: 1015/1250
torch.Size([8, 100])
tensor([5, 0, 5, 8, 0, 5, 3, 9])
Batch: 1016/1250
torch.Size([8, 100])
tensor([3, 7, 2, 8, 3, 7, 8, 8])
Batch: 1017/1250
torch.Size([8, 100])
tensor([1, 4, 2, 4, 9, 1, 8, 4])
Batch: 1018/1250
torch.Size([8, 100])
tensor([2, 0, 4, 2, 6, 1, 8, 9])
Batch: 1019/1250
torch.Size([8, 100])
tensor([9, 9, 9, 8, 7, 8, 8, 6])
Batch: 1020/1250
torch.Size([8, 100])
tensor([1, 5, 1, 0, 4, 2, 5, 5])
Batch: 1021/1250
torch.Size([8, 100])
tensor([1, 4, 7, 5, 5, 5, 5, 7])
Batch: 1022/1250
torch.Size([8, 100])
tensor([2, 7, 2, 7, 5, 9, 5, 7])
Batch: 1023/1250
torch.Size([8, 100])
tensor([3, 5, 8, 8, 2, 1, 3, 6])
Batch: 1024/1250
torch.Size([8, 100])
tensor([0, 2, 8, 5, 7, 3, 0, 8])
Batch: 1025/1250
torch.Size([8, 100])
tensor([7, 0, 9, 8, 5, 9, 7, 7])
Batch: 1026/1250
torch.Size([8, 100])
tensor([7, 6, 3, 9, 4, 2, 1, 9])
Batch: 1027/1250
torch.Size([8, 100])
tensor([4, 6, 0, 9, 1, 1, 5, 9])
Batch: 1028/1250
torch.Size([8, 100])
tensor([0, 0, 1, 4, 1, 6, 0, 7])
Batch: 1029/1250
torch.Size([8, 100])
tensor([1, 3, 7, 7, 0, 8, 8, 1])
Batch: 1030/1250
torch.Size([8, 100])
tensor([1, 7, 1, 6, 3, 6, 4, 6])
Batch: 1031/1250
torch.Size([8, 100])
tensor([1, 7, 0, 8, 7, 7, 3, 1])
Batch: 1032/1250
torch.Size([8, 100])
tensor([5, 5, 4, 0, 2, 6, 9, 8])
Batch: 1033/1250
torch.Size([8, 100])
tensor([2, 6, 7, 8, 1, 2, 2, 5])
Batch: 1034/1250
torch.Size([8, 100])
tensor([4, 8, 5, 0, 2, 0, 9, 6])
Batch: 1035/1250
torch.Size([8, 100])
tensor([0, 0, 0, 4, 3, 4, 7, 2])
Batch: 1036/1250
torch.Size([8, 100])
tensor([3, 0, 6, 4, 2, 8, 5, 0])
Batch: 1037/1250
torch.Size([8, 100])
tensor([9, 6, 0, 5, 5, 3, 0, 9])
Batch: 1038/1250
torch.Size([8, 100])
tensor([8, 0, 2, 3, 2, 3, 2, 9])
Batch: 1039/1250
torch.Size([8, 100])
tensor([7, 5, 7, 1, 0, 8, 1, 5])
Batch: 1040/1250
torch.Size([8, 100])
tensor([2, 9, 7, 8, 1, 4, 2, 9])
Batch: 1041/1250
torch.Size([8, 100])
tensor([0, 5, 6, 9, 9, 4, 4, 3])
Batch: 1042/1250
torch.Size([8, 100])
tensor([2, 8, 3, 8, 2, 6, 7, 7])
Batch: 1043/1250
torch.Size([8, 100])
tensor([0, 4, 9, 4, 2, 4, 9, 5])
Batch: 1044/1250
torch.Size([8, 100])
tensor([2, 0, 1, 2, 1, 2, 6, 7])
Batch: 1045/1250
torch.Size([8, 100])
tensor([0, 2, 4, 0, 8, 6, 8, 2])
Batch: 1046/1250
torch.Size([8, 100])
tensor([6, 1, 9, 7, 5, 7, 3, 3])
Batch: 1047/1250
torch.Size([8, 100])
tensor([9, 7, 1, 1, 0, 9, 0, 7])
Batch: 1048/1250
torch.Size([8, 100])
tensor([4, 8, 5, 3, 1, 4, 1, 7])
Batch: 1049/1250
torch.Size([8, 100])
tensor([0, 8, 0, 4, 6, 2, 5, 7])
Batch: 1050/1250
torch.Size([8, 100])
tensor([8, 0, 2, 0, 7, 2, 1, 3])
Batch: 1051/1250
torch.Size([8, 100])
tensor([3, 7, 7, 6, 0, 4, 3, 9])
Batch: 1052/1250
torch.Size([8, 100])
tensor([9, 6, 8, 4, 4, 1, 5, 4])
Batch: 1053/1250
torch.Size([8, 100])
tensor([2, 7, 2, 4, 6, 1, 9, 4])
Batch: 1054/1250
torch.Size([8, 100])
tensor([0, 6, 2, 5, 7, 0, 2, 0])
Batch: 1055/1250
torch.Size([8, 100])
tensor([4, 7, 0, 9, 1, 6, 3, 0])
Batch: 1056/1250
torch.Size([8, 100])
tensor([0, 5, 7, 1, 2, 2, 6, 6])
Batch: 1057/1250
torch.Size([8, 100])
tensor([2, 8, 7, 1, 1, 7, 1, 5])
Batch: 1058/1250
torch.Size([8, 100])
tensor([2, 8, 9, 5, 7, 6, 7, 7])
Batch: 1059/1250
torch.Size([8, 100])
tensor([4, 2, 7, 9, 9, 4, 6, 2])
Batch: 1060/1250
torch.Size([8, 100])
tensor([7, 1, 0, 1, 3, 7, 2, 3])
Batch: 1061/1250
torch.Size([8, 100])
tensor([5, 6, 9, 5, 6, 1, 3, 7])
Batch: 1062/1250
torch.Size([8, 100])
tensor([9, 4, 9, 2, 1, 2, 7, 3])
Batch: 1063/1250
torch.Size([8, 100])
tensor([1, 8, 9, 3, 9, 7, 2, 8])
Batch: 1064/1250
torch.Size([8, 100])
tensor([9, 4, 5, 1, 4, 7, 9, 9])
Batch: 1065/1250
torch.Size([8, 100])
tensor([4, 4, 2, 0, 3, 5, 2, 7])
Batch: 1066/1250
torch.Size([8, 100])
tensor([2, 1, 9, 8, 2, 6, 1, 8])
Batch: 1067/1250
torch.Size([8, 100])
tensor([5, 0, 2, 2, 0, 5, 1, 3])
Batch: 1068/1250
torch.Size([8, 100])
tensor([6, 6, 2, 0, 7, 7, 1, 2])
Batch: 1069/1250
torch.Size([8, 100])
tensor([5, 2, 6, 9, 3, 3, 3, 3])
Batch: 1070/1250
torch.Size([8, 100])
tensor([4, 7, 9, 6, 9, 1, 7, 9])
Batch: 1071/1250
torch.Size([8, 100])
tensor([1, 9, 7, 5, 4, 7, 1, 3])
Batch: 1072/1250
torch.Size([8, 100])
tensor([0, 3, 0, 7, 9, 2, 6, 6])
Batch: 1073/1250
torch.Size([8, 100])
tensor([4, 7, 8, 8, 7, 7, 1, 7])
Batch: 1074/1250
torch.Size([8, 100])
tensor([7, 6, 8, 8, 7, 9, 3, 6])
Batch: 1075/1250
torch.Size([8, 100])
tensor([2, 0, 9, 5, 2, 4, 3, 2])
Batch: 1076/1250
torch.Size([8, 100])
tensor([3, 1, 7, 0, 1, 1, 5, 1])
Batch: 1077/1250
torch.Size([8, 100])
tensor([9, 6, 1, 0, 6, 2, 3, 0])
Batch: 1078/1250
torch.Size([8, 100])
tensor([8, 4, 4, 5, 4, 4, 4, 2])
Batch: 1079/1250
torch.Size([8, 100])
tensor([7, 6, 6, 9, 6, 9, 3, 1])
Batch: 1080/1250
torch.Size([8, 100])
tensor([5, 6, 2, 4, 6, 2, 5, 4])
Batch: 1081/1250
torch.Size([8, 100])
tensor([7, 8, 7, 2, 1, 6, 7, 5])
Batch: 1082/1250
torch.Size([8, 100])
tensor([9, 5, 8, 7, 5, 5, 8, 0])
Batch: 1083/1250
torch.Size([8, 100])
tensor([2, 1, 1, 2, 9, 6, 6, 7])
Batch: 1084/1250
torch.Size([8, 100])
tensor([3, 6, 2, 2, 2, 2, 9, 1])
Batch: 1085/1250
torch.Size([8, 100])
tensor([4, 0, 6, 3, 3, 7, 3, 3])
Batch: 1086/1250
torch.Size([8, 100])
tensor([6, 8, 4, 2, 0, 6, 8, 6])
Batch: 1087/1250
torch.Size([8, 100])
tensor([9, 6, 1, 2, 7, 1, 3, 9])
Batch: 1088/1250
torch.Size([8, 100])
tensor([5, 3, 9, 1, 8, 9, 2, 7])
Batch: 1089/1250
torch.Size([8, 100])
tensor([8, 4, 5, 7, 7, 9, 3, 0])
Batch: 1090/1250
torch.Size([8, 100])
tensor([9, 8, 0, 7, 4, 3, 0, 0])
Batch: 1091/1250
torch.Size([8, 100])
tensor([8, 0, 4, 5, 5, 6, 8, 3])
Batch: 1092/1250
torch.Size([8, 100])
tensor([8, 0, 4, 5, 9, 1, 0, 0])
Batch: 1093/1250
torch.Size([8, 100])
tensor([3, 1, 8, 6, 0, 7, 9, 9])
Batch: 1094/1250
torch.Size([8, 100])
tensor([2, 6, 4, 3, 5, 5, 3, 7])
Batch: 1095/1250
torch.Size([8, 100])
tensor([3, 2, 2, 2, 2, 6, 5, 9])
Batch: 1096/1250
torch.Size([8, 100])
tensor([7, 9, 1, 5, 5, 9, 7, 0])
Batch: 1097/1250
torch.Size([8, 100])
tensor([9, 2, 9, 2, 3, 0, 6, 3])
Batch: 1098/1250
torch.Size([8, 100])
tensor([1, 9, 9, 0, 5, 6, 4, 9])
Batch: 1099/1250
torch.Size([8, 100])
tensor([7, 5, 7, 4, 9, 7, 2, 5])
Batch: 1100/1250
torch.Size([8, 100])
tensor([7, 1, 9, 7, 7, 3, 9, 6])
Batch: 1101/1250
torch.Size([8, 100])
tensor([0, 0, 1, 3, 1, 7, 2, 0])
Batch: 1102/1250
torch.Size([8, 100])
tensor([9, 7, 7, 9, 7, 6, 2, 6])
Batch: 1103/1250
torch.Size([8, 100])
tensor([9, 5, 3, 7, 7, 5, 5, 8])
Batch: 1104/1250
torch.Size([8, 100])
tensor([2, 1, 1, 1, 4, 6, 7, 6])
Batch: 1105/1250
torch.Size([8, 100])
tensor([7, 4, 4, 0, 3, 8, 9, 0])
Batch: 1106/1250
torch.Size([8, 100])
tensor([8, 9, 5, 9, 3, 3, 7, 0])
Batch: 1107/1250
torch.Size([8, 100])
tensor([9, 5, 3, 8, 7, 6, 0, 3])
Batch: 1108/1250
torch.Size([8, 100])
tensor([7, 0, 6, 5, 6, 8, 7, 8])
Batch: 1109/1250
torch.Size([8, 100])
tensor([6, 8, 3, 8, 9, 4, 1, 2])
Batch: 1110/1250
torch.Size([8, 100])
tensor([1, 9, 4, 5, 1, 5, 4, 5])
Batch: 1111/1250
torch.Size([8, 100])
tensor([1, 2, 7, 5, 9, 6, 1, 7])
Batch: 1112/1250
torch.Size([8, 100])
tensor([4, 2, 1, 8, 4, 2, 4, 0])
Batch: 1113/1250
torch.Size([8, 100])
tensor([4, 0, 0, 3, 9, 6, 2, 4])
Batch: 1114/1250
torch.Size([8, 100])
tensor([1, 4, 0, 8, 7, 4, 1, 6])
Batch: 1115/1250
torch.Size([8, 100])
tensor([2, 8, 7, 8, 2, 9, 3, 4])
Batch: 1116/1250
torch.Size([8, 100])
tensor([6, 6, 5, 7, 5, 7, 6, 4])
Batch: 1117/1250
torch.Size([8, 100])
tensor([9, 5, 7, 8, 9, 2, 3, 1])
Batch: 1118/1250
torch.Size([8, 100])
tensor([2, 1, 5, 8, 3, 4, 9, 1])
Batch: 1119/1250
torch.Size([8, 100])
tensor([2, 2, 6, 8, 8, 2, 1, 5])
Batch: 1120/1250
torch.Size([8, 100])
tensor([9, 2, 4, 4, 0, 3, 1, 1])
Batch: 1121/1250
torch.Size([8, 100])
tensor([5, 6, 2, 6, 2, 0, 7, 2])
Batch: 1122/1250
torch.Size([8, 100])
tensor([3, 2, 7, 7, 7, 5, 3, 8])
Batch: 1123/1250
torch.Size([8, 100])
tensor([9, 8, 8, 6, 2, 2, 8, 2])
Batch: 1124/1250
torch.Size([8, 100])
tensor([5, 7, 7, 5, 4, 3, 6, 8])
Batch: 1125/1250
torch.Size([8, 100])
tensor([9, 2, 9, 6, 7, 4, 4, 3])
Batch: 1126/1250
torch.Size([8, 100])
tensor([2, 5, 8, 6, 2, 5, 7, 0])
Batch: 1127/1250
torch.Size([8, 100])
tensor([2, 4, 3, 8, 0, 1, 8, 8])
Batch: 1128/1250
torch.Size([8, 100])
tensor([3, 9, 3, 7, 7, 9, 9, 0])
Batch: 1129/1250
torch.Size([8, 100])
tensor([8, 5, 1, 1, 1, 3, 7, 9])
Batch: 1130/1250
torch.Size([8, 100])
tensor([9, 3, 7, 6, 0, 9, 6, 2])
Batch: 1131/1250
torch.Size([8, 100])
tensor([2, 5, 2, 7, 0, 1, 1, 4])
Batch: 1132/1250
torch.Size([8, 100])
tensor([5, 4, 6, 5, 6, 0, 0, 1])
Batch: 1133/1250
torch.Size([8, 100])
tensor([7, 6, 9, 3, 4, 9, 8, 9])
Batch: 1134/1250
torch.Size([8, 100])
tensor([1, 7, 6, 5, 0, 6, 3, 1])
Batch: 1135/1250
torch.Size([8, 100])
tensor([2, 4, 6, 0, 9, 9, 9, 0])
Batch: 1136/1250
torch.Size([8, 100])
tensor([5, 9, 7, 9, 1, 7, 9, 1])
Batch: 1137/1250
torch.Size([8, 100])
tensor([5, 3, 1, 7, 8, 8, 0, 9])
Batch: 1138/1250
torch.Size([8, 100])
tensor([8, 2, 1, 6, 2, 0, 5, 7])
Batch: 1139/1250
torch.Size([8, 100])
tensor([2, 6, 5, 8, 0, 0, 4, 0])
Batch: 1140/1250
torch.Size([8, 100])
tensor([0, 6, 4, 3, 0, 3, 6, 1])
Batch: 1141/1250
torch.Size([8, 100])
tensor([2, 1, 5, 7, 9, 4, 9, 9])
Batch: 1142/1250
torch.Size([8, 100])
tensor([2, 0, 8, 2, 4, 4, 1, 3])
Batch: 1143/1250
torch.Size([8, 100])
tensor([7, 3, 1, 9, 7, 3, 4, 7])
Batch: 1144/1250
torch.Size([8, 100])
tensor([1, 6, 3, 2, 9, 2, 1, 6])
Batch: 1145/1250
torch.Size([8, 100])
tensor([2, 7, 0, 6, 3, 7, 6, 1])
Batch: 1146/1250
torch.Size([8, 100])
tensor([8, 3, 0, 6, 2, 5, 8, 6])
Batch: 1147/1250
torch.Size([8, 100])
tensor([9, 5, 7, 0, 4, 3, 5, 7])
Batch: 1148/1250
torch.Size([8, 100])
tensor([5, 5, 1, 2, 4, 7, 6, 7])
Batch: 1149/1250
torch.Size([8, 100])
tensor([4, 2, 4, 3, 4, 6, 0, 1])
Batch: 1150/1250
torch.Size([8, 100])
tensor([6, 0, 1, 5, 8, 5, 1, 4])
Batch: 1151/1250
torch.Size([8, 100])
tensor([6, 2, 1, 4, 4, 1, 5, 8])
Batch: 1152/1250
torch.Size([8, 100])
tensor([0, 1, 9, 3, 0, 8, 8, 1])
Batch: 1153/1250
torch.Size([8, 100])
tensor([2, 5, 2, 1, 5, 6, 0, 7])
Batch: 1154/1250
torch.Size([8, 100])
tensor([3, 6, 8, 8, 4, 3, 0, 1])
Batch: 1155/1250
torch.Size([8, 100])
tensor([0, 8, 9, 9, 9, 9, 1, 5])
Batch: 1156/1250
torch.Size([8, 100])
tensor([5, 7, 3, 3, 6, 1, 3, 2])
Batch: 1157/1250
torch.Size([8, 100])
tensor([5, 5, 8, 2, 7, 6, 2, 5])
Batch: 1158/1250
torch.Size([8, 100])
tensor([8, 6, 6, 0, 4, 4, 2, 0])
Batch: 1159/1250
torch.Size([8, 100])
tensor([8, 1, 1, 8, 3, 7, 5, 9])
Batch: 1160/1250
torch.Size([8, 100])
tensor([6, 3, 7, 4, 1, 0, 7, 6])
Batch: 1161/1250
torch.Size([8, 100])
tensor([6, 3, 0, 5, 2, 5, 5, 5])
Batch: 1162/1250
torch.Size([8, 100])
tensor([0, 2, 5, 4, 2, 9, 6, 3])
Batch: 1163/1250
torch.Size([8, 100])
tensor([4, 1, 8, 3, 9, 1, 0, 4])
Batch: 1164/1250
torch.Size([8, 100])
tensor([0, 3, 7, 4, 6, 6, 5, 5])
Batch: 1165/1250
torch.Size([8, 100])
tensor([9, 2, 0, 4, 1, 8, 3, 8])
Batch: 1166/1250
torch.Size([8, 100])
tensor([9, 0, 9, 1, 2, 4, 0, 1])
Batch: 1167/1250
torch.Size([8, 100])
tensor([8, 5, 4, 9, 5, 4, 6, 8])
Batch: 1168/1250
torch.Size([8, 100])
tensor([2, 6, 7, 8, 5, 4, 0, 4])
Batch: 1169/1250
torch.Size([8, 100])
tensor([4, 3, 8, 2, 6, 1, 9, 7])
Batch: 1170/1250
torch.Size([8, 100])
tensor([8, 9, 6, 4, 7, 4, 6, 7])
Batch: 1171/1250
torch.Size([8, 100])
tensor([2, 0, 2, 9, 2, 0, 9, 2])
Batch: 1172/1250
torch.Size([8, 100])
tensor([1, 1, 2, 7, 0, 3, 3, 5])
Batch: 1173/1250
torch.Size([8, 100])
tensor([0, 6, 5, 6, 9, 5, 5, 7])
Batch: 1174/1250
torch.Size([8, 100])
tensor([4, 3, 3, 7, 0, 2, 0, 2])
Batch: 1175/1250
torch.Size([8, 100])
tensor([1, 4, 8, 6, 7, 6, 9, 2])
Batch: 1176/1250
torch.Size([8, 100])
tensor([7, 5, 3, 4, 3, 0, 7, 0])
Batch: 1177/1250
torch.Size([8, 100])
tensor([0, 8, 8, 9, 9, 3, 0, 6])
Batch: 1178/1250
torch.Size([8, 100])
tensor([9, 7, 7, 4, 3, 6, 6, 9])
Batch: 1179/1250
torch.Size([8, 100])
tensor([7, 8, 3, 4, 1, 1, 4, 2])
Batch: 1180/1250
torch.Size([8, 100])
tensor([8, 3, 7, 0, 0, 9, 9, 7])
Batch: 1181/1250
torch.Size([8, 100])
tensor([3, 4, 9, 8, 9, 6, 8, 2])
Batch: 1182/1250
torch.Size([8, 100])
tensor([1, 4, 6, 1, 2, 6, 6, 9])
Batch: 1183/1250
torch.Size([8, 100])
tensor([6, 4, 0, 6, 9, 2, 2, 8])
Batch: 1184/1250
torch.Size([8, 100])
tensor([8, 6, 4, 4, 1, 1, 9, 0])
Batch: 1185/1250
torch.Size([8, 100])
tensor([4, 1, 3, 1, 3, 2, 5, 2])
Batch: 1186/1250
torch.Size([8, 100])
tensor([1, 0, 8, 9, 0, 9, 7, 2])
Batch: 1187/1250
torch.Size([8, 100])
tensor([4, 5, 0, 3, 3, 0, 9, 2])
Batch: 1188/1250
torch.Size([8, 100])
tensor([5, 9, 6, 3, 0, 9, 5, 1])
Batch: 1189/1250
torch.Size([8, 100])
tensor([9, 1, 6, 7, 0, 6, 3, 7])
Batch: 1190/1250
torch.Size([8, 100])
tensor([6, 2, 7, 8, 3, 3, 5, 4])
Batch: 1191/1250
torch.Size([8, 100])
tensor([0, 4, 0, 8, 1, 4, 6, 1])
Batch: 1192/1250
torch.Size([8, 100])
tensor([9, 2, 8, 3, 4, 5, 0, 9])
Batch: 1193/1250
torch.Size([8, 100])
tensor([7, 6, 7, 0, 6, 2, 0, 9])
Batch: 1194/1250
torch.Size([8, 100])
tensor([4, 5, 6, 9, 9, 7, 0, 7])
Batch: 1195/1250
torch.Size([8, 100])
tensor([4, 2, 8, 1, 2, 7, 8, 0])
Batch: 1196/1250
torch.Size([8, 100])
tensor([2, 1, 3, 6, 1, 5, 9, 8])
Batch: 1197/1250
torch.Size([8, 100])
tensor([0, 3, 7, 9, 6, 9, 7, 8])
Batch: 1198/1250
torch.Size([8, 100])
tensor([5, 1, 8, 9, 2, 2, 4, 0])
Batch: 1199/1250
torch.Size([8, 100])
tensor([0, 2, 6, 1, 4, 8, 4, 0])
Batch: 1200/1250
torch.Size([8, 100])
tensor([3, 2, 3, 2, 4, 8, 5, 1])
Batch: 1201/1250
torch.Size([8, 100])
tensor([9, 2, 5, 1, 1, 8, 3, 1])
Batch: 1202/1250
torch.Size([8, 100])
tensor([1, 2, 8, 2, 5, 2, 1, 7])
Batch: 1203/1250
torch.Size([8, 100])
tensor([8, 7, 8, 5, 5, 8, 0, 2])
Batch: 1204/1250
torch.Size([8, 100])
tensor([1, 7, 4, 1, 5, 5, 5, 9])
Batch: 1205/1250
torch.Size([8, 100])
tensor([4, 4, 4, 0, 3, 7, 5, 6])
Batch: 1206/1250
torch.Size([8, 100])
tensor([6, 7, 7, 4, 4, 5, 2, 1])
Batch: 1207/1250
torch.Size([8, 100])
tensor([4, 2, 6, 1, 9, 6, 4, 2])
Batch: 1208/1250
torch.Size([8, 100])
tensor([5, 1, 0, 3, 2, 5, 6, 1])
Batch: 1209/1250
torch.Size([8, 100])
tensor([2, 4, 2, 0, 1, 9, 9, 3])
Batch: 1210/1250
torch.Size([8, 100])
tensor([6, 8, 9, 0, 0, 1, 9, 8])
Batch: 1211/1250
torch.Size([8, 100])
tensor([3, 5, 5, 3, 9, 0, 7, 6])
Batch: 1212/1250
torch.Size([8, 100])
tensor([1, 2, 0, 5, 7, 9, 0, 7])
Batch: 1213/1250
torch.Size([8, 100])
tensor([0, 9, 1, 6, 8, 7, 5, 2])
Batch: 1214/1250
torch.Size([8, 100])
tensor([5, 5, 0, 8, 5, 2, 8, 9])
Batch: 1215/1250
torch.Size([8, 100])
tensor([8, 1, 4, 1, 1, 0, 4, 9])
Batch: 1216/1250
torch.Size([8, 100])
tensor([7, 6, 9, 6, 8, 0, 6, 3])
Batch: 1217/1250
torch.Size([8, 100])
tensor([6, 7, 6, 6, 2, 4, 1, 5])
Batch: 1218/1250
torch.Size([8, 100])
tensor([8, 2, 6, 7, 1, 0, 9, 4])
Batch: 1219/1250
torch.Size([8, 100])
tensor([3, 0, 1, 6, 1, 2, 1, 0])
Batch: 1220/1250
torch.Size([8, 100])
tensor([1, 4, 9, 2, 5, 6, 0, 2])
Batch: 1221/1250
torch.Size([8, 100])
tensor([7, 3, 8, 4, 1, 4, 3, 3])
Batch: 1222/1250
torch.Size([8, 100])
tensor([9, 5, 6, 7, 5, 8, 2, 6])
Batch: 1223/1250
torch.Size([8, 100])
tensor([5, 2, 1, 8, 4, 4, 4, 3])
Batch: 1224/1250
torch.Size([8, 100])
tensor([6, 9, 2, 4, 7, 6, 7, 5])
Batch: 1225/1250
torch.Size([8, 100])
tensor([3, 8, 5, 5, 4, 5, 0, 4])
Batch: 1226/1250
torch.Size([8, 100])
tensor([0, 9, 4, 7, 3, 6, 5, 5])
Batch: 1227/1250
torch.Size([8, 100])
tensor([5, 4, 1, 0, 8, 3, 7, 4])
Batch: 1228/1250
torch.Size([8, 100])
tensor([4, 6, 6, 4, 0, 6, 8, 3])
Batch: 1229/1250
torch.Size([8, 100])
tensor([5, 3, 4, 3, 8, 8, 9, 1])
Batch: 1230/1250
torch.Size([8, 100])
tensor([9, 6, 2, 1, 6, 4, 8, 8])
Batch: 1231/1250
torch.Size([8, 100])
tensor([8, 6, 2, 8, 1, 5, 9, 2])
Batch: 1232/1250
torch.Size([8, 100])
tensor([5, 0, 2, 1, 2, 3, 8, 5])
Batch: 1233/1250
torch.Size([8, 100])
tensor([2, 9, 6, 7, 5, 1, 3, 5])
Batch: 1234/1250
torch.Size([8, 100])
tensor([8, 0, 5, 7, 2, 1, 4, 1])
Batch: 1235/1250
torch.Size([8, 100])
tensor([1, 0, 7, 8, 3, 1, 8, 9])
Batch: 1236/1250
torch.Size([8, 100])
tensor([9, 6, 0, 6, 1, 2, 9, 6])
Batch: 1237/1250
torch.Size([8, 100])
tensor([9, 1, 4, 6, 9, 4, 4, 5])
Batch: 1238/1250
torch.Size([8, 100])
tensor([0, 8, 7, 3, 1, 6, 9, 5])
Batch: 1239/1250
torch.Size([8, 100])
tensor([4, 1, 7, 2, 3, 5, 6, 6])
Batch: 1240/1250
torch.Size([8, 100])
tensor([1, 4, 2, 0, 4, 1, 7, 4])
Batch: 1241/1250
torch.Size([8, 100])
tensor([8, 5, 3, 6, 8, 3, 4, 0])
Batch: 1242/1250
torch.Size([8, 100])
tensor([1, 0, 6, 4, 5, 4, 6, 9])
Batch: 1243/1250
torch.Size([8, 100])
tensor([2, 8, 8, 9, 8, 6, 9, 1])
Batch: 1244/1250
torch.Size([8, 100])
tensor([0, 9, 4, 6, 4, 5, 3, 5])
Batch: 1245/1250
torch.Size([8, 100])
tensor([9, 9, 8, 1, 0, 9, 1, 5])
Batch: 1246/1250
torch.Size([8, 100])
tensor([1, 8, 4, 3, 1, 6, 7, 2])
Batch: 1247/1250
torch.Size([8, 100])
tensor([5, 3, 3, 1, 6, 1, 4, 4])
Batch: 1248/1250
torch.Size([8, 100])
tensor([3, 3, 6, 7, 4, 9, 4, 6])
Batch: 1249/1250
torch.Size([8, 100])
tensor([1, 6, 5, 6, 6, 5, 0, 4])
Batch: 1250/1250
torch.Size([8, 100])
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
WARNING    g:\my drive\msc\0_project\msc_project\main.py:309: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored
  plt.scatter(tsne[:, 0], tsne[:, 1], cmap='viridis')
 [py.warnings]
tensor([9, 5, 1, 2, 6, 0, 2, 0])
9 added
5 added
1 added
2 added
6 added
