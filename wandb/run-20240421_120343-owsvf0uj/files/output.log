Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/1, 10/1875] train_loss: 13049.8787109375
[1/1, 20/1875] train_loss: 10821.822167968752
[1/1, 30/1875] train_loss: 10384.05078125
[1/1, 40/1875] train_loss: 9972.05478515625
[1/1, 50/1875] train_loss: 9324.09208984375
[1/1, 60/1875] train_loss: 8855.308984375
[1/1, 70/1875] train_loss: 8672.499609375001
[1/1, 80/1875] train_loss: 8503.660546875
[1/1, 90/1875] train_loss: 8262.03583984375
[1/1, 100/1875] train_loss: 8078.530908203125
[1/1, 110/1875] train_loss: 7978.5625
[1/1, 120/1875] train_loss: 7948.282324218751
[1/1, 130/1875] train_loss: 7876.508056640624
[1/1, 140/1875] train_loss: 7762.428662109375
[1/1, 150/1875] train_loss: 7629.843847656251
[1/1, 160/1875] train_loss: 7501.31982421875
[1/1, 170/1875] train_loss: 7407.827783203125
[1/1, 180/1875] train_loss: 7374.512548828126
[1/1, 190/1875] train_loss: 7251.810839843751
[1/1, 200/1875] train_loss: 7196.256103515626
[1/1, 210/1875] train_loss: 7148.649511718751
[1/1, 220/1875] train_loss: 7132.357226562501
[1/1, 230/1875] train_loss: 7130.227197265624
[1/1, 240/1875] train_loss: 7111.000634765625
[1/1, 250/1875] train_loss: 7056.671142578125
[1/1, 260/1875] train_loss: 6985.394677734375
[1/1, 270/1875] train_loss: 6833.680859374999
[1/1, 280/1875] train_loss: 6730.926757812501
[1/1, 290/1875] train_loss: 6719.99697265625
[1/1, 300/1875] train_loss: 6693.444580078124
[1/1, 310/1875] train_loss: 6689.78642578125
[1/1, 320/1875] train_loss: 6665.720019531251
[1/1, 330/1875] train_loss: 6633.8252929687515
[1/1, 340/1875] train_loss: 6529.408935546875
[1/1, 350/1875] train_loss: 6481.47421875
[1/1, 360/1875] train_loss: 6431.501855468749
[1/1, 370/1875] train_loss: 6397.760205078124
[1/1, 380/1875] train_loss: 6398.474755859374
[1/1, 390/1875] train_loss: 6420.450439453124
[1/1, 400/1875] train_loss: 6443.696240234375
[1/1, 410/1875] train_loss: 6444.681396484375
[1/1, 420/1875] train_loss: 6417.45625
[1/1, 430/1875] train_loss: 6361.923779296874
[1/1, 440/1875] train_loss: 6271.922216796875
[1/1, 450/1875] train_loss: 6233.322265624999
[1/1, 460/1875] train_loss: 6101.930126953124
[1/1, 470/1875] train_loss: 5981.126171875
[1/1, 480/1875] train_loss: 5854.608154296875
[1/1, 490/1875] train_loss: 5736.739697265625
[1/1, 500/1875] train_loss: 5632.515039062499
[1/1, 510/1875] train_loss: 5600.531201171875
[1/1, 520/1875] train_loss: 5616.9720703125
[1/1, 530/1875] train_loss: 5486.440087890625
[1/1, 540/1875] train_loss: 5319.175878906251
[1/1, 550/1875] train_loss: 5204.945751953125
[1/1, 560/1875] train_loss: 5205.48525390625
[1/1, 570/1875] train_loss: 5205.1306640625
[1/1, 580/1875] train_loss: 5186.527587890626
[1/1, 590/1875] train_loss: 5159.535595703124
[1/1, 600/1875] train_loss: 5122.042236328125
[1/1, 610/1875] train_loss: 5080.428271484375
[1/1, 620/1875] train_loss: 4992.81689453125
[1/1, 630/1875] train_loss: 4869.806884765624
[1/1, 640/1875] train_loss: 4864.664599609375
[1/1, 650/1875] train_loss: 4814.749609375
[1/1, 660/1875] train_loss: 4779.537841796874
[1/1, 670/1875] train_loss: 4730.438574218751
[1/1, 680/1875] train_loss: 4712.2525390625
[1/1, 690/1875] train_loss: 4623.60419921875
[1/1, 700/1875] train_loss: 4618.427148437499
[1/1, 710/1875] train_loss: 4620.676660156249
[1/1, 720/1875] train_loss: 4618.452294921875
[1/1, 730/1875] train_loss: 4618.8060546875
[1/1, 740/1875] train_loss: 4583.989013671875
[1/1, 750/1875] train_loss: 4570.519775390625
[1/1, 760/1875] train_loss: 4569.782568359375
[1/1, 770/1875] train_loss: 4568.23603515625
[1/1, 780/1875] train_loss: 4568.931689453126
[1/1, 790/1875] train_loss: 4569.772998046875
[1/1, 800/1875] train_loss: 4544.860546874999
[1/1, 810/1875] train_loss: 4518.508837890625
[1/1, 820/1875] train_loss: 4519.51162109375
[1/1, 830/1875] train_loss: 4485.704785156249
[1/1, 840/1875] train_loss: 4471.232666015625
[1/1, 850/1875] train_loss: 4470.9333984375
[1/1, 860/1875] train_loss: 4472.078076171876
[1/1, 870/1875] train_loss: 4604.128662109375
[1/1, 880/1875] train_loss: 4171.728784179687
[1/1, 890/1875] train_loss: 4046.043408203125
[1/1, 900/1875] train_loss: 3766.8671142578123
[1/1, 910/1875] train_loss: 3462.9791015624996
[1/1, 920/1875] train_loss: 3144.2090087890624
[1/1, 930/1875] train_loss: 2805.43603515625
[1/1, 940/1875] train_loss: 2558.0887939453123
[1/1, 950/1875] train_loss: 2695.5252197265627
[1/1, 960/1875] train_loss: 2487.007373046875
[1/1, 970/1875] train_loss: 2403.7633056640625
[1/1, 980/1875] train_loss: 2314.987475585938
[1/1, 990/1875] train_loss: 2127.251025390625
[1/1, 1000/1875] train_loss: 2105.6572265625
[1/1, 1010/1875] train_loss: 1939.5121582031254
[1/1, 1020/1875] train_loss: 1848.5521240234375
[1/1, 1030/1875] train_loss: 1766.310437011719
[1/1, 1040/1875] train_loss: 1681.9530517578125
[1/1, 1050/1875] train_loss: 1666.28818359375
[1/1, 1060/1875] train_loss: 1667.4296264648435
[1/1, 1070/1875] train_loss: 1632.4583862304687
[1/1, 1080/1875] train_loss: 1619.3216552734375
[1/1, 1090/1875] train_loss: 1618.3287231445313
[1/1, 1100/1875] train_loss: 1619.1191162109374
[1/1, 1110/1875] train_loss: 1618.3946044921877
[1/1, 1120/1875] train_loss: 1618.7159423828127
[1/1, 1130/1875] train_loss: 1617.9426391601564
[1/1, 1140/1875] train_loss: 1603.1688110351565
[1/1, 1150/1875] train_loss: 1568.131164550781
[1/1, 1160/1875] train_loss: 1568.0693847656248
[1/1, 1170/1875] train_loss: 1568.8374145507814
[1/1, 1180/1875] train_loss: 1623.0887329101565
[1/1, 1190/1875] train_loss: 1622.7454467773437
[1/1, 1200/1875] train_loss: 1618.0663696289064
[1/1, 1210/1875] train_loss: 1618.5279052734375
[1/1, 1220/1875] train_loss: 1574.389501953125
[1/1, 1230/1875] train_loss: 1524.5091308593749
[1/1, 1240/1875] train_loss: 1569.4355224609376
[1/1, 1250/1875] train_loss: 1568.901318359375
[1/1, 1260/1875] train_loss: 1534.8472900390623
[1/1, 1270/1875] train_loss: 1521.1905029296875
[1/1, 1280/1875] train_loss: 1519.3765625
[1/1, 1290/1875] train_loss: 1518.58984375
[1/1, 1300/1875] train_loss: 1519.0499877929688
[1/1, 1310/1875] train_loss: 1519.8891845703124
[1/1, 1320/1875] train_loss: 1519.708386230469
[1/1, 1330/1875] train_loss: 1519.1507080078125
[1/1, 1340/1875] train_loss: 1519.9624755859377
[1/1, 1350/1875] train_loss: 1519.2565429687502
[1/1, 1360/1875] train_loss: 1519.282958984375
[1/1, 1370/1875] train_loss: 1518.5514770507814
[1/1, 1380/1875] train_loss: 1518.9738525390626
[1/1, 1390/1875] train_loss: 1519.9207519531249
[1/1, 1400/1875] train_loss: 1519.6497802734373
[1/1, 1410/1875] train_loss: 1518.5329833984372
[1/1, 1420/1875] train_loss: 1505.4343872070312
[1/1, 1430/1875] train_loss: 1469.9440673828126
[1/1, 1440/1875] train_loss: 1469.9588989257813
[1/1, 1450/1875] train_loss: 1469.658703613281
[1/1, 1460/1875] train_loss: 1469.736706542969
[1/1, 1470/1875] train_loss: 1469.098864746094
[1/1, 1480/1875] train_loss: 1469.885437011719
[1/1, 1490/1875] train_loss: 1469.0210205078126
[1/1, 1500/1875] train_loss: 1434.5566284179688
[1/1, 1510/1875] train_loss: 1419.3374877929687
[1/1, 1520/1875] train_loss: 1421.4458007812498
[1/1, 1530/1875] train_loss: 1418.8528686523437
[1/1, 1540/1875] train_loss: 1420.1835327148438
[1/1, 1550/1875] train_loss: 1442.5081909179687
[1/1, 1560/1875] train_loss: 1327.6934692382813
[1/1, 1570/1875] train_loss: 1114.8890625
[1/1, 1580/1875] train_loss: 1040.350244140625
[1/1, 1590/1875] train_loss: 987.086785888672
[1/1, 1600/1875] train_loss: 987.9262451171875
[1/1, 1610/1875] train_loss: 977.932244873047
[1/1, 1620/1875] train_loss: 943.9129699707032
[1/1, 1630/1875] train_loss: 978.2067138671873
[1/1, 1640/1875] train_loss: 978.91767578125
[1/1, 1650/1875] train_loss: 983.3976745605468
[1/1, 1660/1875] train_loss: 929.1546813964843
[1/1, 1670/1875] train_loss: 937.9659912109374
[1/1, 1680/1875] train_loss: 928.440185546875
[1/1, 1690/1875] train_loss: 936.0138122558594
[1/1, 1700/1875] train_loss: 927.8306091308593
[1/1, 1710/1875] train_loss: 928.5427734375
[1/1, 1720/1875] train_loss: 929.137774658203
[1/1, 1730/1875] train_loss: 932.7311706542968
[1/1, 1740/1875] train_loss: 928.1992492675781
[1/1, 1750/1875] train_loss: 928.880810546875
[1/1, 1760/1875] train_loss: 928.7295349121093
[1/1, 1770/1875] train_loss: 928.2346313476562
[1/1, 1780/1875] train_loss: 928.4015625000002
[1/1, 1790/1875] train_loss: 930.0773193359375
[1/1, 1800/1875] train_loss: 960.9258056640624
[1/1, 1810/1875] train_loss: 928.4852661132812
[1/1, 1820/1875] train_loss: 878.6774841308593
[1/1, 1830/1875] train_loss: 812.5254943847657
[1/1, 1840/1875] train_loss: 782.8242553710937
[1/1, 1850/1875] train_loss: 797.8416442871094
[1/1, 1860/1875] train_loss: 748.1173828124998
[1/1, 1870/1875] train_loss: 729.4352661132813
5/5: Testing network...
test_loss: 0.6990025334464856
Training Finished
tensor([9, 8, 8, 9, 3, 2, 6, 1, 5, 1, 9, 9, 6, 7, 2, 3, 2, 0, 5, 9, 0, 9, 1, 0,
        7, 9, 8, 4, 9, 2, 8, 1])
9 added
8 added
3 added
2 added
6 added
1 added
5 added
7 added
0 added
4 added