Starting Sweep: Batch Size: 64, Learning Rate: 0.001
Making datasets and defining subsets
Training: 60000 -> 12000
Testing: 10000 -> 2000
Making Subsets
Training: 12000
Testing: 2000
Making Dataloaders
Defining network
Training!
[1/6, 10/188] Training Loss: 5.179575395584107
[1/6, 20/188] Training Loss: 1.1049717903137206
[1/6, 30/188] Training Loss: 1.12150799036026
[1/6, 40/188] Training Loss: 1.088376957178116
[1/6, 50/188] Training Loss: 1.0439496338367462
[1/6, 60/188] Training Loss: 1.0579824984073638
[1/6, 70/188] Training Loss: 1.0303987622261048
[1/6, 80/188] Training Loss: 1.0179775595664977
[1/6, 90/188] Training Loss: 1.0069439113140106
[1/6, 100/188] Training Loss: 1.0126767575740814
[1/6, 110/188] Training Loss: 0.9601849913597107
[1/6, 120/188] Training Loss: 1.034476923942566
[1/6, 130/188] Training Loss: 0.9917110919952392
[1/6, 140/188] Training Loss: 1.0020461559295655
[1/6, 150/188] Training Loss: 1.0680174767971038
[1/6, 160/188] Training Loss: 1.0311610519886016
[1/6, 170/188] Training Loss: 0.9757598996162414
[1/6, 180/188] Training Loss: 0.9772197544574738
Testing!
[1/6, 1/32]
[1/6, 3/32]
[1/6, 5/32]
[1/6, 7/32]
[1/6, 9/32]
[1/6, 11/32]
[1/6, 13/32]
[1/6, 15/32]
[1/6, 17/32]
[1/6, 19/32]
[1/6, 21/32]
[1/6, 23/32]
[1/6, 25/32]
[1/6, 27/32]
[1/6, 29/32]
[1/6, 31/32]
Testing Loss: 0.9435512442141771
Training!
[2/6, 10/188] Training Loss: 0.9614276587963104
[2/6, 20/188] Training Loss: 0.9826124787330628
[2/6, 30/188] Training Loss: 0.9493228554725647
[2/6, 40/188] Training Loss: 0.9257335424423218
[2/6, 50/188] Training Loss: 0.922352421283722
[2/6, 60/188] Training Loss: 0.9196320354938508
[2/6, 70/188] Training Loss: 0.9383799314498902
[2/6, 80/188] Training Loss: 0.9104814946651458
[2/6, 90/188] Training Loss: 0.9230429947376251
[2/6, 100/188] Training Loss: 1.083360207080841
[2/6, 110/188] Training Loss: 0.9904165863990784
[2/6, 120/188] Training Loss: 0.9835276484489441
[2/6, 130/188] Training Loss: 0.9297789692878723
[2/6, 140/188] Training Loss: 0.937617939710617
[2/6, 150/188] Training Loss: 0.9308218538761139
[2/6, 160/188] Training Loss: 0.9001502931118012
[2/6, 170/188] Training Loss: 0.8913106501102448
[2/6, 180/188] Training Loss: 0.8733715057373047
Testing!
[2/6, 1/32]
[2/6, 3/32]
[2/6, 5/32]
[2/6, 7/32]
[2/6, 9/32]
[2/6, 11/32]
[2/6, 13/32]
[2/6, 15/32]
[2/6, 17/32]
[2/6, 19/32]
[2/6, 21/32]
[2/6, 23/32]
[2/6, 25/32]
[2/6, 27/32]
[2/6, 29/32]
[2/6, 31/32]
Testing Loss: 0.9006815068423748
Training!
[3/6, 10/188] Training Loss: 0.8977154970169068
[3/6, 20/188] Training Loss: 0.8877213835716248
[3/6, 30/188] Training Loss: 0.8365525603294373
[3/6, 40/188] Training Loss: 0.8856931149959564
[3/6, 50/188] Training Loss: 0.8347726941108704
[3/6, 60/188] Training Loss: 0.8203141629695893
[3/6, 70/188] Training Loss: 0.8555614411830902
[3/6, 80/188] Training Loss: 0.8450604021549225
[3/6, 90/188] Training Loss: 0.8401816606521606
[3/6, 100/188] Training Loss: 0.8212524592876435
[3/6, 110/188] Training Loss: 0.8181868493556976
[3/6, 120/188] Training Loss: 0.8850703120231629
[3/6, 130/188] Training Loss: 1.0465138137340546
[3/6, 140/188] Training Loss: 1.1059999227523805
[3/6, 150/188] Training Loss: 1.1084807157516479
[3/6, 160/188] Training Loss: 1.0890812277793884
[3/6, 170/188] Training Loss: 1.0895000457763673
[3/6, 180/188] Training Loss: 1.0923851132392883
Testing!
[3/6, 1/32]
[3/6, 3/32]
[3/6, 5/32]
[3/6, 7/32]
[3/6, 9/32]
[3/6, 11/32]
[3/6, 13/32]
[3/6, 15/32]
[3/6, 17/32]
[3/6, 19/32]
[3/6, 21/32]
[3/6, 23/32]
[3/6, 25/32]
[3/6, 27/32]
[3/6, 29/32]
[3/6, 31/32]
Testing Loss: 0.9490701382358869
Training!
[4/6, 10/188] Training Loss: 1.0629755914211274
[4/6, 20/188] Training Loss: 1.0233417749404907
[4/6, 30/188] Training Loss: 1.0287713527679443
[4/6, 40/188] Training Loss: 1.016209852695465
[4/6, 50/188] Training Loss: 1.059228640794754
[4/6, 60/188] Training Loss: 1.0078065872192383
[4/6, 70/188] Training Loss: 1.021886122226715
[4/6, 80/188] Training Loss: 0.9847640752792358
[4/6, 90/188] Training Loss: 1.0671904027462005
[4/6, 100/188] Training Loss: 1.0205972790718079
[4/6, 110/188] Training Loss: 1.027332329750061
[4/6, 120/188] Training Loss: 1.0253178477287292
[4/6, 130/188] Training Loss: 1.0510937929153443
[4/6, 140/188] Training Loss: 1.028143048286438
[4/6, 150/188] Training Loss: 1.0101163506507873
[4/6, 160/188] Training Loss: 1.0125999569892883
[4/6, 170/188] Training Loss: 0.973094379901886
[4/6, 180/188] Training Loss: 0.9826904177665711
Testing!
[4/6, 1/32]
[4/6, 3/32]
[4/6, 5/32]
[4/6, 7/32]
[4/6, 9/32]
[4/6, 11/32]
[4/6, 13/32]
[4/6, 15/32]
[4/6, 17/32]
[4/6, 19/32]
[4/6, 21/32]
[4/6, 23/32]
[4/6, 25/32]
[4/6, 27/32]
[4/6, 29/32]
[4/6, 31/32]
Testing Loss: 0.9518139795400202
Training!
[5/6, 10/188] Training Loss: 0.9782128632068634
[5/6, 20/188] Training Loss: 0.9449363589286804
[5/6, 30/188] Training Loss: 0.9820537865161896
[5/6, 40/188] Training Loss: 0.9905571579933167
[5/6, 50/188] Training Loss: 0.9926430761814118
[5/6, 60/188] Training Loss: 0.9423712909221649
[5/6, 70/188] Training Loss: 0.9600812196731567
[5/6, 80/188] Training Loss: 0.9582602381706238
[5/6, 90/188] Training Loss: 0.959672749042511
[5/6, 100/188] Training Loss: 0.9323790073394775
[5/6, 110/188] Training Loss: 0.919196480512619
[5/6, 120/188] Training Loss: 0.9347030818462372
[5/6, 130/188] Training Loss: 0.9431238353252411
[5/6, 140/188] Training Loss: 0.9100410103797912
[5/6, 150/188] Training Loss: 0.9241600334644318
[5/6, 160/188] Training Loss: 0.9595871806144715
[5/6, 170/188] Training Loss: 0.9464449107646942
[5/6, 180/188] Training Loss: 0.9207227945327758
Testing!
[5/6, 1/32]
[5/6, 3/32]
[5/6, 5/32]
[5/6, 7/32]
[5/6, 9/32]
[5/6, 11/32]
[5/6, 13/32]
[5/6, 15/32]
[5/6, 17/32]
[5/6, 19/32]
[5/6, 21/32]
[5/6, 23/32]
[5/6, 25/32]
[5/6, 27/32]
[5/6, 29/32]
[5/6, 31/32]
Testing Loss: 0.9386461392045021
Training!
[6/6, 10/188] Training Loss: 0.9556264698505401
[6/6, 20/188] Training Loss: 0.9155275881290436
[6/6, 30/188] Training Loss: 0.9091046452522278
[6/6, 40/188] Training Loss: 0.9151840567588806
[6/6, 50/188] Training Loss: 0.9033113241195678
[6/6, 60/188] Training Loss: 0.8680143475532531
[6/6, 70/188] Training Loss: 0.9243841588497161
[6/6, 80/188] Training Loss: 0.8810433506965637
[6/6, 90/188] Training Loss: 0.8660159230232238
[6/6, 100/188] Training Loss: 0.9071230411529541
[6/6, 110/188] Training Loss: 0.8727007389068604
[6/6, 120/188] Training Loss: 0.8740478873252868
[6/6, 130/188] Training Loss: 0.8560902535915375
[6/6, 140/188] Training Loss: 0.8721884548664093
[6/6, 150/188] Training Loss: 0.8608186900615692
[6/6, 160/188] Training Loss: 0.8558023512363434
[6/6, 170/188] Training Loss: 0.8452053904533386
[6/6, 180/188] Training Loss: 0.8460909962654114
Testing!
[6/6, 1/32]
[6/6, 3/32]
[6/6, 5/32]
[6/6, 7/32]
[6/6, 9/32]
[6/6, 11/32]
[6/6, 13/32]
[6/6, 15/32]
[6/6, 17/32]
[6/6, 19/32]
[6/6, 21/32]
[6/6, 23/32]
[6/6, 25/32]
[6/6, 27/32]
[6/6, 29/32]
[6/6, 31/32]
Testing Loss: 0.9180784228568276
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/32
Batch: 2/32
Batch: 3/32
Batch: 4/32
Batch: 5/32
Batch: 6/32
Batch: 7/32
Batch: 8/32
Batch: 9/32
Batch: 10/32
Batch: 11/32
Batch: 12/32
Batch: 13/32
Batch: 14/32
Batch: 15/32
Batch: 16/32
Batch: 17/32
Batch: 18/32
Batch: 19/32
Batch: 20/32
Batch: 21/32
Batch: 22/32
Batch: 23/32
Batch: 24/32
Batch: 25/32
Batch: 26/32
Batch: 27/32
Batch: 28/32
Batch: 29/32
Batch: 30/32
Batch: 31/32
Batch: 32/32
Features: (2000, 100)
All Labels: (2000,)
All Original Images: (2000, 28, 28)
All Reconstructed Images: (2000, 28, 28)
Applying t-SNE
Plotting Results Grid
3 added
6 added
1 added
0 added
4 added
2 added
9 added
7 added
8 added
5 added
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation