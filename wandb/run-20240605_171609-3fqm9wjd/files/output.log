Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Subsets
Training: 6000
Testing: 1000
(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0745, 0.5843, 0.7529, 0.9961, 0.9961, 0.9059,
          0.1804, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0118, 0.2431, 0.7725, 0.9922, 0.9922, 0.9922, 0.9804, 0.9961,
          0.8667, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.3961, 0.9922, 0.9922, 0.9922, 0.8000, 0.3569, 0.2118, 0.6549,
          0.9490, 0.0863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078,
          0.7098, 0.9922, 0.8353, 0.1569, 0.0314, 0.0000, 0.0000, 0.2196,
          0.9804, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,
          0.9922, 0.9137, 0.2078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.8392, 0.3294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0510,
          0.8706, 0.8902, 0.0353, 0.0000, 0.0000, 0.0000, 0.0510, 0.3059,
          0.9804, 0.9294, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,
          0.7686, 0.9922, 0.7961, 0.5059, 0.6314, 0.7216, 0.8353, 0.9922,
          0.9922, 0.9922, 0.5294, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.4275, 0.9176, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,
          0.9922, 0.9922, 0.9098, 0.0471, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.1686, 0.8941, 0.9922, 0.9961, 0.9922, 0.6941, 0.3804,
          0.2549, 0.8706, 0.9961, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0118, 0.0157, 0.0157, 0.0157, 0.0039, 0.0000,
          0.0000, 0.6627, 0.9961, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3529, 0.9922, 0.4549, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.1020, 0.9216, 0.6784, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8784, 0.8235, 0.0353, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8784, 0.9922, 0.0784, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8824, 0.9922, 0.0784, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8784, 0.8941, 0.0549, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8824, 0.6784, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8824, 0.3922, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8824, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.8784, 0.2902, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]]), 9)
<class 'tuple'>
2
Making Dataloaders
Defining network
2024-06-05 17:16:11.274784
Training!
0:00:00.014883
[1/3, 5/94] Training Loss: 2.6337430477142334
[1/3, 10/94] Training Loss: 1.656256079673767
[1/3, 15/94] Training Loss: 1.2283728122711182
[1/3, 20/94] Training Loss: 1.128537654876709
[1/3, 25/94] Training Loss: 1.0375646352767944
[1/3, 30/94] Training Loss: 1.0379419326782227
[1/3, 35/94] Training Loss: 0.9627936482429504
[1/3, 40/94] Training Loss: 1.0336649417877197
[1/3, 45/94] Training Loss: 0.9901201128959656
[1/3, 50/94] Training Loss: 0.9739565849304199
[1/3, 55/94] Training Loss: 0.9531819224357605
[1/3, 60/94] Training Loss: 0.9534501433372498
[1/3, 65/94] Training Loss: 0.9648928642272949
[1/3, 70/94] Training Loss: 0.971179187297821
[1/3, 75/94] Training Loss: 0.9440068602561951
[1/3, 80/94] Training Loss: 0.9258179068565369
[1/3, 85/94] Training Loss: 0.9109340906143188
[1/3, 90/94] Training Loss: 0.9549280405044556
Testing!
[1/3, 1/16]
[1/3, 2/16]
[1/3, 3/16]
[1/3, 4/16]
[1/3, 5/16]
[1/3, 6/16]
[1/3, 7/16]
[1/3, 8/16]
[1/3, 9/16]
[1/3, 10/16]
[1/3, 11/16]
[1/3, 12/16]
[1/3, 13/16]
[1/3, 14/16]
[1/3, 15/16]
[1/3, 16/16]
Testing Loss: 0.9159870147705078
Training!
0:02:05.863731
[2/3, 5/94] Training Loss: 0.9613187909126282
[2/3, 10/94] Training Loss: 0.9280031323432922
[2/3, 15/94] Training Loss: 0.9197784662246704
[2/3, 20/94] Training Loss: 0.88922518491745
[2/3, 25/94] Training Loss: 0.8795620799064636
[2/3, 30/94] Training Loss: 0.9092747569084167
[2/3, 35/94] Training Loss: 0.8645338416099548
[2/3, 40/94] Training Loss: 0.8548556566238403
[2/3, 45/94] Training Loss: 0.8695617914199829
[2/3, 50/94] Training Loss: 0.8611550331115723
[2/3, 55/94] Training Loss: 0.8660001754760742
[2/3, 60/94] Training Loss: 0.8239928483963013
[2/3, 65/94] Training Loss: 0.8048061728477478
[2/3, 70/94] Training Loss: 0.8065109252929688
[2/3, 75/94] Training Loss: 0.8093876838684082
[2/3, 80/94] Training Loss: 0.8196477293968201
[2/3, 85/94] Training Loss: 0.8324311375617981
[2/3, 90/94] Training Loss: 0.8127649426460266
Testing!
[2/3, 1/16]
[2/3, 2/16]
[2/3, 3/16]
[2/3, 4/16]
[2/3, 5/16]
[2/3, 6/16]
[2/3, 7/16]
[2/3, 8/16]
[2/3, 9/16]
[2/3, 10/16]
[2/3, 11/16]
[2/3, 12/16]
[2/3, 13/16]
[2/3, 14/16]
[2/3, 15/16]
[2/3, 16/16]
Testing Loss: 0.7939566969871521
Training!
0:04:09.273562
[3/3, 5/94] Training Loss: 0.80427485704422
[3/3, 10/94] Training Loss: 0.7929432988166809
[3/3, 15/94] Training Loss: 0.7557235360145569
[3/3, 20/94] Training Loss: 0.7849944829940796
[3/3, 25/94] Training Loss: 0.7716407179832458
[3/3, 30/94] Training Loss: 0.7493374943733215
[3/3, 35/94] Training Loss: 0.7360020279884338
[3/3, 40/94] Training Loss: 0.7554642558097839
[3/3, 45/94] Training Loss: 0.7668289542198181
[3/3, 50/94] Training Loss: 0.7919823527336121
[3/3, 55/94] Training Loss: 0.7534301280975342
[3/3, 60/94] Training Loss: 0.709599494934082
[3/3, 65/94] Training Loss: 0.7259828448295593
[3/3, 70/94] Training Loss: 0.7262550592422485
[3/3, 75/94] Training Loss: 0.7118861079216003
[3/3, 80/94] Training Loss: 0.7242509722709656
[3/3, 85/94] Training Loss: 0.6903401017189026
[3/3, 90/94] Training Loss: 0.7197365760803223
Testing!
[3/3, 1/16]
[3/3, 2/16]
[3/3, 3/16]
[3/3, 4/16]
[3/3, 5/16]
[3/3, 6/16]
[3/3, 7/16]
[3/3, 8/16]
[3/3, 9/16]
[3/3, 10/16]
[3/3, 11/16]
[3/3, 12/16]
[3/3, 13/16]
[3/3, 14/16]
[3/3, 15/16]
[3/3, 16/16]
Testing Loss: 0.7785961627960205
Training and Testing Finished
0.7785961627960205
<class 'float'>
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation