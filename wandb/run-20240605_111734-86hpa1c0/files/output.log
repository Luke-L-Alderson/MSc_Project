Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 3000
Testing: 10000 -> 500
Making Subsets
Training: 3000
Testing: 500
Making Dataloaders
Defining network
2024-06-05 11:17:36.100577
Training!
0:00:00.000992
0:00:00.000992
0:00:00.034225
0:00:00.034721
0:00:00.161237
0:00:00.161733
0:00:01.111604
0:00:01.134421
0:00:01.664780
0:00:01.686125
tensor(17.4480, device='cuda:0', grad_fn=<DivBackward0>)
tensor(17.4480, device='cuda:0')
0:00:01.717870
0:00:01.732750
0:00:01.733246
0:00:01.848390
0:00:01.849387
0:00:02.467593
0:00:02.472553
0:00:02.939398
0:00:02.940390
tensor(14.5501, device='cuda:0', grad_fn=<DivBackward0>)
tensor(14.5501, device='cuda:0')
0:00:02.942870
0:00:02.961222
0:00:02.961718
0:00:03.091732
0:00:03.092724
0:00:03.714881
0:00:03.720337
0:00:04.215017
0:00:04.216008
tensor(10.8432, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.8432, device='cuda:0')
0:00:04.218489
[1/1, 3/47] Training Loss: 14.28040885925293
0:00:04.236841
0:00:04.237339
0:00:04.365322
0:00:04.365817
0:00:05.031105
0:00:05.037058
0:00:05.515407
0:00:05.516399
tensor(10.0879, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.0879, device='cuda:0')
0:00:05.518879
0:00:05.536735
0:00:05.536735
0:00:05.661780
0:00:05.662274
0:00:06.373298
0:00:06.379251
0:00:06.841143
0:00:06.842135
tensor(7.9785, device='cuda:0', grad_fn=<DivBackward0>)
tensor(7.9785, device='cuda:0')
0:00:06.844615
0:00:06.862967
0:00:06.863463
0:00:06.988533
0:00:06.989525
0:00:07.667377
0:00:07.673329
0:00:08.149148
0:00:08.150140
tensor(6.4009, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4009, device='cuda:0')
0:00:08.152620
[1/1, 6/47] Training Loss: 8.155780792236328
0:00:08.171968
0:00:08.172462
0:00:08.302971
0:00:08.303466
0:00:08.902760
0:00:08.908712
0:00:09.374593
0:00:09.376081
tensor(6.4200, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4200, device='cuda:0')
0:00:09.378066
0:00:09.395923
0:00:09.395923
0:00:09.517606
0:00:09.518102
0:00:10.104506
0:00:10.109981
0:00:10.554994
0:00:10.555986
tensor(5.4065, device='cuda:0', grad_fn=<DivBackward0>)
tensor(5.4065, device='cuda:0')
0:00:10.558467
0:00:10.575331
0:00:10.575331
0:00:10.691497
0:00:10.691993
0:00:11.272969
0:00:11.278426
0:00:11.738609
0:00:11.739601
tensor(4.2060, device='cuda:0', grad_fn=<DivBackward0>)
tensor(4.2060, device='cuda:0')
0:00:11.742081
[1/1, 9/47] Training Loss: 5.344161033630371
0:00:11.764898
0:00:11.765394
0:00:11.900821
0:00:11.901317
0:00:12.487273
0:00:12.492731
0:00:12.944693
0:00:12.945685
tensor(3.9051, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.9051, device='cuda:0')
0:00:12.947669
0:00:12.968006
0:00:12.968006
0:00:13.098471
0:00:13.098968
0:00:13.679465
0:00:13.684922
0:00:14.127236
0:00:14.128227
tensor(3.3635, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.3635, device='cuda:0')
0:00:14.130708
0:00:14.151540
0:00:14.152035
0:00:14.273155
0:00:14.273651
0:00:14.847649
0:00:14.853105
0:00:15.312528
0:00:15.313520
tensor(2.9286, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.9286, device='cuda:0')
0:00:15.315504
[1/1, 12/47] Training Loss: 3.399035930633545
0:00:15.332865
0:00:15.333360
0:00:15.461344
0:00:15.462337
0:00:16.038968
0:00:16.044921
0:00:16.495931
0:00:16.496925
tensor(2.7782, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.7782, device='cuda:0')
0:00:16.498909
0:00:16.516282
0:00:16.516779
0:00:16.634368
0:00:16.634864
0:00:17.212830
0:00:17.219293
0:00:17.664798
0:00:17.666287
tensor(2.4232, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.4232, device='cuda:0')
0:00:17.669262
0:00:17.687118
0:00:17.687118
0:00:17.804192
0:00:17.804689
0:00:18.389334
0:00:18.394791
0:00:18.841795
0:00:18.842786
tensor(2.1855, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.1855, device='cuda:0')
0:00:18.845266
[1/1, 15/47] Training Loss: 2.4623208045959473
0:00:18.862628
0:00:18.863122
0:00:18.986645
0:00:18.987637
0:00:19.566649
0:00:19.572601
0:00:20.022583
0:00:20.024072
tensor(2.0892, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.0892, device='cuda:0')
0:00:20.026056
0:00:20.043416
0:00:20.043912
0:00:20.168147
0:00:20.168643
0:00:20.735731
0:00:20.741186
0:00:21.192653
0:00:21.193645
tensor(1.8822, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.8822, device='cuda:0')
0:00:21.196125
0:00:21.212495
0:00:21.212991
0:00:21.334542
0:00:21.335038
0:00:21.918470
0:00:21.923938
0:00:22.366759
0:00:22.367751
tensor(1.7336, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.7336, device='cuda:0')
0:00:22.369735
[1/1, 18/47] Training Loss: 1.9016878604888916
0:00:22.387096
0:00:22.387591
0:00:22.504171
0:00:22.504667
0:00:23.073216
0:00:23.078670
0:00:23.524196
0:00:23.525188
tensor(1.6284, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6284, device='cuda:0')
0:00:23.527683
0:00:23.545043
0:00:23.545043
0:00:23.664116
0:00:23.664612
0:00:24.268443
0:00:24.273899
0:00:24.741981
0:00:24.742973
tensor(1.6005, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6005, device='cuda:0')
0:00:24.745453
0:00:24.763309
0:00:24.763309
0:00:24.886333
0:00:24.887325
0:00:25.506505
0:00:25.514937
0:00:26.128166
0:00:26.129158
tensor(1.4798, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.4798, device='cuda:0')
0:00:26.131651
[1/1, 21/47] Training Loss: 1.569542407989502
0:00:26.154005
0:00:26.154005
0:00:26.272671
0:00:26.273166
0:00:26.856150
0:00:26.861605
0:00:27.301667
0:00:27.302659
tensor(1.3867, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3867, device='cuda:0')
0:00:27.305139
0:00:27.322006
0:00:27.322500
0:00:27.442566
0:00:27.443062
0:00:28.019535
0:00:28.024992
0:00:28.523169
0:00:28.524161
tensor(1.3875, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3875, device='cuda:0')
0:00:28.526641
0:00:28.543522
0:00:28.544017
0:00:28.665086
0:00:28.665581
0:00:29.283610
0:00:29.291546
0:00:29.792105
0:00:29.793097
tensor(1.2764, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2764, device='cuda:0')
0:00:29.795576
[1/1, 24/47] Training Loss: 1.3502087593078613
0:00:29.813929
0:00:29.813929
0:00:29.938940
0:00:29.939944
0:00:30.558620
0:00:30.564573
0:00:31.025116
0:00:31.026108
tensor(1.2538, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2538, device='cuda:0')
0:00:31.028094
0:00:31.044476
0:00:31.044971
0:00:31.163534
0:00:31.164538
0:00:31.782216
0:00:31.788167
0:00:32.284355
0:00:32.285347
tensor(1.2355, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2355, device='cuda:0')
0:00:32.287827
0:00:32.305188
0:00:32.305683
0:00:32.434662
0:00:32.435158
0:00:33.066929
0:00:33.072384
0:00:33.526372
0:00:33.527364
tensor(1.1637, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1637, device='cuda:0')
0:00:33.529844
[1/1, 27/47] Training Loss: 1.2176612615585327
0:00:33.548212
0:00:33.548707
0:00:33.654868
0:00:33.655364
0:00:34.261672
0:00:34.268616
0:00:34.750340
0:00:34.751332
tensor(1.2085, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2085, device='cuda:0')
0:00:34.753812
0:00:34.770676
0:00:34.770676
0:00:34.889733
0:00:34.890229
0:00:35.481169
0:00:35.486626
0:00:35.972317
0:00:35.973309
tensor(1.1864, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1864, device='cuda:0')
0:00:35.975789
0:00:35.996623
0:00:35.997117
0:00:36.121631
0:00:36.122623
0:00:36.696188
0:00:36.701645
0:00:37.145159
0:00:37.146150
tensor(1.1314, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1314, device='cuda:0')
0:00:37.148630
[1/1, 30/47] Training Loss: 1.175424337387085
0:00:37.166502
0:00:37.166502
0:00:37.293747
0:00:37.294242
0:00:37.887582
0:00:37.893534
0:00:38.344536
0:00:38.345528
tensor(1.0943, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0943, device='cuda:0')
0:00:38.348008
0:00:38.364885
0:00:38.365381
0:00:38.485428
0:00:38.485924
0:00:39.061903
0:00:39.067359
0:00:39.524980
0:00:39.525972
tensor(1.2049, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2049, device='cuda:0')
0:00:39.528452
0:00:39.545815
0:00:39.545815
0:00:39.672824
0:00:39.673816
0:00:40.301439
0:00:40.307390
0:00:40.761833
0:00:40.762826
tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0425, device='cuda:0')
0:00:40.765307
[1/1, 33/47] Training Loss: 1.1139262914657593
0:00:40.783659
0:00:40.784154
0:00:40.906190
0:00:40.906687
0:00:41.502030
0:00:41.507629
0:00:41.961117
0:00:41.962108
tensor(1.0232, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0232, device='cuda:0')
0:00:41.964601
0:00:41.981466
0:00:41.981466
0:00:42.095563
0:00:42.096556
0:00:42.684972
0:00:42.690441
0:00:43.142372
0:00:43.143363
tensor(1.0806, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0806, device='cuda:0')
0:00:43.145844
0:00:43.163206
0:00:43.163702
0:00:43.288262
0:00:43.289254
0:00:43.880115
0:00:43.885571
0:00:44.334761
0:00:44.335752
tensor(1.1029, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1029, device='cuda:0')
0:00:44.338232
[1/1, 36/47] Training Loss: 1.0688917636871338
0:00:44.356588
0:00:44.356588
0:00:44.476662
0:00:44.477158
0:00:45.061556
0:00:45.067011
0:00:45.508068
0:00:45.509060
tensor(1.0298, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0298, device='cuda:0')
0:00:45.511044
0:00:45.527909
0:00:45.528405
0:00:45.646967
0:00:45.647959
0:00:46.246476
0:00:46.251933
0:00:46.702406
0:00:46.703398
tensor(1.0997, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0997, device='cuda:0')
0:00:46.705878
0:00:46.723238
0:00:46.723238
0:00:46.843786
0:00:46.844282
0:00:47.416826
0:00:47.422282
0:00:47.864802
0:00:47.865794
tensor(0.9907, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9907, device='cuda:0')
0:00:47.868274
[1/1, 39/47] Training Loss: 1.0400631427764893
0:00:47.885649
0:00:47.885649
0:00:48.012150
0:00:48.012646
0:00:48.727345
0:00:48.733297
0:00:49.177789
0:00:49.178793
tensor(0.9719, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9719, device='cuda:0')
0:00:49.181279
0:00:49.198178
0:00:49.198178
0:00:49.314754
0:00:49.315251
0:00:49.909582
0:00:49.915038
0:00:50.377155
0:00:50.378147
tensor(1.0818, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0818, device='cuda:0')
0:00:50.380131
0:00:50.397506
0:00:50.397506
0:00:50.514083
0:00:50.514579
0:00:51.084114
0:00:51.089570
0:00:51.532117
0:00:51.533108
tensor(1.0463, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0463, device='cuda:0')
0:00:51.535588
[1/1, 42/47] Training Loss: 1.033341646194458
0:00:51.553941
0:00:51.553941
0:00:51.671017
0:00:51.672008
0:00:52.271404
0:00:52.277355
0:00:52.727552
0:00:52.728544
tensor(1.0078, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0078, device='cuda:0')
0:00:52.731024
0:00:52.747888
0:00:52.748384
0:00:52.873395
0:00:52.873891
0:00:53.448897
0:00:53.454848
0:00:53.904828
0:00:53.906317
tensor(1.0364, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0364, device='cuda:0')
0:00:53.908301
0:00:53.925661
0:00:53.925661
0:00:54.038768
0:00:54.039264
0:00:54.640133
0:00:54.645589
0:00:55.098548
0:00:55.099540
tensor(1.0439, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0439, device='cuda:0')
0:00:55.101523
[1/1, 45/47] Training Loss: 1.029369592666626
0:00:55.119380
0:00:55.119380
0:00:55.238481
0:00:55.238976
0:00:55.820919
0:00:55.827368
0:00:56.301671
0:00:56.302663
tensor(0.9066, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9066, device='cuda:0')
0:00:56.304647
0:00:56.321015
0:00:56.321015
0:00:56.434119
0:00:56.434615
0:00:57.201744
0:00:57.207213
0:00:57.694884
0:00:57.695876
tensor(1.0236, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0236, device='cuda:0')
0:00:57.697884
Testing!
[1/1, 1/8]
[1/1, 2/8]
[1/1, 3/8]
[1/1, 4/8]
[1/1, 5/8]
[1/1, 6/8]
[1/1, 7/8]
[1/1, 8/8]
Testing Loss: 1.0532517433166504
Training and Testing Finished
[tensor(14.2804, device='cuda:0'), tensor(8.1558, device='cuda:0'), tensor(5.3442, device='cuda:0'), tensor(3.3990, device='cuda:0'), tensor(2.4623, device='cuda:0'), tensor(1.9017, device='cuda:0'), tensor(1.5695, device='cuda:0'), tensor(1.3502, device='cuda:0'), tensor(1.2177, device='cuda:0'), tensor(1.1754, device='cuda:0'), tensor(1.1139, device='cuda:0'), tensor(1.0689, device='cuda:0'), tensor(1.0401, device='cuda:0'), tensor(1.0333, device='cuda:0'), tensor(1.0294, device='cuda:0')]
<class 'list'>
Assembling test data for t-sne projection
-- 1/8 --
-- 2/8 --
-- 3/8 --
-- 4/8 --
-- 5/8 --
-- 6/8 --
-- 7/8 --
-- 8/8 --
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation