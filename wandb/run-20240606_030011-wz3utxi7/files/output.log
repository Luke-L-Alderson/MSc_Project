Starting Sweep: Batch Size: 32, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 18000
Testing: 10000 -> 3000
Making Subsets
Training: 18000
Testing: 3000
Making Dataloaders
Defining network
2024-06-06 03:00:13.402588
Training!
0:00:00.008929
[1/9, 29/563] Training Loss: 8.805096626281738
[1/9, 58/563] Training Loss: 1.6792954206466675
[1/9, 87/563] Training Loss: 0.9764677286148071
[1/9, 116/563] Training Loss: 0.934244692325592
[1/9, 145/563] Training Loss: 0.9084039330482483
[1/9, 174/563] Training Loss: 0.8884062170982361
[1/9, 203/563] Training Loss: 0.884194016456604
[1/9, 232/563] Training Loss: 0.8758966326713562
[1/9, 261/563] Training Loss: 0.846434473991394
[1/9, 290/563] Training Loss: 0.8481103777885437
[1/9, 319/563] Training Loss: 0.8567253351211548
[1/9, 348/563] Training Loss: 0.8595340251922607
[1/9, 377/563] Training Loss: 0.8256229758262634
[1/9, 406/563] Training Loss: 0.8260849714279175
[1/9, 435/563] Training Loss: 0.8079137206077576
[1/9, 464/563] Training Loss: 0.8040643930435181
[1/9, 493/563] Training Loss: 0.7999528646469116
[1/9, 522/563] Training Loss: 0.7574009895324707
[1/9, 551/563] Training Loss: 0.7602030038833618
Testing!
[1/9, 1/94]
[1/9, 6/94]
[1/9, 11/94]
[1/9, 16/94]
[1/9, 21/94]
[1/9, 26/94]
[1/9, 31/94]
[1/9, 36/94]
[1/9, 41/94]
[1/9, 46/94]
[1/9, 51/94]
[1/9, 56/94]
[1/9, 61/94]
[1/9, 66/94]
[1/9, 71/94]
[1/9, 76/94]
[1/9, 81/94]
[1/9, 86/94]
[1/9, 91/94]
Testing Loss: 0.7542993426322937
Training!
0:12:00.769554
[2/9, 29/563] Training Loss: 0.7480895519256592
[2/9, 58/563] Training Loss: 0.7663527131080627
[2/9, 87/563] Training Loss: 0.7147689461708069
[2/9, 116/563] Training Loss: 0.7203013896942139
[2/9, 145/563] Training Loss: 0.6898055076599121
[2/9, 174/563] Training Loss: 0.676087498664856
[2/9, 203/563] Training Loss: 0.6609814167022705
[2/9, 232/563] Training Loss: 0.6539945006370544
[2/9, 261/563] Training Loss: 0.643868088722229
[2/9, 290/563] Training Loss: 0.6200143694877625
[2/9, 319/563] Training Loss: 0.5958938002586365
[2/9, 348/563] Training Loss: 0.6002261638641357
[2/9, 377/563] Training Loss: 0.5924312472343445
[2/9, 406/563] Training Loss: 0.5734246373176575
[2/9, 435/563] Training Loss: 0.5636935830116272
[2/9, 464/563] Training Loss: 0.5734723806381226
[2/9, 493/563] Training Loss: 0.5673670172691345
[2/9, 522/563] Training Loss: 0.5494493842124939
[2/9, 551/563] Training Loss: 0.5408996939659119
Testing!
[2/9, 1/94]
[2/9, 6/94]
[2/9, 11/94]
[2/9, 16/94]
[2/9, 21/94]
[2/9, 26/94]
[2/9, 31/94]
[2/9, 36/94]
[2/9, 41/94]
[2/9, 46/94]
[2/9, 51/94]
[2/9, 56/94]
[2/9, 61/94]
[2/9, 66/94]
[2/9, 71/94]
[2/9, 76/94]
[2/9, 81/94]
[2/9, 86/94]
[2/9, 91/94]
Testing Loss: 0.5374458432197571
Training!
0:24:00.099873
[3/9, 29/563] Training Loss: 0.5343011021614075
[3/9, 58/563] Training Loss: 0.5233039855957031
[3/9, 87/563] Training Loss: 0.5157113671302795
[3/9, 116/563] Training Loss: 0.5207935571670532
[3/9, 145/563] Training Loss: 0.4946249723434448
[3/9, 174/563] Training Loss: 0.49496957659721375
[3/9, 203/563] Training Loss: 0.49530526995658875
[3/9, 232/563] Training Loss: 0.5026810169219971
[3/9, 261/563] Training Loss: 0.4895460605621338
[3/9, 290/563] Training Loss: 0.48211875557899475
[3/9, 319/563] Training Loss: 0.4704586863517761
[3/9, 348/563] Training Loss: 0.4722173810005188
[3/9, 377/563] Training Loss: 0.46045202016830444
[3/9, 406/563] Training Loss: 0.45433899760246277
[3/9, 435/563] Training Loss: 0.44913822412490845
[3/9, 464/563] Training Loss: 0.4370553493499756
[3/9, 493/563] Training Loss: 0.4369862973690033
[3/9, 522/563] Training Loss: 0.4370572566986084
[3/9, 551/563] Training Loss: 0.42992308735847473
Testing!
[3/9, 1/94]
[3/9, 6/94]
[3/9, 11/94]
[3/9, 16/94]
[3/9, 21/94]
[3/9, 26/94]
[3/9, 31/94]
[3/9, 36/94]
[3/9, 41/94]
[3/9, 46/94]
[3/9, 51/94]
[3/9, 56/94]
[3/9, 61/94]
[3/9, 66/94]
[3/9, 71/94]
[3/9, 76/94]
[3/9, 81/94]
[3/9, 86/94]
[3/9, 91/94]
Testing Loss: 0.4208548665046692
Training!
0:35:59.501604
[4/9, 29/563] Training Loss: 0.4214705228805542
[4/9, 58/563] Training Loss: 0.41174638271331787
[4/9, 87/563] Training Loss: 0.4121148884296417
[4/9, 116/563] Training Loss: 0.40603727102279663
[4/9, 145/563] Training Loss: 0.3908003568649292
[4/9, 174/563] Training Loss: 0.3872845470905304
[4/9, 203/563] Training Loss: 0.39222684502601624
[4/9, 232/563] Training Loss: 0.3723541796207428
[4/9, 261/563] Training Loss: 0.3698877692222595
[4/9, 290/563] Training Loss: 0.3653128743171692
[4/9, 319/563] Training Loss: 0.3535057008266449
[4/9, 348/563] Training Loss: 0.3492650091648102
[4/9, 377/563] Training Loss: 0.34589993953704834
[4/9, 406/563] Training Loss: 0.3444738984107971
[4/9, 435/563] Training Loss: 0.3373456597328186
[4/9, 464/563] Training Loss: 0.33405718207359314
[4/9, 493/563] Training Loss: 0.33152642846107483
[4/9, 522/563] Training Loss: 0.32347795367240906
[4/9, 551/563] Training Loss: 0.32976964116096497
Testing!
[4/9, 1/94]
[4/9, 6/94]
[4/9, 11/94]
[4/9, 16/94]
[4/9, 21/94]
[4/9, 26/94]
[4/9, 31/94]
[4/9, 36/94]
[4/9, 41/94]
[4/9, 46/94]
[4/9, 51/94]
[4/9, 56/94]
[4/9, 61/94]
[4/9, 66/94]
[4/9, 71/94]
[4/9, 76/94]
[4/9, 81/94]
[4/9, 86/94]
[4/9, 91/94]
Testing Loss: 0.3132535219192505
Training!
0:48:00.623341
[5/9, 29/563] Training Loss: 0.320724755525589
[5/9, 58/563] Training Loss: 0.3225201368331909
[5/9, 87/563] Training Loss: 0.3097750246524811
[5/9, 116/563] Training Loss: 0.30777066946029663
[5/9, 145/563] Training Loss: 0.3086422085762024
[5/9, 174/563] Training Loss: 0.30307143926620483
[5/9, 203/563] Training Loss: 0.2979993522167206
[5/9, 232/563] Training Loss: 0.2976617217063904
[5/9, 261/563] Training Loss: 0.29214051365852356
[5/9, 290/563] Training Loss: 0.28638896346092224
[5/9, 319/563] Training Loss: 0.28845569491386414
[5/9, 348/563] Training Loss: 0.2878996729850769
[5/9, 377/563] Training Loss: 0.28011372685432434
[5/9, 406/563] Training Loss: 0.27854272723197937
[5/9, 435/563] Training Loss: 0.274772047996521
[5/9, 464/563] Training Loss: 0.2675076127052307
[5/9, 493/563] Training Loss: 0.2638377249240875
[5/9, 522/563] Training Loss: 0.2617260217666626
[5/9, 551/563] Training Loss: 0.2696434259414673
Testing!
[5/9, 1/94]
[5/9, 6/94]
[5/9, 11/94]
[5/9, 16/94]
[5/9, 21/94]
[5/9, 26/94]
[5/9, 31/94]
[5/9, 36/94]
[5/9, 41/94]
[5/9, 46/94]
[5/9, 51/94]
[5/9, 56/94]
[5/9, 61/94]
[5/9, 66/94]
[5/9, 71/94]
[5/9, 76/94]
[5/9, 81/94]
[5/9, 86/94]
[5/9, 91/94]
Testing Loss: 0.2593475878238678
Training!
1:00:00.001925
[6/9, 29/563] Training Loss: 0.260925829410553
[6/9, 58/563] Training Loss: 0.25721004605293274
[6/9, 87/563] Training Loss: 0.25452446937561035
[6/9, 116/563] Training Loss: 0.253653883934021
[6/9, 145/563] Training Loss: 0.24787981808185577
[6/9, 174/563] Training Loss: 0.2518090605735779
[6/9, 203/563] Training Loss: 0.24281634390354156
[6/9, 232/563] Training Loss: 0.24376192688941956
[6/9, 261/563] Training Loss: 0.23958720266819
[6/9, 290/563] Training Loss: 0.24585948884487152
[6/9, 319/563] Training Loss: 0.23937460780143738
[6/9, 348/563] Training Loss: 0.23318032920360565
[6/9, 377/563] Training Loss: 0.2374057173728943
[6/9, 406/563] Training Loss: 0.23691749572753906
[6/9, 435/563] Training Loss: 0.23460233211517334
[6/9, 464/563] Training Loss: 0.23376284539699554
[6/9, 493/563] Training Loss: 0.2295517772436142
[6/9, 522/563] Training Loss: 0.22548893094062805
[6/9, 551/563] Training Loss: 0.22471708059310913
Testing!
[6/9, 1/94]
[6/9, 6/94]
[6/9, 11/94]
[6/9, 16/94]
[6/9, 21/94]
[6/9, 26/94]
[6/9, 31/94]
[6/9, 36/94]
[6/9, 41/94]
[6/9, 46/94]
[6/9, 51/94]
[6/9, 56/94]
[6/9, 61/94]
[6/9, 66/94]
[6/9, 71/94]
[6/9, 76/94]
[6/9, 81/94]
[6/9, 86/94]
[6/9, 91/94]
Testing Loss: 0.2326768934726715
Training!
1:11:59.293029
[7/9, 29/563] Training Loss: 0.222190260887146
[7/9, 58/563] Training Loss: 0.22227197885513306
[7/9, 87/563] Training Loss: 0.22084805369377136
[7/9, 116/563] Training Loss: 0.22099384665489197
[7/9, 145/563] Training Loss: 0.22007215023040771
[7/9, 174/563] Training Loss: 0.21377426385879517
[7/9, 203/563] Training Loss: 0.21267397701740265
[7/9, 232/563] Training Loss: 0.21597613394260406
