Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Subsets
Training: 6000
Testing: 1000
(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2863, 0.7137, 0.9608,
          1.0000, 0.8275, 0.5255, 0.0588, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0745, 0.7686, 0.9765, 0.9961, 0.9961,
          0.9961, 0.9961, 0.9961, 0.6980, 0.0157, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.3725, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.9961, 0.9961, 0.4392, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.6157, 0.9961, 0.9961, 0.5020, 0.0980,
          0.3294, 0.9961, 0.9961, 0.9961, 0.2118, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0431, 0.8980, 0.9961, 0.6471, 0.0039, 0.0000,
          0.1216, 0.9647, 0.9961, 0.9020, 0.1059, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.4745, 0.9961, 0.9529, 0.1255, 0.0000, 0.1608,
          0.8627, 0.9961, 0.9804, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.1137, 0.9647, 0.9961, 0.6549, 0.0000, 0.3294, 0.9686,
          0.9961, 0.9961, 0.5020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2118, 0.9961, 0.9961, 0.4157, 0.4510, 0.9843, 0.9961,
          0.8824, 0.2314, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.4431, 0.9961, 0.9961, 0.1176, 0.8275, 0.9961, 0.9882,
          0.2941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.7725, 0.9961, 0.9961, 0.6667, 0.9961, 0.9765, 0.5255,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.5294, 0.9961, 0.9961, 0.9961, 0.9961, 0.5255, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2510, 0.9843, 0.9961, 0.9961, 0.8431, 0.1490, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.9294, 0.9961, 0.9961, 0.9961, 0.9804, 0.7176,
          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0431, 0.9412, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9569, 0.6000, 0.4118, 0.0745, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3922, 0.9961, 0.9765, 0.3098, 0.5176, 0.8902, 0.9961,
          0.9961, 0.9961, 0.9961, 0.6667, 0.0745, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2706, 0.9961, 0.7569, 0.0000, 0.0000, 0.1882, 0.4902,
          0.7333, 0.9961, 0.9961, 0.9961, 0.2118, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3216, 0.9961, 0.9843, 0.0667, 0.0000, 0.0000, 0.0000,
          0.0118, 0.4706, 0.9961, 0.9961, 0.2902, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3451, 0.9961, 0.9961, 0.5725, 0.0431, 0.0000, 0.0000,
          0.0000, 0.7529, 0.9961, 0.9765, 0.1569, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.2667, 0.9882, 0.9961, 0.9961, 0.8196, 0.6863, 0.4549,
          0.8157, 0.9922, 0.9961, 0.6941, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.6902, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.7255, 0.0824, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]]), 8)
<class 'tuple'>
2
Making Dataloaders
Defining network
2024-06-05 17:09:23.381213
Training!
0:00:00.013394
[1/3, 5/94] Training Loss: 3.1427485942840576
[1/3, 10/94] Training Loss: 1.8831655979156494
[1/3, 15/94] Training Loss: 1.1791819334030151
[1/3, 20/94] Training Loss: 0.8985859155654907
[1/3, 25/94] Training Loss: 0.9279503226280212
[1/3, 30/94] Training Loss: 0.9150304794311523
[1/3, 35/94] Training Loss: 0.8168613314628601
[1/3, 40/94] Training Loss: 0.8346655964851379
[1/3, 45/94] Training Loss: 0.813156247138977
[1/3, 50/94] Training Loss: 0.8018501400947571
[1/3, 55/94] Training Loss: 0.7905382513999939
[1/3, 60/94] Training Loss: 0.7853225469589233
[1/3, 65/94] Training Loss: 0.7949616312980652
[1/3, 70/94] Training Loss: 0.7672726511955261
[1/3, 75/94] Training Loss: 0.6973336338996887
[1/3, 80/94] Training Loss: 0.7819963693618774
[1/3, 85/94] Training Loss: 0.7629122138023376
[1/3, 90/94] Training Loss: 0.7533730864524841
Testing!
[1/3, 1/16]
[1/3, 2/16]
[1/3, 3/16]
[1/3, 4/16]
[1/3, 5/16]
[1/3, 6/16]
[1/3, 7/16]
[1/3, 8/16]
[1/3, 9/16]
[1/3, 10/16]
[1/3, 11/16]
[1/3, 12/16]
[1/3, 13/16]
[1/3, 14/16]
[1/3, 15/16]
[1/3, 16/16]
Testing Loss: 0.7737640738487244
Training!
0:02:04.128429
[2/3, 5/94] Training Loss: 0.7452951073646545
[2/3, 10/94] Training Loss: 0.6988624334335327
[2/3, 15/94] Training Loss: 0.7031943798065186
[2/3, 20/94] Training Loss: 0.7256463170051575
[2/3, 25/94] Training Loss: 0.6967324614524841
[2/3, 30/94] Training Loss: 0.7645850777626038
[2/3, 35/94] Training Loss: 0.718357264995575
[2/3, 40/94] Training Loss: 0.7153814435005188
[2/3, 45/94] Training Loss: 0.7104706168174744
[2/3, 50/94] Training Loss: 0.7149938941001892
[2/3, 55/94] Training Loss: 0.6594891548156738
[2/3, 60/94] Training Loss: 0.7158674001693726
[2/3, 65/94] Training Loss: 0.7050331234931946
[2/3, 70/94] Training Loss: 0.6716251373291016
[2/3, 75/94] Training Loss: 0.6886205673217773
[2/3, 80/94] Training Loss: 0.6813539862632751
[2/3, 85/94] Training Loss: 0.6960781216621399
[2/3, 90/94] Training Loss: 0.6588754057884216
Testing!
[2/3, 1/16]
[2/3, 2/16]
[2/3, 3/16]
[2/3, 4/16]
[2/3, 5/16]
[2/3, 6/16]
[2/3, 7/16]
[2/3, 8/16]
[2/3, 9/16]
[2/3, 10/16]
[2/3, 11/16]
[2/3, 12/16]
[2/3, 13/16]
[2/3, 14/16]
[2/3, 15/16]
[2/3, 16/16]
Testing Loss: 0.7274836897850037
Training!
0:04:08.171812
[3/3, 5/94] Training Loss: 0.6456409692764282
[3/3, 10/94] Training Loss: 0.6626603007316589
[3/3, 15/94] Training Loss: 0.6438416242599487
[3/3, 20/94] Training Loss: 0.6651531457901001
[3/3, 25/94] Training Loss: 0.6471837162971497
[3/3, 30/94] Training Loss: 0.6531199812889099
[3/3, 35/94] Training Loss: 0.6160299181938171
[3/3, 40/94] Training Loss: 0.6303961277008057
[3/3, 45/94] Training Loss: 0.6385948657989502
[3/3, 50/94] Training Loss: 0.6404220461845398
[3/3, 55/94] Training Loss: 0.6452314853668213
[3/3, 60/94] Training Loss: 0.6302971243858337
[3/3, 65/94] Training Loss: 0.5939790606498718
[3/3, 70/94] Training Loss: 0.6324722766876221
[3/3, 75/94] Training Loss: 0.593708336353302
[3/3, 80/94] Training Loss: 0.6066337823867798
[3/3, 85/94] Training Loss: 0.6011655926704407
[3/3, 90/94] Training Loss: 0.5936245322227478
Testing!
[3/3, 1/16]
[3/3, 2/16]
[3/3, 3/16]
[3/3, 4/16]
[3/3, 5/16]
[3/3, 6/16]
[3/3, 7/16]
[3/3, 8/16]
[3/3, 9/16]
[3/3, 10/16]
[3/3, 11/16]
[3/3, 12/16]
[3/3, 13/16]
[3/3, 14/16]
[3/3, 15/16]
[3/3, 16/16]
Testing Loss: 0.5647491216659546
Training and Testing Finished
0.5647491216659546
<class 'float'>
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation