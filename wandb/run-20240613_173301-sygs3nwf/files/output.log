Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-13 17:33:02.219360
Training - 2024-06-13 17:33:02.219857
[1/9, 10/94] Training Loss: 5.19 - Iteration Time: 0:00:01.265596
[1/9, 20/94] Training Loss: 5.05 - Iteration Time: 0:00:01.263176
[1/9, 30/94] Training Loss: 5.04 - Iteration Time: 0:00:01.266591
[1/9, 40/94] Training Loss: 5.04 - Iteration Time: 0:00:01.259005
[1/9, 50/94] Training Loss: 5.01 - Iteration Time: 0:00:01.296805
[1/9, 60/94] Training Loss: 4.94 - Iteration Time: 0:00:01.260159
[1/9, 70/94] Training Loss: 4.87 - Iteration Time: 0:00:01.257690
[1/9, 80/94] Training Loss: 4.89 - Iteration Time: 0:00:01.283934
[1/9, 90/94] Training Loss: 4.83 - Iteration Time: 0:00:01.257257
Testing - 2024-06-13 17:35:05.686622
[1/9, 10/16]
Testing Loss: 4.65 - Epoch Time: 0:02:15.390344
Training - 2024-06-13 17:35:17.611192
[2/9, 10/94] Training Loss: 4.83 - Iteration Time: 0:00:01.285457
[2/9, 20/94] Training Loss: 4.69 - Iteration Time: 0:00:01.272021
[2/9, 30/94] Training Loss: 4.69 - Iteration Time: 0:00:01.273520
[2/9, 40/94] Training Loss: 4.65 - Iteration Time: 0:00:01.281519
[2/9, 50/94] Training Loss: 4.58 - Iteration Time: 0:00:01.282939
[2/9, 60/94] Training Loss: 4.60 - Iteration Time: 0:00:01.262106
[2/9, 70/94] Training Loss: 4.51 - Iteration Time: 0:00:01.285960
[2/9, 80/94] Training Loss: 4.44 - Iteration Time: 0:00:01.347986
[2/9, 90/94] Training Loss: 4.46 - Iteration Time: 0:00:01.259252
Testing - 2024-06-13 17:37:20.605109
[2/9, 10/16]
Testing Loss: 4.30 - Epoch Time: 0:02:14.870973
Training - 2024-06-13 17:37:32.482661
[3/9, 10/94] Training Loss: 4.39 - Iteration Time: 0:00:01.248728
[3/9, 20/94] Training Loss: 4.41 - Iteration Time: 0:00:01.295889
[3/9, 30/94] Training Loss: 4.32 - Iteration Time: 0:00:01.269031
[3/9, 40/94] Training Loss: 4.31 - Iteration Time: 0:00:01.269074
[3/9, 50/94] Training Loss: 4.29 - Iteration Time: 0:00:01.251692
[3/9, 60/94] Training Loss: 4.24 - Iteration Time: 0:00:01.347447
[3/9, 70/94] Training Loss: 4.16 - Iteration Time: 0:00:01.278469
[3/9, 80/94] Training Loss: 4.17 - Iteration Time: 0:00:01.278060
[3/9, 90/94] Training Loss: 4.15 - Iteration Time: 0:00:01.276485
Testing - 2024-06-13 17:39:35.719440
[3/9, 10/16]
Testing Loss: 3.97 - Epoch Time: 0:02:15.131099
Training - 2024-06-13 17:39:47.614256
[4/9, 10/94] Training Loss: 4.04 - Iteration Time: 0:00:01.357376
[4/9, 20/94] Training Loss: 4.07 - Iteration Time: 0:00:01.280935
[4/9, 30/94] Training Loss: 4.01 - Iteration Time: 0:00:01.279985
[4/9, 40/94] Training Loss: 3.95 - Iteration Time: 0:00:01.281432
[4/9, 50/94] Training Loss: 3.94 - Iteration Time: 0:00:01.268565
[4/9, 60/94] Training Loss: 3.89 - Iteration Time: 0:00:01.301802
[4/9, 70/94] Training Loss: 3.91 - Iteration Time: 0:00:01.275556
[4/9, 80/94] Training Loss: 3.88 - Iteration Time: 0:00:01.295845
[4/9, 90/94] Training Loss: 3.82 - Iteration Time: 0:00:01.275060
Testing - 2024-06-13 17:41:50.865843
[4/9, 10/16]
Testing Loss: 3.66 - Epoch Time: 0:02:15.318547
Training - 2024-06-13 17:42:02.933807
[5/9, 10/94] Training Loss: 3.81 - Iteration Time: 0:00:01.313752
[5/9, 20/94] Training Loss: 3.81 - Iteration Time: 0:00:01.256689
[5/9, 30/94] Training Loss: 3.74 - Iteration Time: 0:00:01.277479
[5/9, 40/94] Training Loss: 3.73 - Iteration Time: 0:00:01.272106
[5/9, 50/94] Training Loss: 3.72 - Iteration Time: 0:00:01.306138
[5/9, 60/94] Training Loss: 3.71 - Iteration Time: 0:00:01.267625
[5/9, 70/94] Training Loss: 3.68 - Iteration Time: 0:00:01.269052
[5/9, 80/94] Training Loss: 3.59 - Iteration Time: 0:00:01.265107
[5/9, 90/94] Training Loss: 3.57 - Iteration Time: 0:00:01.322145
Testing - 2024-06-13 17:44:06.272222
[5/9, 10/16]
Testing Loss: 3.44 - Epoch Time: 0:02:15.192124
Training - 2024-06-13 17:44:18.126427
[6/9, 10/94] Training Loss: 3.55 - Iteration Time: 0:00:01.322156
[6/9, 20/94] Training Loss: 3.49 - Iteration Time: 0:00:01.291962
[6/9, 30/94] Training Loss: 3.47 - Iteration Time: 0:00:01.283158
[6/9, 40/94] Training Loss: 3.47 - Iteration Time: 0:00:01.251725
[6/9, 50/94] Training Loss: 3.43 - Iteration Time: 0:00:01.293989
[6/9, 60/94] Training Loss: 3.41 - Iteration Time: 0:00:01.259173
[6/9, 70/94] Training Loss: 3.40 - Iteration Time: 0:00:01.318709
[6/9, 80/94] Training Loss: 3.38 - Iteration Time: 0:00:01.251659
[6/9, 90/94] Training Loss: 3.37 - Iteration Time: 0:00:01.264636
Testing - 2024-06-13 17:46:22.720379
[6/9, 10/16]
Testing Loss: 3.23 - Epoch Time: 0:02:16.386642
Training - 2024-06-13 17:46:34.513069
[7/9, 10/94] Training Loss: 3.31 - Iteration Time: 0:00:01.256673
[7/9, 20/94] Training Loss: 3.28 - Iteration Time: 0:00:01.301328
[7/9, 30/94] Training Loss: 3.26 - Iteration Time: 0:00:01.275063
[7/9, 40/94] Training Loss: 3.23 - Iteration Time: 0:00:01.247764
[7/9, 50/94] Training Loss: 3.21 - Iteration Time: 0:00:01.262182
[7/9, 60/94] Training Loss: 3.13 - Iteration Time: 0:00:01.254688
[7/9, 70/94] Training Loss: 3.17 - Iteration Time: 0:00:01.314743
[7/9, 80/94] Training Loss: 3.15 - Iteration Time: 0:00:01.265733
[7/9, 90/94] Training Loss: 3.15 - Iteration Time: 0:00:01.247749
Testing - 2024-06-13 17:48:37.356288
[7/9, 10/16]
Testing Loss: 3.03 - Epoch Time: 0:02:14.777528
Training - 2024-06-13 17:48:49.291092
[8/9, 10/94] Training Loss: 3.13 - Iteration Time: 0:00:01.367286
[8/9, 20/94] Training Loss: 3.08 - Iteration Time: 0:00:01.278609
[8/9, 30/94] Training Loss: 3.06 - Iteration Time: 0:00:01.290418
[8/9, 40/94] Training Loss: 3.04 - Iteration Time: 0:00:01.293863
[8/9, 50/94] Training Loss: 3.03 - Iteration Time: 0:00:01.448235
[8/9, 60/94] Training Loss: 3.02 - Iteration Time: 0:00:01.270188
[8/9, 70/94] Training Loss: 2.99 - Iteration Time: 0:00:01.283476
[8/9, 80/94] Training Loss: 2.97 - Iteration Time: 0:00:01.273023
[8/9, 90/94] Training Loss: 2.95 - Iteration Time: 0:00:01.251780
Testing - 2024-06-13 17:50:52.503712
[8/9, 10/16]
Testing Loss: 2.86 - Epoch Time: 0:02:15.294828
Training - 2024-06-13 17:51:04.586416
[9/9, 10/94] Training Loss: 2.92 - Iteration Time: 0:00:01.301825
[9/9, 20/94] Training Loss: 2.92 - Iteration Time: 0:00:01.296393
[9/9, 30/94] Training Loss: 2.91 - Iteration Time: 0:00:01.271044
[9/9, 40/94] Training Loss: 2.87 - Iteration Time: 0:00:01.344961
[9/9, 50/94] Training Loss: 2.86 - Iteration Time: 0:00:01.265166
[9/9, 60/94] Training Loss: 2.88 - Iteration Time: 0:00:01.269095
[9/9, 70/94] Training Loss: 2.85 - Iteration Time: 0:00:01.246291
[9/9, 80/94] Training Loss: 2.82 - Iteration Time: 0:00:01.321664
[9/9, 90/94] Training Loss: 2.87 - Iteration Time: 0:00:01.490962
Testing - 2024-06-13 17:53:08.438590
[9/9, 10/16]
Testing Loss: 2.79 - Epoch Time: 0:02:15.891392
Training and Testing Finished - Time: 0:20:18.258944
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
WARNING    c:\users\lukea\documents\masters project code\msc_project\main.py:185: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
 [py.warnings]
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2