Starting Sweep: Batch Size: 64, Learning Rate: 1e-06
Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/1, 10/938] Training Loss: 1.084422767162323
[1/1, 20/938] Training Loss: 1.0801681399345398
[1/1, 30/938] Training Loss: 1.0977174043655396
[1/1, 40/938] Training Loss: 1.0953465282917023
[1/1, 50/938] Training Loss: 1.060717636346817
[1/1, 60/938] Training Loss: 1.1068921089172363
[1/1, 70/938] Training Loss: 1.0670218467712402
[1/1, 80/938] Training Loss: 1.094805896282196
[1/1, 90/938] Training Loss: 1.0893686234951019
[1/1, 100/938] Training Loss: 1.0970781981945037
[1/1, 110/938] Training Loss: 1.0740627825260163
[1/1, 120/938] Training Loss: 1.0652427673339844
[1/1, 130/938] Training Loss: 1.0989171802997588
[1/1, 140/938] Training Loss: 1.089799702167511
[1/1, 150/938] Training Loss: 1.0892456412315368
[1/1, 160/938] Training Loss: 1.0530792355537415
[1/1, 170/938] Training Loss: 1.0431915819644928
[1/1, 180/938] Training Loss: 1.095173180103302
[1/1, 190/938] Training Loss: 1.0458315908908844
[1/1, 200/938] Training Loss: 1.0987226903438567
[1/1, 210/938] Training Loss: 1.0925139725208282
[1/1, 220/938] Training Loss: 1.0479674994945527
[1/1, 230/938] Training Loss: 1.02191521525383
[1/1, 240/938] Training Loss: 1.0416236758232116
[1/1, 250/938] Training Loss: 1.0461624562740326
[1/1, 260/938] Training Loss: 1.053118097782135
[1/1, 270/938] Training Loss: 1.0412712395191193
[1/1, 280/938] Training Loss: 1.0626522719860076
[1/1, 290/938] Training Loss: 1.075659304857254
[1/1, 300/938] Training Loss: 1.0549240827560424
[1/1, 310/938] Training Loss: 1.0567225575447083
[1/1, 320/938] Training Loss: 1.044603741168976
[1/1, 330/938] Training Loss: 1.0239856839179993
[1/1, 340/938] Training Loss: 1.0550928831100463
[1/1, 350/938] Training Loss: 1.0328911423683167
[1/1, 360/938] Training Loss: 1.0390666127204895
[1/1, 370/938] Training Loss: 1.038323324918747
[1/1, 380/938] Training Loss: 1.0478274941444397
[1/1, 390/938] Training Loss: 1.0476613283157348
[1/1, 400/938] Training Loss: 1.0254777252674103
[1/1, 410/938] Training Loss: 1.0539399921894073
[1/1, 420/938] Training Loss: 1.0210663735866548
[1/1, 430/938] Training Loss: 1.0501788854599
[1/1, 440/938] Training Loss: 1.014238852262497
[1/1, 450/938] Training Loss: 0.9974534928798675
[1/1, 460/938] Training Loss: 1.026696264743805
[1/1, 470/938] Training Loss: 1.0106254458427428
[1/1, 480/938] Training Loss: 1.0243943691253663
[1/1, 490/938] Training Loss: 1.0110654771327972
[1/1, 500/938] Training Loss: 1.0081512987613679
[1/1, 510/938] Training Loss: 0.9962409973144531
[1/1, 520/938] Training Loss: 0.9773264467716217
[1/1, 530/938] Training Loss: 0.9914894342422486
[1/1, 540/938] Training Loss: 1.0159511506557464
[1/1, 550/938] Training Loss: 0.991553795337677
[1/1, 560/938] Training Loss: 1.0255989670753478
[1/1, 570/938] Training Loss: 1.0224226057529449
[1/1, 580/938] Training Loss: 0.9896390259265899
[1/1, 590/938] Training Loss: 1.0317526817321778
[1/1, 600/938] Training Loss: 1.0210183560848236
[1/1, 610/938] Training Loss: 1.01695716381073
[1/1, 620/938] Training Loss: 1.0218556940555572
[1/1, 630/938] Training Loss: 1.020936018228531
[1/1, 640/938] Training Loss: 1.009559839963913
[1/1, 650/938] Training Loss: 1.0076006412506104
[1/1, 660/938] Training Loss: 1.0076068818569184
[1/1, 670/938] Training Loss: 0.9936926007270813
[1/1, 680/938] Training Loss: 0.989015519618988
[1/1, 690/938] Training Loss: 0.9484067618846893
[1/1, 700/938] Training Loss: 0.9988902866840362
[1/1, 710/938] Training Loss: 0.9782715857028961
[1/1, 720/938] Training Loss: 0.9960383594036102
[1/1, 730/938] Training Loss: 0.9593489170074463
[1/1, 740/938] Training Loss: 0.9923757672309875
[1/1, 750/938] Training Loss: 0.9626970052719116
[1/1, 760/938] Training Loss: 0.9934888362884522
[1/1, 770/938] Training Loss: 0.9955459833145142
[1/1, 780/938] Training Loss: 1.023223066329956
[1/1, 790/938] Training Loss: 0.9744071066379547
[1/1, 800/938] Training Loss: 0.9877555668354034
[1/1, 810/938] Training Loss: 1.0321820974349976
[1/1, 820/938] Training Loss: 0.9811788380146027
[1/1, 830/938] Training Loss: 0.9945111513137818
[1/1, 840/938] Training Loss: 0.9825966894626618
[1/1, 850/938] Training Loss: 0.9787246346473694
[1/1, 860/938] Training Loss: 1.0135046124458313
[1/1, 870/938] Training Loss: 0.9356441795825958
[1/1, 880/938] Training Loss: 0.9921611309051513
[1/1, 890/938] Training Loss: 0.9762457191944123
[1/1, 900/938] Training Loss: 0.9989514529705048
[1/1, 910/938] Training Loss: 0.9857947885990143
[1/1, 920/938] Training Loss: 0.9579759001731872
[1/1, 930/938] Training Loss: 0.9454447448253631
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.9877581508954366
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
Applying t-SNE
7 added
0 added
4 added
2 added
3 added
5 added
1 added
6 added
9 added
8 added
[34m[1mwandb[39m[22m: Network error (ConnectionError), entering retry loop.