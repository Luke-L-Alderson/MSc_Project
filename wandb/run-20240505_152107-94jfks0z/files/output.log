Making Datasets...
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Using downloaded and verified file: dataset/MNIST\raw\train-images-idx3-ubyte.gz
Extracting dataset/MNIST\raw\train-images-idx3-ubyte.gz to dataset/MNIST\raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Using downloaded and verified file: dataset/MNIST\raw\train-labels-idx1-ubyte.gz
Extracting dataset/MNIST\raw\train-labels-idx1-ubyte.gz to dataset/MNIST\raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Using downloaded and verified file: dataset/MNIST\raw\t10k-images-idx3-ubyte.gz
Extracting dataset/MNIST\raw\t10k-images-idx3-ubyte.gz to dataset/MNIST\raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Using downloaded and verified file: dataset/MNIST\raw\t10k-labels-idx1-ubyte.gz
Extracting dataset/MNIST\raw\t10k-labels-idx1-ubyte.gz to dataset/MNIST\raw
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/938] Training Loss: 1.084422767162323
[1/3, 20/938] Training Loss: 1.0801681399345398
[1/3, 30/938] Training Loss: 1.0977174043655396
[1/3, 40/938] Training Loss: 1.0953465282917023
[1/3, 50/938] Training Loss: 1.060717636346817
[1/3, 60/938] Training Loss: 1.1068921089172363
[1/3, 70/938] Training Loss: 1.0670218467712402
[1/3, 80/938] Training Loss: 1.094805896282196
[1/3, 90/938] Training Loss: 1.0893686234951019
[1/3, 100/938] Training Loss: 1.0970781981945037
[1/3, 110/938] Training Loss: 1.0740627825260163
[1/3, 120/938] Training Loss: 1.0652427673339844
[1/3, 130/938] Training Loss: 1.0989171802997588
[1/3, 140/938] Training Loss: 1.089799702167511
[1/3, 150/938] Training Loss: 1.0892456412315368
[1/3, 160/938] Training Loss: 1.0530792355537415
[1/3, 170/938] Training Loss: 1.0431915819644928
[1/3, 180/938] Training Loss: 1.095173180103302
[1/3, 190/938] Training Loss: 1.0458315908908844
[1/3, 200/938] Training Loss: 1.0987226903438567
[1/3, 210/938] Training Loss: 1.0925139725208282
[1/3, 220/938] Training Loss: 1.0479674994945527
[1/3, 230/938] Training Loss: 1.02191521525383
[1/3, 240/938] Training Loss: 1.0416236758232116
[1/3, 250/938] Training Loss: 1.0461624562740326
[1/3, 260/938] Training Loss: 1.053118097782135
[1/3, 270/938] Training Loss: 1.0412712395191193
[1/3, 280/938] Training Loss: 1.0626522719860076
[1/3, 290/938] Training Loss: 1.075659304857254
[1/3, 300/938] Training Loss: 1.0549240827560424
[1/3, 310/938] Training Loss: 1.0567225575447083
[1/3, 320/938] Training Loss: 1.044603741168976
[1/3, 330/938] Training Loss: 1.0239856839179993
[1/3, 340/938] Training Loss: 1.0550928831100463
[1/3, 350/938] Training Loss: 1.0328911423683167
[1/3, 360/938] Training Loss: 1.0390666127204895
[1/3, 370/938] Training Loss: 1.038323324918747
[1/3, 380/938] Training Loss: 1.0478274941444397
[1/3, 390/938] Training Loss: 1.0476613283157348
[1/3, 400/938] Training Loss: 1.0254777252674103
[1/3, 410/938] Training Loss: 1.0539399921894073
[1/3, 420/938] Training Loss: 1.0210663735866548
[1/3, 430/938] Training Loss: 1.0501788854599
[1/3, 440/938] Training Loss: 1.014238852262497
[1/3, 450/938] Training Loss: 0.9974534928798675
[1/3, 460/938] Training Loss: 1.026696264743805
[1/3, 470/938] Training Loss: 1.0106254458427428
[1/3, 480/938] Training Loss: 1.0243943691253663
[1/3, 490/938] Training Loss: 1.0110654771327972
[1/3, 500/938] Training Loss: 1.0081512987613679
[1/3, 510/938] Training Loss: 0.9962409973144531
[1/3, 520/938] Training Loss: 0.9773264467716217
[1/3, 530/938] Training Loss: 0.9914894342422486
[1/3, 540/938] Training Loss: 1.0159511506557464
[1/3, 550/938] Training Loss: 0.991553795337677
[1/3, 560/938] Training Loss: 1.0255989670753478
[1/3, 570/938] Training Loss: 1.0224226057529449
[1/3, 580/938] Training Loss: 0.9896390259265899
[1/3, 590/938] Training Loss: 1.0317526817321778
[1/3, 600/938] Training Loss: 1.0210183560848236
[1/3, 610/938] Training Loss: 1.01695716381073
[1/3, 620/938] Training Loss: 1.0218556940555572
[1/3, 630/938] Training Loss: 1.020936018228531
[1/3, 640/938] Training Loss: 1.009559839963913
[1/3, 650/938] Training Loss: 1.0076006412506104
[1/3, 660/938] Training Loss: 1.0076068818569184
[1/3, 670/938] Training Loss: 0.9936926007270813
[1/3, 680/938] Training Loss: 0.989015519618988
[1/3, 690/938] Training Loss: 0.9484067618846893
[1/3, 700/938] Training Loss: 0.9988902866840362
[1/3, 710/938] Training Loss: 0.9782715857028961
[1/3, 720/938] Training Loss: 0.9960383594036102
[1/3, 730/938] Training Loss: 0.9593489170074463
[1/3, 740/938] Training Loss: 0.9923757672309875
[1/3, 750/938] Training Loss: 0.9626970052719116
[1/3, 760/938] Training Loss: 0.9934888362884522
[1/3, 770/938] Training Loss: 0.9955459833145142
[1/3, 780/938] Training Loss: 1.023223066329956
[1/3, 790/938] Training Loss: 0.9744071066379547
[1/3, 800/938] Training Loss: 0.9877555668354034
[1/3, 810/938] Training Loss: 1.0321820974349976
[1/3, 820/938] Training Loss: 0.9811788380146027
[1/3, 830/938] Training Loss: 0.9945111513137818
[1/3, 840/938] Training Loss: 0.9825966894626618
[1/3, 850/938] Training Loss: 0.9787246346473694
[1/3, 860/938] Training Loss: 1.0135046124458313
[1/3, 870/938] Training Loss: 0.9356441795825958
[1/3, 880/938] Training Loss: 0.9921611309051513
[1/3, 890/938] Training Loss: 0.9762457191944123
[1/3, 900/938] Training Loss: 0.9989514529705048
[1/3, 910/938] Training Loss: 0.9857947885990143
[1/3, 920/938] Training Loss: 0.9579759001731872
[1/3, 930/938] Training Loss: 0.9454447448253631
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.9877581508954366
5/5: Training network...
[2/3, 10/938] Training Loss: 0.9892654061317444
[2/3, 20/938] Training Loss: 0.9807011127471924
[2/3, 30/938] Training Loss: 0.9616461277008057
[2/3, 40/938] Training Loss: 0.9766135156154633
[2/3, 50/938] Training Loss: 0.9796532452106476
[2/3, 60/938] Training Loss: 0.9699125170707703
[2/3, 70/938] Training Loss: 0.9492793917655945
[2/3, 80/938] Training Loss: 0.9450232028961182
[2/3, 90/938] Training Loss: 0.9380151510238648
[2/3, 100/938] Training Loss: 0.9398935079574585
[2/3, 110/938] Training Loss: 0.9304274022579193
[2/3, 120/938] Training Loss: 0.9703705966472626
[2/3, 130/938] Training Loss: 0.9844881534576416
[2/3, 140/938] Training Loss: 0.9374398946762085
[2/3, 150/938] Training Loss: 0.9526552081108093
[2/3, 160/938] Training Loss: 0.9445927679538727
[2/3, 170/938] Training Loss: 0.9602189779281616
[2/3, 180/938] Training Loss: 0.9537345886230468
[2/3, 190/938] Training Loss: 0.9630352020263672
[2/3, 200/938] Training Loss: 0.963960486650467
[2/3, 210/938] Training Loss: 0.9648346900939941
[2/3, 220/938] Training Loss: 0.9403497695922851
[2/3, 230/938] Training Loss: 0.9130091607570648
[2/3, 240/938] Training Loss: 0.955149257183075
[2/3, 250/938] Training Loss: 0.9337541937828064
[2/3, 260/938] Training Loss: 0.9516128420829773
[2/3, 270/938] Training Loss: 0.9454795062541962
[2/3, 280/938] Training Loss: 0.9354512214660644
[2/3, 290/938] Training Loss: 0.9299689531326294
[2/3, 300/938] Training Loss: 0.9359733581542968
[2/3, 310/938] Training Loss: 0.949663645029068
[2/3, 320/938] Training Loss: 0.9488764464855194
[2/3, 330/938] Training Loss: 0.9177774369716645
[2/3, 340/938] Training Loss: 0.936667400598526
[2/3, 350/938] Training Loss: 0.914891266822815
[2/3, 360/938] Training Loss: 0.9422598659992218
[2/3, 370/938] Training Loss: 0.9468740165233612
[2/3, 380/938] Training Loss: 0.9429617106914521
[2/3, 390/938] Training Loss: 0.9379774510860444
[2/3, 400/938] Training Loss: 0.9373967349529266
[2/3, 410/938] Training Loss: 0.9422188937664032
[2/3, 420/938] Training Loss: 0.910287469625473
[2/3, 430/938] Training Loss: 0.9399364173412323
[2/3, 440/938] Training Loss: 0.9286123216152191
[2/3, 450/938] Training Loss: 0.8840771317481995
[2/3, 460/938] Training Loss: 0.8897052884101868
[2/3, 470/938] Training Loss: 0.9287443876266479
[2/3, 480/938] Training Loss: 0.9423119843006134
[2/3, 490/938] Training Loss: 0.9147344350814819
[2/3, 500/938] Training Loss: 0.9321028828620911
[2/3, 510/938] Training Loss: 0.9406883776187897
[2/3, 520/938] Training Loss: 0.9231460213661193
[2/3, 530/938] Training Loss: 0.9216495335102082
[2/3, 540/938] Training Loss: 0.9358158648014069
[2/3, 550/938] Training Loss: 0.9287020623683929
[2/3, 560/938] Training Loss: 0.901053273677826
[2/3, 570/938] Training Loss: 0.9579446732997894
[2/3, 580/938] Training Loss: 0.9230769097805023
[2/3, 590/938] Training Loss: 0.9507471203804017
[2/3, 600/938] Training Loss: 0.919715803861618
[2/3, 610/938] Training Loss: 0.8977548539638519
[2/3, 620/938] Training Loss: 0.9123204827308655
[2/3, 630/938] Training Loss: 0.9316655099391937
[2/3, 640/938] Training Loss: 0.918084317445755
[2/3, 650/938] Training Loss: 0.9092922806739807
[2/3, 660/938] Training Loss: 0.9321929037570953
[2/3, 670/938] Training Loss: 0.9225394368171692
[2/3, 680/938] Training Loss: 0.930855107307434
[2/3, 690/938] Training Loss: 0.8701108157634735
[2/3, 700/938] Training Loss: 0.9281436920166015
[2/3, 710/938] Training Loss: 0.9139626383781433
[2/3, 720/938] Training Loss: 0.9287458956241608
[2/3, 730/938] Training Loss: 0.9347804188728333
[2/3, 740/938] Training Loss: 0.9350368678569794
[2/3, 750/938] Training Loss: 0.9094561338424683
[2/3, 760/938] Training Loss: 0.9196713209152222
[2/3, 770/938] Training Loss: 0.9159033000469208
[2/3, 780/938] Training Loss: 0.9297319173812866
[2/3, 790/938] Training Loss: 0.9113694071769715
[2/3, 800/938] Training Loss: 0.9041702508926391
[2/3, 810/938] Training Loss: 0.912406188249588
[2/3, 820/938] Training Loss: 0.8777493119239808
[2/3, 830/938] Training Loss: 0.9117340207099914
[2/3, 840/938] Training Loss: 0.9093887686729432
[2/3, 850/938] Training Loss: 0.9169301152229309
[2/3, 860/938] Training Loss: 0.9197902202606201
[2/3, 870/938] Training Loss: 0.8846733033657074
[2/3, 880/938] Training Loss: 0.8934021294116974
[2/3, 890/938] Training Loss: 0.8965348243713379
[2/3, 900/938] Training Loss: 0.9120971262454987
[2/3, 910/938] Training Loss: 0.9088241040706635
[2/3, 920/938] Training Loss: 0.9120575726032257
[2/3, 930/938] Training Loss: 0.9199921071529389
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.9465340052048367
5/5: Training network...
[3/3, 10/938] Training Loss: 0.9022707283496857
[3/3, 20/938] Training Loss: 0.9107160091400146
[3/3, 30/938] Training Loss: 0.8861193358898163
[3/3, 40/938] Training Loss: 0.9073579609394073
[3/3, 50/938] Training Loss: 0.8843096554279327
[3/3, 60/938] Training Loss: 0.8822607517242431
[3/3, 70/938] Training Loss: 0.8946996033191681
[3/3, 80/938] Training Loss: 0.8914520263671875
[3/3, 90/938] Training Loss: 0.883189219236374
[3/3, 100/938] Training Loss: 0.9144013047218322
[3/3, 110/938] Training Loss: 0.8925281047821045
[3/3, 120/938] Training Loss: 0.8862536489963532
[3/3, 130/938] Training Loss: 0.8819202244281769
[3/3, 140/938] Training Loss: 0.8997483253479004
[3/3, 150/938] Training Loss: 0.8873841226100921
[3/3, 160/938] Training Loss: 0.8595334649085998
[3/3, 170/938] Training Loss: 0.8816751897335052
[3/3, 180/938] Training Loss: 0.8865422368049621
[3/3, 190/938] Training Loss: 0.8959374487400055
[3/3, 200/938] Training Loss: 0.8921627938747406
[3/3, 210/938] Training Loss: 0.8859666347503662
[3/3, 220/938] Training Loss: 0.8508228540420533
[3/3, 230/938] Training Loss: 0.8932020843029023
[3/3, 240/938] Training Loss: 0.8784321844577789
[3/3, 250/938] Training Loss: 0.8754431545734406
[3/3, 260/938] Training Loss: 0.8750647306442261
[3/3, 270/938] Training Loss: 0.8847561836242676
[3/3, 280/938] Training Loss: 0.8882951259613037
[3/3, 290/938] Training Loss: 0.8573807418346405
[3/3, 300/938] Training Loss: 0.9138771057128906
[3/3, 310/938] Training Loss: 0.8839627087116242
[3/3, 320/938] Training Loss: 0.8503959715366364
[3/3, 330/938] Training Loss: 0.8798425972461701
[3/3, 340/938] Training Loss: 0.8649796307086944
[3/3, 350/938] Training Loss: 0.8649802207946777
[3/3, 360/938] Training Loss: 0.8425156891345977
[3/3, 370/938] Training Loss: 0.8564603269100189
[3/3, 380/938] Training Loss: 0.8920533776283264
[3/3, 390/938] Training Loss: 0.8661963641643524
[3/3, 400/938] Training Loss: 0.8724271178245544
[3/3, 410/938] Training Loss: 0.8874444186687469
[3/3, 420/938] Training Loss: 0.8499385952949524
[3/3, 430/938] Training Loss: 0.8671687245368958
[3/3, 440/938] Training Loss: 0.9033551752567291
[3/3, 450/938] Training Loss: 0.8321777045726776
[3/3, 460/938] Training Loss: 0.869370824098587
[3/3, 470/938] Training Loss: 0.892493587732315
[3/3, 480/938] Training Loss: 0.8747755587100983
[3/3, 490/938] Training Loss: 0.8586297273635864
[3/3, 500/938] Training Loss: 0.8810367286205292
[3/3, 510/938] Training Loss: 0.8791858732700348
[3/3, 520/938] Training Loss: 0.8584451794624328
[3/3, 530/938] Training Loss: 0.9153933882713318
[3/3, 540/938] Training Loss: 0.8764444530010224
[3/3, 550/938] Training Loss: 0.8823122560977936
[3/3, 560/938] Training Loss: 0.899682343006134
[3/3, 570/938] Training Loss: 0.8674095630645752
[3/3, 580/938] Training Loss: 0.8529509842395783
[3/3, 590/938] Training Loss: 0.8854795753955841
[3/3, 600/938] Training Loss: 0.8776163518428802
[3/3, 610/938] Training Loss: 0.8888862490653991
[3/3, 620/938] Training Loss: 0.8457976400852203
[3/3, 630/938] Training Loss: 0.829275381565094
[3/3, 640/938] Training Loss: 0.8782886505126953
[3/3, 650/938] Training Loss: 0.86935014128685
[3/3, 660/938] Training Loss: 0.8499098300933838
[3/3, 670/938] Training Loss: 0.8601480841636657
[3/3, 680/938] Training Loss: 0.8757230579853058
[3/3, 690/938] Training Loss: 0.869727897644043
[3/3, 700/938] Training Loss: 0.8776399254798889
[3/3, 710/938] Training Loss: 0.8543204605579376
[3/3, 720/938] Training Loss: 0.847767460346222
[3/3, 730/938] Training Loss: 0.8544215261936188
[3/3, 740/938] Training Loss: 0.879718029499054
[3/3, 750/938] Training Loss: 0.8663326978683472
[3/3, 760/938] Training Loss: 0.847570389509201
[3/3, 770/938] Training Loss: 0.8600500524044037
[3/3, 780/938] Training Loss: 0.8368586599826813
[3/3, 790/938] Training Loss: 0.8374768435955048
[3/3, 800/938] Training Loss: 0.8522066831588745
[3/3, 810/938] Training Loss: 0.8417245507240295
[3/3, 820/938] Training Loss: 0.8671936333179474
[3/3, 830/938] Training Loss: 0.8655475497245788
[3/3, 840/938] Training Loss: 0.9048823595046998
[3/3, 850/938] Training Loss: 0.8387745380401611
[3/3, 860/938] Training Loss: 0.8738664746284485
[3/3, 870/938] Training Loss: 0.8618582367897034
[3/3, 880/938] Training Loss: 0.825859797000885
[3/3, 890/938] Training Loss: 0.8480262994766236
[3/3, 900/938] Training Loss: 0.8521531343460083
[3/3, 910/938] Training Loss: 0.842407488822937
[3/3, 920/938] Training Loss: 0.8437152564525604
[3/3, 930/938] Training Loss: 0.8646783411502839
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.9135500586032868
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
0 added
3 added
6 added
7 added
1 added
9 added
8 added
4 added
5 added
2 added
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(