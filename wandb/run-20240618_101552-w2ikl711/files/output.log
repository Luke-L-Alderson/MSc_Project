Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-18 10:15:54.013179
Scaler Value: 0.06756756756756756
Training - 2024-06-18 10:15:54.014171
[1/9, 10/94] Training Loss: 0.3494 - Iteration Time: 0:00:01.573546
[1/9, 20/94] Training Loss: 0.3408 - Iteration Time: 0:00:01.766590
[1/9, 30/94] Training Loss: 0.3421 - Iteration Time: 0:00:01.604372
[1/9, 40/94] Training Loss: 0.3429 - Iteration Time: 0:00:01.690513
[1/9, 50/94] Training Loss: 0.3401 - Iteration Time: 0:00:01.561605
[1/9, 60/94] Training Loss: 0.3358 - Iteration Time: 0:00:01.641389
[1/9, 70/94] Training Loss: 0.3322 - Iteration Time: 0:00:01.623782
[1/9, 80/94] Training Loss: 0.3344 - Iteration Time: 0:00:01.602118
[1/9, 90/94] Training Loss: 0.3310 - Iteration Time: 0:00:01.592072
Testing - 2024-06-18 10:18:28.717545
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 0.3173 - Epoch Time: 0:02:49.985177
Training - 2024-06-18 10:18:43.999853
[2/9, 10/94] Training Loss: 0.3324 - Iteration Time: 0:00:01.611702
[2/9, 20/94] Training Loss: 0.3222 - Iteration Time: 0:00:01.605572
[2/9, 30/94] Training Loss: 0.3214 - Iteration Time: 0:00:01.604159
[2/9, 40/94] Training Loss: 0.3190 - Iteration Time: 0:00:01.574279
[2/9, 50/94] Training Loss: 0.3154 - Iteration Time: 0:00:01.626743
[2/9, 60/94] Training Loss: 0.3167 - Iteration Time: 0:00:01.711088
[2/9, 70/94] Training Loss: 0.3111 - Iteration Time: 0:00:01.593778
[2/9, 80/94] Training Loss: 0.3049 - Iteration Time: 0:00:01.650039
[2/9, 90/94] Training Loss: 0.3064 - Iteration Time: 0:00:01.533291
Testing - 2024-06-18 10:21:17.997982
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 0.2972 - Epoch Time: 0:02:48.815138
Training - 2024-06-18 10:21:32.814991
[3/9, 10/94] Training Loss: 0.3014 - Iteration Time: 0:00:01.545520
[3/9, 20/94] Training Loss: 0.3033 - Iteration Time: 0:00:01.536633
[3/9, 30/94] Training Loss: 0.2966 - Iteration Time: 0:00:01.683908
[3/9, 40/94] Training Loss: 0.2967 - Iteration Time: 0:00:01.683120
[3/9, 50/94] Training Loss: 0.2964 - Iteration Time: 0:00:01.553690
[3/9, 60/94] Training Loss: 0.2933 - Iteration Time: 0:00:01.539474
[3/9, 70/94] Training Loss: 0.2879 - Iteration Time: 0:00:01.512517
[3/9, 80/94] Training Loss: 0.2891 - Iteration Time: 0:00:01.635161
[3/9, 90/94] Training Loss: 0.2865 - Iteration Time: 0:00:01.515605
Testing - 2024-06-18 10:24:01.694441
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 0.2770 - Epoch Time: 0:02:43.752361
Training - 2024-06-18 10:24:16.567846
[4/9, 10/94] Training Loss: 0.2807 - Iteration Time: 0:00:01.524991
[4/9, 20/94] Training Loss: 0.2836 - Iteration Time: 0:00:01.482393
[4/9, 30/94] Training Loss: 0.2795 - Iteration Time: 0:00:01.705841
[4/9, 40/94] Training Loss: 0.2756 - Iteration Time: 0:00:02.381223
[4/9, 50/94] Training Loss: 0.2751 - Iteration Time: 0:00:01.732416
[4/9, 60/94] Training Loss: 0.2712 - Iteration Time: 0:00:01.971582
[4/9, 70/94] Training Loss: 0.2730 - Iteration Time: 0:00:01.685843
[4/9, 80/94] Training Loss: 0.2701 - Iteration Time: 0:00:01.887522
[4/9, 90/94] Training Loss: 0.2647 - Iteration Time: 0:00:01.586066
Testing - 2024-06-18 10:27:00.598470
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 0.2601 - Epoch Time: 0:02:59.797664
Training - 2024-06-18 10:27:16.366005
[5/9, 10/94] Training Loss: 0.2632 - Iteration Time: 0:00:01.521757
[5/9, 20/94] Training Loss: 0.2622 - Iteration Time: 0:00:01.548787
[5/9, 30/94] Training Loss: 0.2555 - Iteration Time: 0:00:02.063050
[5/9, 40/94] Training Loss: 0.2539 - Iteration Time: 0:00:01.576813
[5/9, 50/94] Training Loss: 0.2526 - Iteration Time: 0:00:01.628933
[5/9, 60/94] Training Loss: 0.2506 - Iteration Time: 0:00:01.586042
[5/9, 70/94] Training Loss: 0.2480 - Iteration Time: 0:00:01.506389
[5/9, 80/94] Training Loss: 0.2424 - Iteration Time: 0:00:01.628060
[5/9, 90/94] Training Loss: 0.2421 - Iteration Time: 0:00:01.455719
Testing - 2024-06-18 10:29:49.288166
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 0.2350 - Epoch Time: 0:02:48.602162
Training - 2024-06-18 10:30:04.968663
[6/9, 10/94] Training Loss: 0.2411 - Iteration Time: 0:00:01.667758
[6/9, 20/94] Training Loss: 0.2372 - Iteration Time: 0:00:01.524840
[6/9, 30/94] Training Loss: 0.2364 - Iteration Time: 0:00:01.471327
[6/9, 40/94] Training Loss: 0.2365 - Iteration Time: 0:00:01.634975
[6/9, 50/94] Training Loss: 0.2336 - Iteration Time: 0:00:01.531164
[6/9, 60/94] Training Loss: 0.2312 - Iteration Time: 0:00:01.574494
[6/9, 70/94] Training Loss: 0.2302 - Iteration Time: 0:00:01.464533
[6/9, 80/94] Training Loss: 0.2283 - Iteration Time: 0:00:01.523077
[6/9, 90/94] Training Loss: 0.2268 - Iteration Time: 0:00:01.603861
Testing - 2024-06-18 10:32:37.436422
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 0.2131 - Epoch Time: 0:02:47.226744
Training - 2024-06-18 10:32:52.195407
[7/9, 10/94] Training Loss: 0.2228 - Iteration Time: 0:00:01.464996
[7/9, 20/94] Training Loss: 0.2202 - Iteration Time: 0:00:01.557662
[7/9, 30/94] Training Loss: 0.2185 - Iteration Time: 0:00:01.545847
[7/9, 40/94] Training Loss: 0.2155 - Iteration Time: 0:00:01.548785
[7/9, 50/94] Training Loss: 0.2147 - Iteration Time: 0:00:01.611779
[7/9, 60/94] Training Loss: 0.2089 - Iteration Time: 0:00:01.557864
[7/9, 70/94] Training Loss: 0.2111 - Iteration Time: 0:00:01.602339
[7/9, 80/94] Training Loss: 0.2094 - Iteration Time: 0:00:01.479178
[7/9, 90/94] Training Loss: 0.2084 - Iteration Time: 0:00:01.543463
Testing - 2024-06-18 10:35:22.076179
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 0.1993 - Epoch Time: 0:02:44.607442
Training - 2024-06-18 10:35:36.802849
[8/9, 10/94] Training Loss: 0.2080 - Iteration Time: 0:00:01.489401
[8/9, 20/94] Training Loss: 0.2038 - Iteration Time: 0:00:01.614440
[8/9, 30/94] Training Loss: 0.2037 - Iteration Time: 0:00:01.647582
[8/9, 40/94] Training Loss: 0.2019 - Iteration Time: 0:00:01.465399
[8/9, 50/94] Training Loss: 0.2017 - Iteration Time: 0:00:01.524267
[8/9, 60/94] Training Loss: 0.2017 - Iteration Time: 0:00:01.609067
[8/9, 70/94] Training Loss: 0.1987 - Iteration Time: 0:00:01.689592
[8/9, 80/94] Training Loss: 0.1985 - Iteration Time: 0:00:01.515075
[8/9, 90/94] Training Loss: 0.1964 - Iteration Time: 0:00:01.618462
Testing - 2024-06-18 10:38:06.733274
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 0.1900 - Epoch Time: 0:02:44.367188
Training - 2024-06-18 10:38:21.170037
[9/9, 10/94] Training Loss: 0.1935 - Iteration Time: 0:00:01.742610
[9/9, 20/94] Training Loss: 0.1934 - Iteration Time: 0:00:01.545862
[9/9, 30/94] Training Loss: 0.1923 - Iteration Time: 0:00:01.537510
[9/9, 40/94] Training Loss: 0.1898 - Iteration Time: 0:00:01.443618
[9/9, 50/94] Training Loss: 0.1880 - Iteration Time: 0:00:01.555933
[9/9, 60/94] Training Loss: 0.1889 - Iteration Time: 0:00:01.446870
[9/9, 70/94] Training Loss: 0.1880 - Iteration Time: 0:00:01.526974
[9/9, 80/94] Training Loss: 0.1853 - Iteration Time: 0:00:01.532442
[9/9, 90/94] Training Loss: 0.1896 - Iteration Time: 0:00:01.628808
Testing - 2024-06-18 10:40:47.028186
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 0.1799 - Epoch Time: 0:02:41.323393
Training and Testing Finished - Time: 0:25:08.480747
torch.Size([200, 64, 1, 28, 28])
torch.Size([200, 64, 1, 28, 28])
Assembling test data for t-sne projection
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 1/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 2/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 3/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 4/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 5/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 6/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 7/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 8/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 9/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 10/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 11/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 12/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 13/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 14/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 15/16 --
Code Layer Shape: torch.Size([200, 40, 100])
Code Layer Shape: torch.Size([40, 100])
-- 16/16 --
   Labels      0    1      2      3  ...     95     96     97     98     99
0       2  0.155  0.0  0.135  0.055  ...  0.065  0.070  0.215  0.165  0.060
1       4  0.080  0.0  0.120  0.010  ...  0.020  0.125  0.150  0.110  0.000
2       7  0.120  0.0  0.195  0.090  ...  0.080  0.090  0.210  0.110  0.000
3       3  0.060  0.0  0.035  0.200  ...  0.050  0.175  0.255  0.080  0.035
4       7  0.020  0.0  0.150  0.120  ...  0.025  0.105  0.100  0.105  0.005
[5 rows x 101 columns]
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2
Applying UMAP