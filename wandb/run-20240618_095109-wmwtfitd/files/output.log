Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-18 09:51:11.851963
Scaler Value: 0.1020408163265306
Training - 2024-06-18 09:51:11.853450
[1/9, 10/94] Training Loss: 0.3618 - Iteration Time: 0:00:01.665298
[1/9, 20/94] Training Loss: 0.3548 - Iteration Time: 0:00:01.482430
[1/9, 30/94] Training Loss: 0.3563 - Iteration Time: 0:00:01.548492
[1/9, 40/94] Training Loss: 0.3565 - Iteration Time: 0:00:01.496784
[1/9, 50/94] Training Loss: 0.3537 - Iteration Time: 0:00:01.665130
[1/9, 60/94] Training Loss: 0.3486 - Iteration Time: 0:00:01.558312
[1/9, 70/94] Training Loss: 0.3446 - Iteration Time: 0:00:01.641212
[1/9, 80/94] Training Loss: 0.3452 - Iteration Time: 0:00:01.820898
[1/9, 90/94] Training Loss: 0.3421 - Iteration Time: 0:00:01.469940
Testing - 2024-06-18 09:53:40.843615
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 0.3287 - Epoch Time: 0:02:43.932399
Training - 2024-06-18 09:53:55.785849
[2/9, 10/94] Training Loss: 0.3440 - Iteration Time: 0:00:01.505051
[2/9, 20/94] Training Loss: 0.3350 - Iteration Time: 0:00:03.153840
[2/9, 30/94] Training Loss: 0.3362 - Iteration Time: 0:00:01.624695
[2/9, 40/94] Training Loss: 0.3350 - Iteration Time: 0:00:01.989760
[2/9, 50/94] Training Loss: 0.3324 - Iteration Time: 0:00:01.496003
[2/9, 60/94] Training Loss: 0.3352 - Iteration Time: 0:00:01.504419
[2/9, 70/94] Training Loss: 0.3306 - Iteration Time: 0:00:01.395468
[2/9, 80/94] Training Loss: 0.3247 - Iteration Time: 0:00:01.839498
[2/9, 90/94] Training Loss: 0.3258 - Iteration Time: 0:00:01.409256
Testing - 2024-06-18 09:56:24.569115
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 0.3170 - Epoch Time: 0:02:44.540951
Training - 2024-06-18 09:56:40.327296
[3/9, 10/94] Training Loss: 0.3216 - Iteration Time: 0:00:01.471833
[3/9, 20/94] Training Loss: 0.3231 - Iteration Time: 0:00:01.464772
[3/9, 30/94] Training Loss: 0.3163 - Iteration Time: 0:00:01.571185
[3/9, 40/94] Training Loss: 0.3167 - Iteration Time: 0:00:01.426004
[3/9, 50/94] Training Loss: 0.3173 - Iteration Time: 0:00:01.510114
[3/9, 60/94] Training Loss: 0.3139 - Iteration Time: 0:00:01.554579
[3/9, 70/94] Training Loss: 0.3089 - Iteration Time: 0:00:01.641825
[3/9, 80/94] Training Loss: 0.3105 - Iteration Time: 0:00:01.466044
[3/9, 90/94] Training Loss: 0.3095 - Iteration Time: 0:00:01.473725
Testing - 2024-06-18 09:59:03.502674
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 0.2967 - Epoch Time: 0:02:37.224408
Training - 2024-06-18 09:59:17.551704
[4/9, 10/94] Training Loss: 0.3035 - Iteration Time: 0:00:01.422362
[4/9, 20/94] Training Loss: 0.3069 - Iteration Time: 0:00:01.546377
[4/9, 30/94] Training Loss: 0.3035 - Iteration Time: 0:00:01.482230
[4/9, 40/94] Training Loss: 0.3002 - Iteration Time: 0:00:01.572084
[4/9, 50/94] Training Loss: 0.2988 - Iteration Time: 0:00:01.658078
[4/9, 60/94] Training Loss: 0.2962 - Iteration Time: 0:00:01.481216
[4/9, 70/94] Training Loss: 0.2986 - Iteration Time: 0:00:01.429379
[4/9, 80/94] Training Loss: 0.2955 - Iteration Time: 0:00:01.488514
[4/9, 90/94] Training Loss: 0.2912 - Iteration Time: 0:00:01.483518
Testing - 2024-06-18 10:01:41.245885
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 0.2856 - Epoch Time: 0:02:37.461150
Training - 2024-06-18 10:01:55.012854
[5/9, 10/94] Training Loss: 0.2890 - Iteration Time: 0:00:01.497132
[5/9, 20/94] Training Loss: 0.2890 - Iteration Time: 0:00:01.624756
[5/9, 30/94] Training Loss: 0.2834 - Iteration Time: 0:00:01.505067
[5/9, 40/94] Training Loss: 0.2815 - Iteration Time: 0:00:01.464741
[5/9, 50/94] Training Loss: 0.2808 - Iteration Time: 0:00:01.428137
[5/9, 60/94] Training Loss: 0.2800 - Iteration Time: 0:00:01.640891
[5/9, 70/94] Training Loss: 0.2771 - Iteration Time: 0:00:01.450651
[5/9, 80/94] Training Loss: 0.2713 - Iteration Time: 0:00:01.500167
[5/9, 90/94] Training Loss: 0.2718 - Iteration Time: 0:00:01.400910
Testing - 2024-06-18 10:04:19.463020
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 0.2641 - Epoch Time: 0:02:39.795666
Training - 2024-06-18 10:04:34.808520
[6/9, 10/94] Training Loss: 0.2699 - Iteration Time: 0:00:01.610946
[6/9, 20/94] Training Loss: 0.2648 - Iteration Time: 0:00:01.398887
[6/9, 30/94] Training Loss: 0.2640 - Iteration Time: 0:00:01.746384
[6/9, 40/94] Training Loss: 0.2643 - Iteration Time: 0:00:01.439539
[6/9, 50/94] Training Loss: 0.2620 - Iteration Time: 0:00:01.555953
[6/9, 60/94] Training Loss: 0.2604 - Iteration Time: 0:00:01.588213
[6/9, 70/94] Training Loss: 0.2595 - Iteration Time: 0:00:01.501567
[6/9, 80/94] Training Loss: 0.2577 - Iteration Time: 0:00:01.513165
[6/9, 90/94] Training Loss: 0.2569 - Iteration Time: 0:00:01.463159
Testing - 2024-06-18 10:07:00.207813
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 0.2431 - Epoch Time: 0:02:39.482486
Training - 2024-06-18 10:07:14.291006
[7/9, 10/94] Training Loss: 0.2529 - Iteration Time: 0:00:01.458864
[7/9, 20/94] Training Loss: 0.2512 - Iteration Time: 0:00:01.628580
[7/9, 30/94] Training Loss: 0.2508 - Iteration Time: 0:00:01.410941
[7/9, 40/94] Training Loss: 0.2481 - Iteration Time: 0:00:01.478970
[7/9, 50/94] Training Loss: 0.2459 - Iteration Time: 0:00:01.404212
[7/9, 60/94] Training Loss: 0.2393 - Iteration Time: 0:00:01.756006
[7/9, 70/94] Training Loss: 0.2411 - Iteration Time: 0:00:01.788418
[7/9, 80/94] Training Loss: 0.2394 - Iteration Time: 0:00:01.376590
[7/9, 90/94] Training Loss: 0.2365 - Iteration Time: 0:00:01.387090
Testing - 2024-06-18 10:09:35.534050
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 0.2256 - Epoch Time: 0:02:34.907715
Training - 2024-06-18 10:09:49.199217
[8/9, 10/94] Training Loss: 0.2351 - Iteration Time: 0:00:01.505978
[8/9, 20/94] Training Loss: 0.2311 - Iteration Time: 0:00:01.419961
[8/9, 30/94] Training Loss: 0.2303 - Iteration Time: 0:00:01.525925
[8/9, 40/94] Training Loss: 0.2288 - Iteration Time: 0:00:01.444923
[8/9, 50/94] Training Loss: 0.2289 - Iteration Time: 0:00:01.575786
[8/9, 60/94] Training Loss: 0.2285 - Iteration Time: 0:00:01.506095
[8/9, 70/94] Training Loss: 0.2250 - Iteration Time: 0:00:01.669147
[8/9, 80/94] Training Loss: 0.2238 - Iteration Time: 0:00:01.532318
[8/9, 90/94] Training Loss: 0.2216 - Iteration Time: 0:00:01.610114
Testing - 2024-06-18 10:12:16.232538
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 0.2150 - Epoch Time: 0:02:41.669735
Training - 2024-06-18 10:12:30.868952
[9/9, 10/94] Training Loss: 0.2197 - Iteration Time: 0:00:01.563016
[9/9, 20/94] Training Loss: 0.2198 - Iteration Time: 0:00:01.907173
[9/9, 30/94] Training Loss: 0.2191 - Iteration Time: 0:00:01.451635
[9/9, 40/94] Training Loss: 0.2156 - Iteration Time: 0:00:01.402039
[9/9, 50/94] Training Loss: 0.2148 - Iteration Time: 0:00:01.531555
[9/9, 60/94] Training Loss: 0.2171 - Iteration Time: 0:00:01.650655
[9/9, 70/94] Training Loss: 0.2143 - Iteration Time: 0:00:01.535663
[9/9, 80/94] Training Loss: 0.2125 - Iteration Time: 0:00:01.527595
[9/9, 90/94] Training Loss: 0.2162 - Iteration Time: 0:00:01.519013
Testing - 2024-06-18 10:14:55.725529
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 0.2047 - Epoch Time: 0:02:40.125556
Training and Testing Finished - Time: 0:23:59.142545
torch.Size([200, 64, 1, 28, 28])
torch.Size([200, 64, 1, 28, 28])
Assembling test data for t-sne projection
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 1/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 2/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 3/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 4/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 5/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 6/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 7/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 8/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 9/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 10/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 11/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 12/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 13/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 14/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 15/16 --
Code Layer Shape: torch.Size([200, 40, 100])
Code Layer Shape: torch.Size([40, 100])
-- 16/16 --
   Labels      0    1      2      3      4  ...   94   95   96     97     98     99
0       2  0.130  0.0  0.185  0.055  0.090  ...  0.0  0.0  0.0  0.075  0.075  0.150
1       4  0.025  0.0  0.120  0.120  0.105  ...  0.0  0.0  0.0  0.090  0.090  0.025
2       7  0.095  0.0  0.140  0.060  0.090  ...  0.0  0.0  0.0  0.140  0.070  0.075
3       3  0.090  0.0  0.100  0.165  0.060  ...  0.0  0.0  0.0  0.095  0.045  0.115
4       7  0.080  0.0  0.115  0.060  0.085  ...  0.0  0.0  0.0  0.130  0.070  0.020
[5 rows x 101 columns]
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2
Applying UMAP