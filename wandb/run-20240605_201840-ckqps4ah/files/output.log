Starting Sweep: Batch Size: 32, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 18000
Testing: 10000 -> 3000
Making Subsets
Training: 18000
Testing: 3000
Making Dataloaders
Defining network
2024-06-05 20:18:42.954833
Training!
0:00:00.089421
[1/6, 29/563] Training Loss: 4.257483959197998
[1/6, 58/563] Training Loss: 1.049383282661438
[1/6, 87/563] Training Loss: 1.0151166915893555
[1/6, 116/563] Training Loss: 0.9920311570167542
[1/6, 145/563] Training Loss: 1.00999116897583
[1/6, 174/563] Training Loss: 0.9470045566558838
[1/6, 203/563] Training Loss: 0.971989095211029
[1/6, 232/563] Training Loss: 0.989371657371521
[1/6, 261/563] Training Loss: 0.9853230714797974
[1/6, 290/563] Training Loss: 0.9309132099151611
[1/6, 319/563] Training Loss: 0.9831480383872986
[1/6, 348/563] Training Loss: 0.9722765684127808
[1/6, 377/563] Training Loss: 0.934712827205658
[1/6, 406/563] Training Loss: 0.944560170173645
[1/6, 435/563] Training Loss: 0.9405402541160583
[1/6, 464/563] Training Loss: 0.9415119886398315
[1/6, 493/563] Training Loss: 0.9738470911979675
[1/6, 522/563] Training Loss: 0.933656632900238
[1/6, 551/563] Training Loss: 0.9246039390563965
Testing!
[1/6, 1/94]
[1/6, 6/94]
[1/6, 11/94]
[1/6, 16/94]
[1/6, 21/94]
[1/6, 26/94]
[1/6, 31/94]
[1/6, 36/94]
[1/6, 41/94]
[1/6, 46/94]
[1/6, 51/94]
[1/6, 56/94]
[1/6, 61/94]
[1/6, 66/94]
[1/6, 71/94]
[1/6, 76/94]
[1/6, 81/94]
[1/6, 86/94]
[1/6, 91/94]
Testing Loss: 0.933426022529602
Training!
0:11:55.891146
[2/6, 29/563] Training Loss: 0.9242374300956726
[2/6, 58/563] Training Loss: 0.8919458389282227
[2/6, 87/563] Training Loss: 0.8952277302742004
[2/6, 116/563] Training Loss: 0.8820703029632568
[2/6, 145/563] Training Loss: 0.8827234506607056
[2/6, 174/563] Training Loss: 0.8668460249900818
[2/6, 203/563] Training Loss: 0.9136653542518616
[2/6, 232/563] Training Loss: 0.8374555110931396
[2/6, 261/563] Training Loss: 0.8432270288467407
[2/6, 290/563] Training Loss: 0.8095597624778748
[2/6, 319/563] Training Loss: 0.8269299268722534
[2/6, 348/563] Training Loss: 0.8031500577926636
[2/6, 377/563] Training Loss: 0.7990004420280457
[2/6, 406/563] Training Loss: 0.7896707057952881
[2/6, 435/563] Training Loss: 0.7461187839508057
[2/6, 464/563] Training Loss: 0.7341921925544739
[2/6, 493/563] Training Loss: 0.737556517124176
[2/6, 522/563] Training Loss: 0.7144707441329956
[2/6, 551/563] Training Loss: 0.6959663033485413
Testing!
[2/6, 1/94]
[2/6, 6/94]
[2/6, 11/94]
[2/6, 16/94]
[2/6, 21/94]
[2/6, 26/94]
[2/6, 31/94]
[2/6, 36/94]
[2/6, 41/94]
[2/6, 46/94]
[2/6, 51/94]
[2/6, 56/94]
[2/6, 61/94]
[2/6, 66/94]
[2/6, 71/94]
[2/6, 76/94]
[2/6, 81/94]
[2/6, 86/94]
[2/6, 91/94]
Testing Loss: 0.7385525107383728
Training!
0:23:50.731963
[3/6, 29/563] Training Loss: 0.6909454464912415
[3/6, 58/563] Training Loss: 0.6630772352218628
[3/6, 87/563] Training Loss: 0.6321742534637451
[3/6, 116/563] Training Loss: 0.6286526322364807
[3/6, 145/563] Training Loss: 0.638299822807312
[3/6, 174/563] Training Loss: 0.6232357025146484
[3/6, 203/563] Training Loss: 0.5986972451210022
[3/6, 232/563] Training Loss: 0.602872908115387
[3/6, 261/563] Training Loss: 0.5759440660476685
[3/6, 290/563] Training Loss: 0.5819301605224609
[3/6, 319/563] Training Loss: 0.5470316410064697
[3/6, 348/563] Training Loss: 0.546760082244873
[3/6, 377/563] Training Loss: 0.5354925394058228
[3/6, 406/563] Training Loss: 0.518461287021637
[3/6, 435/563] Training Loss: 0.5106486678123474
[3/6, 464/563] Training Loss: 0.49991166591644287
[3/6, 493/563] Training Loss: 0.4940698444843292
[3/6, 522/563] Training Loss: 0.4819900095462799
[3/6, 551/563] Training Loss: 0.47984543442726135
Testing!
[3/6, 1/94]
[3/6, 6/94]
[3/6, 11/94]
[3/6, 16/94]
[3/6, 21/94]
[3/6, 26/94]
[3/6, 31/94]
[3/6, 36/94]
[3/6, 41/94]
[3/6, 46/94]
[3/6, 51/94]
[3/6, 56/94]
[3/6, 61/94]
[3/6, 66/94]
[3/6, 71/94]
[3/6, 76/94]
[3/6, 81/94]
[3/6, 86/94]
[3/6, 91/94]
Testing Loss: 0.46878281235694885
Training!
0:35:44.465451
[4/6, 29/563] Training Loss: 0.47245168685913086
[4/6, 58/563] Training Loss: 0.4576626718044281
[4/6, 87/563] Training Loss: 0.44939669966697693
[4/6, 116/563] Training Loss: 0.45731672644615173
[4/6, 145/563] Training Loss: 0.4428598880767822
[4/6, 174/563] Training Loss: 0.4393506944179535
[4/6, 203/563] Training Loss: 0.4222168028354645
[4/6, 232/563] Training Loss: 0.4296920895576477
[4/6, 261/563] Training Loss: 0.42338094115257263
[4/6, 290/563] Training Loss: 0.41710782051086426
[4/6, 319/563] Training Loss: 0.4021132290363312
[4/6, 348/563] Training Loss: 0.39672598242759705
[4/6, 377/563] Training Loss: 0.38682734966278076
[4/6, 406/563] Training Loss: 0.3892022669315338
[4/6, 435/563] Training Loss: 0.37793299555778503
[4/6, 464/563] Training Loss: 0.38005316257476807
[4/6, 493/563] Training Loss: 0.37492212653160095
[4/6, 522/563] Training Loss: 0.36622607707977295
[4/6, 551/563] Training Loss: 0.37148603796958923
Testing!
[4/6, 1/94]
[4/6, 6/94]
[4/6, 11/94]
[4/6, 16/94]
[4/6, 21/94]
[4/6, 26/94]
[4/6, 31/94]
[4/6, 36/94]
[4/6, 41/94]
[4/6, 46/94]
[4/6, 51/94]
[4/6, 56/94]
[4/6, 61/94]
[4/6, 66/94]
[4/6, 71/94]
[4/6, 76/94]
[4/6, 81/94]
[4/6, 86/94]
[4/6, 91/94]
Testing Loss: 0.38167962431907654
Training!
0:47:38.491074
[5/6, 29/563] Training Loss: 0.3603983223438263
[5/6, 58/563] Training Loss: 0.3574950098991394
[5/6, 87/563] Training Loss: 0.357257604598999
[5/6, 116/563] Training Loss: 0.3541237711906433
[5/6, 145/563] Training Loss: 0.34627196192741394
[5/6, 174/563] Training Loss: 0.3435516059398651
[5/6, 203/563] Training Loss: 0.3379043638706207
[5/6, 232/563] Training Loss: 0.3326359689235687
[5/6, 261/563] Training Loss: 0.3391610085964203
[5/6, 290/563] Training Loss: 0.32959312200546265
[5/6, 319/563] Training Loss: 0.32584577798843384
[5/6, 348/563] Training Loss: 0.318581223487854
[5/6, 377/563] Training Loss: 0.31754305958747864
[5/6, 406/563] Training Loss: 0.3165464997291565
[5/6, 435/563] Training Loss: 0.31512314081192017
[5/6, 464/563] Training Loss: 0.31726914644241333
[5/6, 493/563] Training Loss: 0.30648353695869446
[5/6, 522/563] Training Loss: 0.30425092577934265
[5/6, 551/563] Training Loss: 0.2966170012950897
Testing!
[5/6, 1/94]
[5/6, 6/94]
[5/6, 11/94]
[5/6, 16/94]
[5/6, 21/94]
[5/6, 26/94]
[5/6, 31/94]
[5/6, 36/94]
[5/6, 41/94]
[5/6, 46/94]
[5/6, 51/94]
[5/6, 56/94]
[5/6, 61/94]
[5/6, 66/94]
[5/6, 71/94]
[5/6, 76/94]
[5/6, 81/94]
[5/6, 86/94]
[5/6, 91/94]
Testing Loss: 0.29713743925094604
Training!
0:59:31.937048
[6/6, 29/563] Training Loss: 0.30312371253967285
[6/6, 58/563] Training Loss: 0.29121971130371094
[6/6, 87/563] Training Loss: 0.29137441515922546
[6/6, 116/563] Training Loss: 0.2954403758049011
[6/6, 145/563] Training Loss: 0.2901475131511688
[6/6, 174/563] Training Loss: 0.28515416383743286
[6/6, 203/563] Training Loss: 0.2791706919670105
[6/6, 232/563] Training Loss: 0.2779856324195862
[6/6, 261/563] Training Loss: 0.27127182483673096
[6/6, 290/563] Training Loss: 0.26886919140815735
[6/6, 319/563] Training Loss: 0.27204930782318115
[6/6, 348/563] Training Loss: 0.26751333475112915
[6/6, 377/563] Training Loss: 0.26485419273376465
[6/6, 406/563] Training Loss: 0.26389947533607483
[6/6, 435/563] Training Loss: 0.257602721452713
[6/6, 464/563] Training Loss: 0.26022395491600037
[6/6, 493/563] Training Loss: 0.25064796209335327
[6/6, 522/563] Training Loss: 0.2507595717906952
[6/6, 551/563] Training Loss: 0.24025379121303558
Testing!
[6/6, 1/94]
[6/6, 6/94]
[6/6, 11/94]
[6/6, 16/94]
[6/6, 21/94]
[6/6, 26/94]
[6/6, 31/94]
[6/6, 36/94]
[6/6, 41/94]
[6/6, 46/94]
[6/6, 51/94]
[6/6, 56/94]
[6/6, 61/94]
[6/6, 66/94]
[6/6, 71/94]
[6/6, 76/94]
[6/6, 81/94]
[6/6, 86/94]
[6/6, 91/94]
Testing Loss: 0.2399127036333084
Training and Testing Finished
Assembling test data for t-sne projection
-- 1/94 --
-- 2/94 --
-- 3/94 --
-- 4/94 --
-- 5/94 --
-- 6/94 --
-- 7/94 --
-- 8/94 --
-- 9/94 --
-- 10/94 --
-- 11/94 --
-- 12/94 --
-- 13/94 --
-- 14/94 --
-- 15/94 --
-- 16/94 --
-- 17/94 --
-- 18/94 --
-- 19/94 --
-- 20/94 --
-- 21/94 --
-- 22/94 --
-- 23/94 --
-- 24/94 --
-- 25/94 --
-- 26/94 --
-- 27/94 --
-- 28/94 --
-- 29/94 --
-- 30/94 --
-- 31/94 --
-- 32/94 --
-- 33/94 --
-- 34/94 --
-- 35/94 --
-- 36/94 --
-- 37/94 --
-- 38/94 --
-- 39/94 --
-- 40/94 --
-- 41/94 --
-- 42/94 --
-- 43/94 --
-- 44/94 --
-- 45/94 --
-- 46/94 --
-- 47/94 --
-- 48/94 --
-- 49/94 --
-- 50/94 --
-- 51/94 --
-- 52/94 --
-- 53/94 --
-- 54/94 --
-- 55/94 --
-- 56/94 --
-- 57/94 --
-- 58/94 --
-- 59/94 --
-- 60/94 --
-- 61/94 --
-- 62/94 --
-- 63/94 --
-- 64/94 --
-- 65/94 --
-- 66/94 --
-- 67/94 --
-- 68/94 --
-- 69/94 --
-- 70/94 --
-- 71/94 --
-- 72/94 --
-- 73/94 --
-- 74/94 --
-- 75/94 --
-- 76/94 --
-- 77/94 --
-- 78/94 --
-- 79/94 --
-- 80/94 --
-- 81/94 --
-- 82/94 --
-- 83/94 --
-- 84/94 --
-- 85/94 --
-- 86/94 --
-- 87/94 --
-- 88/94 --
-- 89/94 --
-- 90/94 --
-- 91/94 --
-- 92/94 --
-- 93/94 --
-- 94/94 --
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation