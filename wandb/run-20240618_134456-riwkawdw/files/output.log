Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-18 13:44:58.047255
Scaler Value: 1.25
Training - 2024-06-18 13:44:58.047750
[1/9, 10/94] Training Loss: 0.9079 - Iteration Time: 0:00:01.416452
[1/9, 20/94] Training Loss: 0.8476 - Iteration Time: 0:00:01.409045
[1/9, 30/94] Training Loss: 0.8555 - Iteration Time: 0:00:01.414959
[1/9, 40/94] Training Loss: 0.8578 - Iteration Time: 0:00:01.360000
[1/9, 50/94] Training Loss: 0.8595 - Iteration Time: 0:00:01.429400
[1/9, 60/94] Training Loss: 0.8572 - Iteration Time: 0:00:01.396637
[1/9, 70/94] Training Loss: 0.8572 - Iteration Time: 0:00:01.403055
[1/9, 80/94] Training Loss: 0.8674 - Iteration Time: 0:00:01.512233
[1/9, 90/94] Training Loss: 0.8646 - Iteration Time: 0:00:01.391141
Testing - 2024-06-18 13:47:15.202097
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 0.8490 - Epoch Time: 0:02:30.429183
Training - 2024-06-18 13:47:28.476933
[2/9, 10/94] Training Loss: 0.8728 - Iteration Time: 0:00:01.704268
[2/9, 20/94] Training Loss: 0.8621 - Iteration Time: 0:00:01.521170
[2/9, 30/94] Training Loss: 0.8645 - Iteration Time: 0:00:01.424414
[2/9, 40/94] Training Loss: 0.8712 - Iteration Time: 0:00:01.422923
[2/9, 50/94] Training Loss: 0.8662 - Iteration Time: 0:00:01.450722
[2/9, 60/94] Training Loss: 0.8734 - Iteration Time: 0:00:01.408517
[2/9, 70/94] Training Loss: 0.8748 - Iteration Time: 0:00:01.455666
[2/9, 80/94] Training Loss: 0.8689 - Iteration Time: 0:00:01.487443
[2/9, 90/94] Training Loss: 0.8757 - Iteration Time: 0:00:01.379244
Testing - 2024-06-18 13:49:46.883879
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 0.8617 - Epoch Time: 0:02:31.394011
Training - 2024-06-18 13:49:59.871440
[3/9, 10/94] Training Loss: 0.8728 - Iteration Time: 0:00:01.376735
[3/9, 20/94] Training Loss: 0.8726 - Iteration Time: 0:00:01.374323
[3/9, 30/94] Training Loss: 0.8701 - Iteration Time: 0:00:01.391665
[3/9, 40/94] Training Loss: 0.8724 - Iteration Time: 0:00:01.356919
[3/9, 50/94] Training Loss: 0.8773 - Iteration Time: 0:00:01.397609
[3/9, 60/94] Training Loss: 0.8760 - Iteration Time: 0:00:01.360867
[3/9, 70/94] Training Loss: 0.8726 - Iteration Time: 0:00:01.372282
[3/9, 80/94] Training Loss: 0.8776 - Iteration Time: 0:00:01.391133
[3/9, 90/94] Training Loss: 0.8801 - Iteration Time: 0:00:01.383229
Testing - 2024-06-18 13:52:11.849749
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 0.8612 - Epoch Time: 0:02:24.888809
Training - 2024-06-18 13:52:24.760249
[4/9, 10/94] Training Loss: 0.8771 - Iteration Time: 0:00:01.414979
[4/9, 20/94] Training Loss: 0.8813 - Iteration Time: 0:00:01.352008
[4/9, 30/94] Training Loss: 0.8807 - Iteration Time: 0:00:01.378324
[4/9, 40/94] Training Loss: 0.8780 - Iteration Time: 0:00:01.366858
[4/9, 50/94] Training Loss: 0.8806 - Iteration Time: 0:00:01.350971
[4/9, 60/94] Training Loss: 0.8802 - Iteration Time: 0:00:01.416589
[4/9, 70/94] Training Loss: 0.8834 - Iteration Time: 0:00:01.539052
[4/9, 80/94] Training Loss: 0.8845 - Iteration Time: 0:00:01.346990
[4/9, 90/94] Training Loss: 0.8805 - Iteration Time: 0:00:01.349045
Testing - 2024-06-18 13:54:36.525605
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 0.8786 - Epoch Time: 0:02:24.524783
Training - 2024-06-18 13:54:49.285032
[5/9, 10/94] Training Loss: 0.8813 - Iteration Time: 0:00:01.428426
[5/9, 20/94] Training Loss: 0.8891 - Iteration Time: 0:00:01.349371
[5/9, 30/94] Training Loss: 0.8826 - Iteration Time: 0:00:01.420448
[5/9, 40/94] Training Loss: 0.8849 - Iteration Time: 0:00:01.360904
[5/9, 50/94] Training Loss: 0.8876 - Iteration Time: 0:00:01.379282
[5/9, 60/94] Training Loss: 0.8874 - Iteration Time: 0:00:01.344604
[5/9, 70/94] Training Loss: 0.8881 - Iteration Time: 0:00:01.391659
[5/9, 80/94] Training Loss: 0.8846 - Iteration Time: 0:00:01.351468
[5/9, 90/94] Training Loss: 0.8889 - Iteration Time: 0:00:01.343572
Testing - 2024-06-18 13:57:00.434849
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 0.8783 - Epoch Time: 0:02:24.212783
Training - 2024-06-18 13:57:13.498312
[6/9, 10/94] Training Loss: 0.8884 - Iteration Time: 0:00:01.334093
[6/9, 20/94] Training Loss: 0.8851 - Iteration Time: 0:00:01.383218
[6/9, 30/94] Training Loss: 0.8881 - Iteration Time: 0:00:01.365385
[6/9, 40/94] Training Loss: 0.8901 - Iteration Time: 0:00:01.407587
[6/9, 50/94] Training Loss: 0.8897 - Iteration Time: 0:00:01.380764
[6/9, 60/94] Training Loss: 0.8876 - Iteration Time: 0:00:01.393679
[6/9, 70/94] Training Loss: 0.8917 - Iteration Time: 0:00:01.338066
[6/9, 80/94] Training Loss: 0.8891 - Iteration Time: 0:00:01.423892
[6/9, 90/94] Training Loss: 0.8969 - Iteration Time: 0:00:01.366863
Testing - 2024-06-18 13:59:24.807599
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 0.8634 - Epoch Time: 0:02:24.211581
Training - 2024-06-18 13:59:37.709893
[7/9, 10/94] Training Loss: 0.8899 - Iteration Time: 0:00:01.364848
[7/9, 20/94] Training Loss: 0.8922 - Iteration Time: 0:00:01.399108
[7/9, 30/94] Training Loss: 0.8911 - Iteration Time: 0:00:01.368446
[7/9, 40/94] Training Loss: 0.8931 - Iteration Time: 0:00:01.371855
[7/9, 50/94] Training Loss: 0.8905 - Iteration Time: 0:00:01.360873
[7/9, 60/94] Training Loss: 0.8870 - Iteration Time: 0:00:01.407548
[7/9, 70/94] Training Loss: 0.8937 - Iteration Time: 0:00:01.389696
[7/9, 80/94] Training Loss: 0.8904 - Iteration Time: 0:00:01.359401
[7/9, 90/94] Training Loss: 0.8919 - Iteration Time: 0:00:01.338099
Testing - 2024-06-18 14:01:48.559998
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 0.8863 - Epoch Time: 0:02:23.796981
Training - 2024-06-18 14:02:01.506874
[8/9, 10/94] Training Loss: 0.8968 - Iteration Time: 0:00:01.362374
[8/9, 20/94] Training Loss: 0.8950 - Iteration Time: 0:00:01.353977
[8/9, 30/94] Training Loss: 0.8963 - Iteration Time: 0:00:01.399132
[8/9, 40/94] Training Loss: 0.8913 - Iteration Time: 0:00:01.378277
[8/9, 50/94] Training Loss: 0.8902 - Iteration Time: 0:00:01.372346
[8/9, 60/94] Training Loss: 0.8921 - Iteration Time: 0:00:01.359435
[8/9, 70/94] Training Loss: 0.8975 - Iteration Time: 0:00:01.360911
[8/9, 80/94] Training Loss: 0.8976 - Iteration Time: 0:00:01.360484
[8/9, 90/94] Training Loss: 0.8940 - Iteration Time: 0:00:01.417506
Testing - 2024-06-18 14:04:12.587615
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 0.9003 - Epoch Time: 0:02:23.938262
Training - 2024-06-18 14:04:25.445631
[9/9, 10/94] Training Loss: 0.8953 - Iteration Time: 0:00:01.410535
[9/9, 20/94] Training Loss: 0.8961 - Iteration Time: 0:00:01.386224
[9/9, 30/94] Training Loss: 0.8996 - Iteration Time: 0:00:01.368854
[9/9, 40/94] Training Loss: 0.8987 - Iteration Time: 0:00:01.387183
[9/9, 50/94] Training Loss: 0.8837 - Iteration Time: 0:00:01.426984
[9/9, 60/94] Training Loss: 0.8958 - Iteration Time: 0:00:01.359119
[9/9, 70/94] Training Loss: 0.8919 - Iteration Time: 0:00:01.464588
[9/9, 80/94] Training Loss: 0.8925 - Iteration Time: 0:00:01.619033
[9/9, 90/94] Training Loss: 0.9028 - Iteration Time: 0:00:01.392207
Testing - 2024-06-18 14:06:37.199574
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 0.8860 - Epoch Time: 0:02:24.882328
Training and Testing Finished - Time: 0:21:52.281201
torch.Size([200, 64, 1, 28, 28])
torch.Size([200, 64, 1, 28, 28])
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
   Labels      0    1      2      3      4  ...   94   95   96   97     98   99
0       2  0.110  0.0  0.170  0.170  0.155  ...  0.0  0.0  0.0  0.0  0.095  0.0
1       4  0.105  0.0  0.170  0.170  0.155  ...  0.0  0.0  0.0  0.0  0.095  0.0
2       7  0.105  0.0  0.170  0.170  0.155  ...  0.0  0.0  0.0  0.0  0.095  0.0
3       3  0.110  0.0  0.170  0.175  0.155  ...  0.0  0.0  0.0  0.0  0.090  0.0
4       7  0.105  0.0  0.165  0.165  0.155  ...  0.0  0.0  0.0  0.0  0.100  0.0
[5 rows x 101 columns]
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2
Applying UMAP