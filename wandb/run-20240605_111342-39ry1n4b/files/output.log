Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 3000
Testing: 10000 -> 500
Making Subsets
Training: 3000
Testing: 500
Making Dataloaders
Defining network
2024-06-05 11:13:44.431364
Training!
0:00:00.001488
0:00:00.001488
0:00:00.037697
0:00:00.038193
0:00:00.165189
0:00:00.166181
0:00:01.120488
0:00:01.142808
0:00:01.695542
0:00:01.716376
tensor(17.4480, device='cuda:0', grad_fn=<DivBackward0>)
tensor(17.4480, device='cuda:0')
0:00:01.747625
0:00:01.791778
0:00:01.792273
0:00:01.927750
0:00:01.928245
0:00:02.541557
0:00:02.547015
0:00:03.011912
0:00:03.013401
tensor(14.5501, device='cuda:0', grad_fn=<DivBackward0>)
tensor(14.5501, device='cuda:0')
0:00:03.015385
0:00:03.031257
0:00:03.031257
0:00:03.151352
0:00:03.151848
0:00:03.750646
0:00:03.756102
0:00:04.243326
0:00:04.244318
tensor(10.8432, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.8432, device='cuda:0')
0:00:04.246798
[1/1, 3/47] Training Loss: 14.28040885925293
0:00:04.273088
0:00:04.273583
0:00:04.384207
0:00:04.385199
0:00:05.021253
0:00:05.026709
0:00:05.509459
0:00:05.510424
tensor(10.0882, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.0882, device='cuda:0')
0:00:05.512932
0:00:05.533736
0:00:05.533736
0:00:05.649890
0:00:05.650386
0:00:06.441758
0:00:06.447215
0:00:06.924187
0:00:06.925180
tensor(7.9827, device='cuda:0', grad_fn=<DivBackward0>)
tensor(7.9827, device='cuda:0')
0:00:06.927660
0:00:06.945516
0:00:06.946012
0:00:07.060609
0:00:07.061601
0:00:07.688689
0:00:07.694642
0:00:08.165525
0:00:08.166517
tensor(6.4072, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4072, device='cuda:0')
0:00:08.168997
[1/1, 6/47] Training Loss: 8.159356117248535
0:00:08.187845
0:00:08.187845
0:00:08.303433
0:00:08.303928
0:00:08.907853
0:00:08.913364
0:00:09.393647
0:00:09.394639
tensor(6.4154, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4154, device='cuda:0')
0:00:09.397617
0:00:09.415487
0:00:09.415487
0:00:09.540993
0:00:09.541488
0:00:10.182505
0:00:10.187962
0:00:10.680105
0:00:10.681097
tensor(5.4047, device='cuda:0', grad_fn=<DivBackward0>)
tensor(5.4047, device='cuda:0')
0:00:10.684074
0:00:10.705403
0:00:10.705899
0:00:10.833404
0:00:10.834396
0:00:11.513677
0:00:11.519148
0:00:11.992444
0:00:11.993437
tensor(4.2051, device='cuda:0', grad_fn=<DivBackward0>)
tensor(4.2051, device='cuda:0')
0:00:11.995917
[1/1, 9/47] Training Loss: 5.341723442077637
0:00:12.014271
0:00:12.014271
0:00:12.127430
0:00:12.127926
0:00:12.720774
0:00:12.726412
0:00:13.215740
0:00:13.216733
tensor(3.9078, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.9078, device='cuda:0')
0:00:13.218716
0:00:13.237109
0:00:13.237579
0:00:13.354655
0:00:13.355647
0:00:13.997612
0:00:14.003564
0:00:14.476874
0:00:14.477865
tensor(3.3683, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.3683, device='cuda:0')
0:00:14.479850
0:00:14.497706
0:00:14.498203
0:00:14.612316
0:00:14.612812
0:00:15.206664
0:00:15.212122
0:00:15.693489
0:00:15.694482
tensor(2.9365, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.9365, device='cuda:0')
0:00:15.696961
[1/1, 12/47] Training Loss: 3.4042043685913086
0:00:15.716308
0:00:15.716803
0:00:15.844806
0:00:15.845798
0:00:16.466963
0:00:16.472916
0:00:16.963587
0:00:16.965076
tensor(2.7819, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.7819, device='cuda:0')
0:00:16.967060
0:00:16.985909
0:00:16.986404
0:00:17.111431
0:00:17.112425
0:00:17.933181
0:00:17.938637
0:00:18.409540
0:00:18.410534
tensor(2.4266, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.4266, device='cuda:0')
0:00:18.413014
0:00:18.430886
0:00:18.430886
0:00:18.549445
0:00:18.549941
0:00:19.155219
0:00:19.160676
0:00:19.706286
0:00:19.707279
tensor(2.1857, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.1857, device='cuda:0')
0:00:19.709759
[1/1, 15/47] Training Loss: 2.4647350311279297
0:00:19.728609
0:00:19.729105
0:00:19.847682
0:00:19.848178
0:00:20.487192
0:00:20.492648
0:00:20.973400
0:00:20.974392
tensor(2.0886, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.0886, device='cuda:0')
0:00:20.976872
0:00:20.994728
0:00:20.995224
0:00:21.119750
0:00:21.120247
0:00:21.768184
0:00:21.774135
0:00:22.264055
0:00:22.265052
tensor(1.8865, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.8865, device='cuda:0')
0:00:22.267526
0:00:22.285382
0:00:22.285382
0:00:22.412904
0:00:22.413898
0:00:23.038011
0:00:23.043481
0:00:23.523205
0:00:23.524196
tensor(1.7354, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.7354, device='cuda:0')
0:00:23.526679
[1/1, 18/47] Training Loss: 1.9034695625305176
0:00:23.545045
0:00:23.545541
0:00:23.672577
0:00:23.673073
0:00:24.293501
0:00:24.299950
0:00:24.775225
0:00:24.776217
tensor(1.6293, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6293, device='cuda:0')
0:00:24.778202
0:00:24.796058
0:00:24.796058
0:00:24.915630
0:00:24.916125
0:00:25.505005
0:00:25.510460
0:00:25.985261
0:00:25.986749
tensor(1.6023, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6023, device='cuda:0')
0:00:25.988734
0:00:26.006590
0:00:26.006590
0:00:26.125180
0:00:26.125676
0:00:26.717775
0:00:26.723232
0:00:27.191082
0:00:27.192074
tensor(1.4799, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.4799, device='cuda:0')
0:00:27.194553
[1/1, 21/47] Training Loss: 1.5705289840698242
0:00:27.214890
0:00:27.214890
0:00:27.347342
0:00:27.347837
0:00:27.994283
0:00:27.999740
0:00:28.463768
0:00:28.464760
tensor(1.3873, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3873, device='cuda:0')
0:00:28.467240
0:00:28.485592
0:00:28.485592
0:00:28.614089
0:00:28.614585
0:00:29.231289
0:00:29.236745
0:00:29.711531
0:00:29.713516
tensor(1.3882, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3882, device='cuda:0')
0:00:29.715997
0:00:29.739807
0:00:29.740304
0:00:29.859873
0:00:29.860370
0:00:30.478208
0:00:30.483664
0:00:30.965913
0:00:30.966905
tensor(1.2772, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2772, device='cuda:0')
0:00:30.969385
[1/1, 24/47] Training Loss: 1.3508739471435547
0:00:30.987737
0:00:30.988234
0:00:31.111768
0:00:31.112267
0:00:31.762214
0:00:31.769158
0:00:32.303999
0:00:32.304990
tensor(1.2546, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2546, device='cuda:0')
0:00:32.308462
0:00:32.326815
0:00:32.327310
0:00:32.470693
0:00:32.471685
0:00:33.264702
0:00:33.271646
0:00:33.761863
0:00:33.763354
tensor(1.2355, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2355, device='cuda:0')
0:00:33.766329
0:00:33.789642
0:00:33.790137
0:00:33.933018
0:00:33.933514
0:00:34.567567
0:00:34.574016
0:00:35.072329
0:00:35.073321
tensor(1.1627, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1627, device='cuda:0')
0:00:35.076793
[1/1, 27/47] Training Loss: 1.2175912857055664
0:00:35.099115
0:00:35.099610
0:00:35.228111
0:00:35.228607
0:00:35.904326
0:00:35.914246
0:00:36.433724
0:00:36.434716
tensor(1.2078, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2078, device='cuda:0')
0:00:36.436700
0:00:36.455054
0:00:36.455550
0:00:36.582061
0:00:36.582558
0:00:37.212211
0:00:37.218163
0:00:37.711296
0:00:37.712289
tensor(1.1862, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1862, device='cuda:0')
0:00:37.714769
0:00:37.732626
0:00:37.733122
0:00:37.854166
0:00:37.854662
0:00:38.459938
0:00:38.465394
0:00:38.990819
0:00:38.991811
tensor(1.1306, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1306, device='cuda:0')
0:00:38.994290
[1/1, 30/47] Training Loss: 1.1748404502868652
0:00:39.013635
0:00:39.013635
0:00:39.146268
0:00:39.147260
0:00:39.805596
0:00:39.811051
0:00:40.294345
0:00:40.295833
tensor(1.0934, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0934, device='cuda:0')
0:00:40.297817
0:00:40.317657
0:00:40.318153
0:00:40.426301
0:00:40.426797
0:00:41.016200
0:00:41.021655
0:00:41.486709
0:00:41.487702
tensor(1.2078, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2078, device='cuda:0')
0:00:41.490182
0:00:41.508038
0:00:41.508038
0:00:41.631076
0:00:41.631572
0:00:42.274592
0:00:42.280065
0:00:42.765253
0:00:42.766244
tensor(1.0455, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0455, device='cuda:0')
0:00:42.768228
[1/1, 33/47] Training Loss: 1.115565299987793
0:00:42.786595
0:00:42.787091
0:00:42.917574
0:00:42.918567
0:00:43.579065
0:00:43.585032
0:00:44.099548
0:00:44.101037
tensor(1.0264, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0264, device='cuda:0')
0:00:44.104013
0:00:44.121871
0:00:44.122367
0:00:44.233008
0:00:44.233503
0:00:44.898809
0:00:44.904265
0:00:45.370596
0:00:45.371588
tensor(1.0835, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0835, device='cuda:0')
0:00:45.374068
0:00:45.391939
0:00:45.391939
0:00:45.517697
0:00:45.518193
0:00:46.130443
0:00:46.135912
0:00:46.609191
0:00:46.610680
tensor(1.1081, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1081, device='cuda:0')
0:00:46.613159
[1/1, 36/47] Training Loss: 1.0726704597473145
0:00:46.631513
0:00:46.632009
0:00:46.758028
0:00:46.759019
0:00:47.374718
0:00:47.380670
0:00:47.874471
0:00:47.875463
tensor(1.0321, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0321, device='cuda:0')
0:00:47.878440
0:00:47.896382
0:00:47.896878
0:00:48.018416
0:00:48.019408
0:00:48.662887
0:00:48.668343
0:00:49.143653
0:00:49.144645
tensor(1.1028, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1028, device='cuda:0')
0:00:49.146628
0:00:49.163988
0:00:49.164485
0:00:49.280572
0:00:49.281564
0:00:49.909224
0:00:49.915175
0:00:50.401892
0:00:50.403380
tensor(0.9939, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9939, device='cuda:0')
0:00:50.405860
[1/1, 39/47] Training Loss: 1.0429366827011108
0:00:50.424708
0:00:50.425204
0:00:50.537321
0:00:50.537816
0:00:51.262670
0:00:51.268153
0:00:51.762752
0:00:51.763743
tensor(0.9739, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9739, device='cuda:0')
0:00:51.766223
0:00:51.783584
0:00:51.784081
0:00:51.897215
0:00:51.898208
0:00:52.506569
0:00:52.512026
0:00:52.980364
0:00:52.981356
tensor(1.0842, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0842, device='cuda:0')
0:00:52.983837
0:00:53.001198
0:00:53.001198
0:00:53.126720
0:00:53.127711
0:00:53.707681
0:00:53.713633
0:00:54.192886
0:00:54.194373
tensor(1.0496, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0496, device='cuda:0')
0:00:54.196357
[1/1, 42/47] Training Loss: 1.0359089374542236
0:00:54.215282
0:00:54.215282
0:00:54.339300
0:00:54.339796
0:00:54.953992
0:00:54.959945
0:00:55.429268
0:00:55.430260
tensor(1.0117, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0117, device='cuda:0')
0:00:55.432244
0:00:55.450100
0:00:55.450595
0:00:55.568165
0:00:55.569159
0:00:56.176445
0:00:56.181902
0:00:56.650453
0:00:56.651445
tensor(1.0383, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0383, device='cuda:0')
0:00:56.654421
0:00:56.672278
0:00:56.672278
0:00:56.800761
0:00:56.801753
0:00:57.427387
0:00:57.432843
0:00:57.940960
0:00:57.941952
tensor(1.0496, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0496, device='cuda:0')
0:00:57.944432
[1/1, 45/47] Training Loss: 1.033210039138794
0:00:57.970224
0:00:57.970224
0:00:58.090273
0:00:58.091265
0:00:58.684877
0:00:58.690333
0:00:59.182011
0:00:59.183500
tensor(0.9069, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9069, device='cuda:0')
0:00:59.185980
0:00:59.207807
0:00:59.208302
0:00:59.310500
0:00:59.310996
0:01:00.092411
0:01:00.097869
0:01:00.613010
0:01:00.614029
tensor(1.0242, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0242, device='cuda:0')
0:01:00.615985
Testing!
[1/1, 1/8]
[1/1, 2/8]
[1/1, 3/8]
[1/1, 4/8]
[1/1, 5/8]
[1/1, 6/8]
[1/1, 7/8]
[1/1, 8/8]
Testing Loss: 1.0553621053695679
Training and Testing Finished