Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/938] Training Loss: 1.07121399641037
[1/3, 20/938] Training Loss: 1.0369687616825103
[1/3, 30/938] Training Loss: 1.0333403885364532
[1/3, 40/938] Training Loss: 1.0141785562038421
[1/3, 50/938] Training Loss: 0.9643324494361878
[1/3, 60/938] Training Loss: 0.9883775591850281
[1/3, 70/938] Training Loss: 0.9338561117649078
[1/3, 80/938] Training Loss: 0.9476238906383514
[1/3, 90/938] Training Loss: 0.9337220132350922
[1/3, 100/938] Training Loss: 0.93568314909935
[1/3, 110/938] Training Loss: 0.9052875399589538
[1/3, 120/938] Training Loss: 0.8842587292194366
[1/3, 130/938] Training Loss: 0.900249844789505
[1/3, 140/938] Training Loss: 0.8845265865325928
[1/3, 150/938] Training Loss: 0.880464243888855
[1/3, 160/938] Training Loss: 0.8470785677433014
[1/3, 170/938] Training Loss: 0.8307802975177765
[1/3, 180/938] Training Loss: 0.86668381690979
[1/3, 190/938] Training Loss: 0.8245330691337586
[1/3, 200/938] Training Loss: 0.8574039340019226
[1/3, 210/938] Training Loss: 0.8437261939048767
[1/3, 220/938] Training Loss: 0.7961423993110657
[1/3, 230/938] Training Loss: 0.7628312110900879
[1/3, 240/938] Training Loss: 0.7679908633232116
[1/3, 250/938] Training Loss: 0.7641157329082489
[1/3, 260/938] Training Loss: 0.7546307861804962
[1/3, 270/938] Training Loss: 0.7424643456935882
[1/3, 280/938] Training Loss: 0.7491696953773499
[1/3, 290/938] Training Loss: 0.7532852351665497
[1/3, 300/938] Training Loss: 0.7339432895183563
[1/3, 310/938] Training Loss: 0.7296351790428162
[1/3, 320/938] Training Loss: 0.7137564122676849
[1/3, 330/938] Training Loss: 0.6925999879837036
[1/3, 340/938] Training Loss: 0.7006991922855377
[1/3, 350/938] Training Loss: 0.6798196077346802
[1/3, 360/938] Training Loss: 0.6777283549308777
[1/3, 370/938] Training Loss: 0.6652028918266296
[1/3, 380/938] Training Loss: 0.6597790062427521
[1/3, 390/938] Training Loss: 0.6468248724937439
[1/3, 400/938] Training Loss: 0.6217135608196258
[1/3, 410/938] Training Loss: 0.6317407608032226
[1/3, 420/938] Training Loss: 0.5996719360351562
[1/3, 430/938] Training Loss: 0.6097547888755799
[1/3, 440/938] Training Loss: 0.5817206978797913
[1/3, 450/938] Training Loss: 0.5635765999555588
[1/3, 460/938] Training Loss: 0.5609507024288177
[1/3, 470/938] Training Loss: 0.546034774184227
[1/3, 480/938] Training Loss: 0.541970407962799
[1/3, 490/938] Training Loss: 0.5230786323547363
[1/3, 500/938] Training Loss: 0.5148668795824051
[1/3, 510/938] Training Loss: 0.4990487933158875
[1/3, 520/938] Training Loss: 0.48736812472343444
[1/3, 530/938] Training Loss: 0.4880921930074692
[1/3, 540/938] Training Loss: 0.4963992923498154
[1/3, 550/938] Training Loss: 0.47939239740371703
[1/3, 560/938] Training Loss: 0.4904307097196579
[1/3, 570/938] Training Loss: 0.4850163787603378
[1/3, 580/938] Training Loss: 0.46336556375026705
[1/3, 590/938] Training Loss: 0.4761234998703003
[1/3, 600/938] Training Loss: 0.46823698580265044
[1/3, 610/938] Training Loss: 0.4596328228712082
[1/3, 620/938] Training Loss: 0.4604521095752716
[1/3, 630/938] Training Loss: 0.45627316534519197
[1/3, 640/938] Training Loss: 0.44545103907585143
[1/3, 650/938] Training Loss: 0.4491588532924652
[1/3, 660/938] Training Loss: 0.4446903496980667
[1/3, 670/938] Training Loss: 0.43333859741687775
[1/3, 680/938] Training Loss: 0.42574226260185244
[1/3, 690/938] Training Loss: 0.4076663821935654
[1/3, 700/938] Training Loss: 0.42754541635513305
[1/3, 710/938] Training Loss: 0.41385078728199004
[1/3, 720/938] Training Loss: 0.4172458529472351
[1/3, 730/938] Training Loss: 0.39879605174064636
[1/3, 740/938] Training Loss: 0.40595701038837434
[1/3, 750/938] Training Loss: 0.3996798902750015
[1/3, 760/938] Training Loss: 0.40108256638050077
[1/3, 770/938] Training Loss: 0.400553759932518
[1/3, 780/938] Training Loss: 0.4119335263967514
[1/3, 790/938] Training Loss: 0.3880899727344513
[1/3, 800/938] Training Loss: 0.3926114410161972
[1/3, 810/938] Training Loss: 0.40212036669254303
[1/3, 820/938] Training Loss: 0.3822260558605194
[1/3, 830/938] Training Loss: 0.3859579384326935
[1/3, 840/938] Training Loss: 0.38122587502002714
[1/3, 850/938] Training Loss: 0.37336801290512084
[1/3, 860/938] Training Loss: 0.38440080881118777
[1/3, 870/938] Training Loss: 0.3499502092599869
[1/3, 880/938] Training Loss: 0.3673023670911789
[1/3, 890/938] Training Loss: 0.3578122198581696
[1/3, 900/938] Training Loss: 0.3597999602556229
[1/3, 910/938] Training Loss: 0.35275169610977175
[1/3, 920/938] Training Loss: 0.34127227365970614
[1/3, 930/938] Training Loss: 0.33729823529720304
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.34471122225125633
5/5: Training network...
[2/3, 10/938] Training Loss: 0.35014706552028657
[2/3, 20/938] Training Loss: 0.344110494852066
[2/3, 30/938] Training Loss: 0.3390417963266373
[2/3, 40/938] Training Loss: 0.34107171893119814
[2/3, 50/938] Training Loss: 0.3393230438232422
[2/3, 60/938] Training Loss: 0.332277438044548
[2/3, 70/938] Training Loss: 0.32729943096637726
[2/3, 80/938] Training Loss: 0.32489305436611177
[2/3, 90/938] Training Loss: 0.31956675052642824
[2/3, 100/938] Training Loss: 0.31831689476966857
[2/3, 110/938] Training Loss: 0.3105289041996002
[2/3, 120/938] Training Loss: 0.32021674513816833
[2/3, 130/938] Training Loss: 0.3241408884525299
[2/3, 140/938] Training Loss: 0.3080969095230103
[2/3, 150/938] Training Loss: 0.30589690506458284
[2/3, 160/938] Training Loss: 0.30611102283000946
[2/3, 170/938] Training Loss: 0.3051499605178833
[2/3, 180/938] Training Loss: 0.30321497917175294
[2/3, 190/938] Training Loss: 0.303088641166687
[2/3, 200/938] Training Loss: 0.3016413986682892
[2/3, 210/938] Training Loss: 0.3021962881088257
[2/3, 220/938] Training Loss: 0.2909981608390808
[2/3, 230/938] Training Loss: 0.28254532516002656
[2/3, 240/938] Training Loss: 0.2897317260503769
[2/3, 250/938] Training Loss: 0.2800398603081703
[2/3, 260/938] Training Loss: 0.2831694781780243
[2/3, 270/938] Training Loss: 0.27360323518514634
[2/3, 280/938] Training Loss: 0.27472494840621947
[2/3, 290/938] Training Loss: 0.2714847922325134
[2/3, 300/938] Training Loss: 0.2719356566667557
[2/3, 310/938] Training Loss: 0.26999076306819914
[2/3, 320/938] Training Loss: 0.2701508939266205
[2/3, 330/938] Training Loss: 0.26023284196853635
[2/3, 340/938] Training Loss: 0.26210456639528273
[2/3, 350/938] Training Loss: 0.2551661401987076
[2/3, 360/938] Training Loss: 0.2610335573554039
[2/3, 370/938] Training Loss: 0.26303827315568923
[2/3, 380/938] Training Loss: 0.26308487057685853
[2/3, 390/938] Training Loss: 0.26239594966173174
[2/3, 400/938] Training Loss: 0.25587399750947953
[2/3, 410/938] Training Loss: 0.2592331230640411
[2/3, 420/938] Training Loss: 0.25015298277139664
[2/3, 430/938] Training Loss: 0.2544463396072388
[2/3, 440/938] Training Loss: 0.24842168837785722
[2/3, 450/938] Training Loss: 0.23969003707170486
[2/3, 460/938] Training Loss: 0.23632387518882753
[2/3, 470/938] Training Loss: 0.24521870017051697
[2/3, 480/938] Training Loss: 0.24576271176338196
[2/3, 490/938] Training Loss: 0.24041277319192886
[2/3, 500/938] Training Loss: 0.24241152703762053
[2/3, 510/938] Training Loss: 0.24266002029180528
[2/3, 520/938] Training Loss: 0.23923058360815047
[2/3, 530/938] Training Loss: 0.23731042295694352
[2/3, 540/938] Training Loss: 0.23775998651981353
[2/3, 550/938] Training Loss: 0.2330867052078247
[2/3, 560/938] Training Loss: 0.22755885273218154
[2/3, 570/938] Training Loss: 0.23668409436941146
[2/3, 580/938] Training Loss: 0.22910777777433394
[2/3, 590/938] Training Loss: 0.23311653584241868
[2/3, 600/938] Training Loss: 0.22583957016468048
[2/3, 610/938] Training Loss: 0.22605548352003096
[2/3, 620/938] Training Loss: 0.22560283541679382
[2/3, 630/938] Training Loss: 0.23273501694202423
[2/3, 640/938] Training Loss: 0.23003736585378648
[2/3, 650/938] Training Loss: 0.22499143332242966
[2/3, 660/938] Training Loss: 0.22876311242580413
[2/3, 670/938] Training Loss: 0.22414416670799256
[2/3, 680/938] Training Loss: 0.21949713677167892
[2/3, 690/938] Training Loss: 0.20979458093643188
[2/3, 700/938] Training Loss: 0.22373296171426774
[2/3, 710/938] Training Loss: 0.22151696532964707
[2/3, 720/938] Training Loss: 0.22038596868515015
[2/3, 730/938] Training Loss: 0.22102145701646805
[2/3, 740/938] Training Loss: 0.21876883655786514
[2/3, 750/938] Training Loss: 0.2118586391210556
[2/3, 760/938] Training Loss: 0.21444271951913835
[2/3, 770/938] Training Loss: 0.21292243748903275
[2/3, 780/938] Training Loss: 0.21500132381916046
[2/3, 790/938] Training Loss: 0.2142355725169182
[2/3, 800/938] Training Loss: 0.21338772028684616
[2/3, 810/938] Training Loss: 0.21303111016750337
[2/3, 820/938] Training Loss: 0.2065669149160385
[2/3, 830/938] Training Loss: 0.21035876423120498
[2/3, 840/938] Training Loss: 0.21168596744537355
[2/3, 850/938] Training Loss: 0.2108261615037918
[2/3, 860/938] Training Loss: 0.21115820556879045
[2/3, 870/938] Training Loss: 0.2061061903834343
[2/3, 880/938] Training Loss: 0.20385462492704393
[2/3, 890/938] Training Loss: 0.20364185720682143
[2/3, 900/938] Training Loss: 0.20645618438720703
[2/3, 910/938] Training Loss: 0.20519760847091675
[2/3, 920/938] Training Loss: 0.20814081877470017
[2/3, 930/938] Training Loss: 0.20907028019428253
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.27360369066397355
5/5: Training network...
[3/3, 10/938] Training Loss: 0.2048996314406395
[3/3, 20/938] Training Loss: 0.20668134987354278
[3/3, 30/938] Training Loss: 0.20117996633052826
[3/3, 40/938] Training Loss: 0.20657212883234025
[3/3, 50/938] Training Loss: 0.20266361385583878
[3/3, 60/938] Training Loss: 0.1985209196805954
[3/3, 70/938] Training Loss: 0.202310049533844
[3/3, 80/938] Training Loss: 0.2023065507411957
[3/3, 90/938] Training Loss: 0.19871219098567963
[3/3, 100/938] Training Loss: 0.20579203814268113
[3/3, 110/938] Training Loss: 0.20285995602607726
[3/3, 120/938] Training Loss: 0.203509184718132
[3/3, 130/938] Training Loss: 0.19899172335863113
[3/3, 140/938] Training Loss: 0.20192182511091233
[3/3, 150/938] Training Loss: 0.19733030200004578
[3/3, 160/938] Training Loss: 0.1939518094062805
[3/3, 170/938] Training Loss: 0.19636681228876113
[3/3, 180/938] Training Loss: 0.20153268724679946
[3/3, 190/938] Training Loss: 0.20018945187330245
[3/3, 200/938] Training Loss: 0.19990266263484954
[3/3, 210/938] Training Loss: 0.19721054136753083
[3/3, 220/938] Training Loss: 0.18974412530660628
[3/3, 230/938] Training Loss: 0.20230292975902558
[3/3, 240/938] Training Loss: 0.19724024832248688
[3/3, 250/938] Training Loss: 0.19438788741827012
[3/3, 260/938] Training Loss: 0.19401121288537979
[3/3, 270/938] Training Loss: 0.19413405656814575
[3/3, 280/938] Training Loss: 0.19459540843963624
[3/3, 290/938] Training Loss: 0.1908253848552704
[3/3, 300/938] Training Loss: 0.2010540634393692
[3/3, 310/938] Training Loss: 0.19586455225944518
[3/3, 320/938] Training Loss: 0.1892663538455963
[3/3, 330/938] Training Loss: 0.19385029673576354
[3/3, 340/938] Training Loss: 0.19014090150594712
[3/3, 350/938] Training Loss: 0.19120851308107376
[3/3, 360/938] Training Loss: 0.1856430485844612
[3/3, 370/938] Training Loss: 0.18940094262361526
[3/3, 380/938] Training Loss: 0.19537680298089982
[3/3, 390/938] Training Loss: 0.19131553173065186
[3/3, 400/938] Training Loss: 0.18996599018573762
[3/3, 410/938] Training Loss: 0.1926218345761299
[3/3, 420/938] Training Loss: 0.1870160847902298
[3/3, 430/938] Training Loss: 0.18992527574300766
[3/3, 440/938] Training Loss: 0.19600946307182313
[3/3, 450/938] Training Loss: 0.18428170382976533
[3/3, 460/938] Training Loss: 0.18979224562644958
[3/3, 470/938] Training Loss: 0.19274918586015702
[3/3, 480/938] Training Loss: 0.19025176912546157
[3/3, 490/938] Training Loss: 0.18745161294937135
[3/3, 500/938] Training Loss: 0.19073788821697235
[3/3, 510/938] Training Loss: 0.19017328470945358
[3/3, 520/938] Training Loss: 0.1854836791753769
[3/3, 530/938] Training Loss: 0.19639813750982285
[3/3, 540/938] Training Loss: 0.18687620311975478
[3/3, 550/938] Training Loss: 0.1925822302699089
[3/3, 560/938] Training Loss: 0.19192061424255372
[3/3, 570/938] Training Loss: 0.18719136118888854
[3/3, 580/938] Training Loss: 0.18636075258255005
[3/3, 590/938] Training Loss: 0.19030682742595673
[3/3, 600/938] Training Loss: 0.18928920179605485
[3/3, 610/938] Training Loss: 0.19136791676282883
[3/3, 620/938] Training Loss: 0.1817727729678154
[3/3, 630/938] Training Loss: 0.17964806258678437
[3/3, 640/938] Training Loss: 0.1870136633515358
[3/3, 650/938] Training Loss: 0.1866261973977089
[3/3, 660/938] Training Loss: 0.18177547305822372
[3/3, 670/938] Training Loss: 0.1843224361538887
[3/3, 680/938] Training Loss: 0.18903491050004959
[3/3, 690/938] Training Loss: 0.18604433238506318
[3/3, 700/938] Training Loss: 0.1844098150730133
[3/3, 710/938] Training Loss: 0.18235109746456146
[3/3, 720/938] Training Loss: 0.18112181574106218
[3/3, 730/938] Training Loss: 0.18505855202674865
[3/3, 740/938] Training Loss: 0.1882645457983017
[3/3, 750/938] Training Loss: 0.18621488958597182
[3/3, 760/938] Training Loss: 0.1822453960776329
[3/3, 770/938] Training Loss: 0.1854420319199562
[3/3, 780/938] Training Loss: 0.1768226683139801
[3/3, 790/938] Training Loss: 0.177840393781662
[3/3, 800/938] Training Loss: 0.18192004561424255
[3/3, 810/938] Training Loss: 0.17798527479171752
[3/3, 820/938] Training Loss: 0.18487063348293303
[3/3, 830/938] Training Loss: 0.18380578905344008
[3/3, 840/938] Training Loss: 0.1907620519399643
[3/3, 850/938] Training Loss: 0.1824830025434494
[3/3, 860/938] Training Loss: 0.1892126500606537
[3/3, 870/938] Training Loss: 0.18544464856386184
[3/3, 880/938] Training Loss: 0.17970044910907745
[3/3, 890/938] Training Loss: 0.18120679408311843
[3/3, 900/938] Training Loss: 0.1818794846534729
[3/3, 910/938] Training Loss: 0.17946474403142929
[3/3, 920/938] Training Loss: 0.18097113966941833
[3/3, 930/938] Training Loss: 0.18406902402639388
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.24325847625732425
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
0 added
3 added
6 added
7 added
1 added
9 added
8 added
4 added
5 added
2 added
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(