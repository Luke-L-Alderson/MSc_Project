Starting Sweep: Batch Size: 64, Learning Rate: 0.001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-12 18:26:13.535466
Training - 2024-06-12 18:26:13.535961
[1/3, 10/94] Training Loss: 247.51 - Iteration Time: 0:00:01.318838
[1/3, 20/94] Training Loss: 171.24 - Iteration Time: 0:00:01.273327
[1/3, 30/94] Training Loss: 172.49 - Iteration Time: 0:00:01.209089
[1/3, 40/94] Training Loss: 172.15 - Iteration Time: 0:00:01.318889
[1/3, 50/94] Training Loss: 169.06 - Iteration Time: 0:00:01.197697
[1/3, 60/94] Training Loss: 164.70 - Iteration Time: 0:00:01.228462
[1/3, 70/94] Training Loss: 159.81 - Iteration Time: 0:00:01.198179
[1/3, 80/94] Training Loss: 159.38 - Iteration Time: 0:00:01.242841
[1/3, 90/94] Training Loss: 152.12 - Iteration Time: 0:00:01.202725
Testing - 2024-06-12 18:28:13.035024
[1/3, 2/16]
[1/3, 4/16]
[1/3, 6/16]
[1/3, 8/16]
[1/3, 10/16]
[1/3, 12/16]
[1/3, 14/16]
[1/3, 16/16]
Testing Loss: 145.09 - Epoch Time: 0:02:10.901307
Training - 2024-06-12 18:28:24.437268
[2/3, 10/94] Training Loss: 149.82 - Iteration Time: 0:00:01.229376
[2/3, 20/94] Training Loss: 140.04 - Iteration Time: 0:00:01.271583
[2/3, 30/94] Training Loss: 138.66 - Iteration Time: 0:00:01.336574
[2/3, 40/94] Training Loss: 136.91 - Iteration Time: 0:00:01.352927
[2/3, 50/94] Training Loss: 132.88 - Iteration Time: 0:00:01.298323
[2/3, 60/94] Training Loss: 134.27 - Iteration Time: 0:00:01.258174
[2/3, 70/94] Training Loss: 130.11 - Iteration Time: 0:00:01.327181
[2/3, 80/94] Training Loss: 125.16 - Iteration Time: 0:00:01.360997
[2/3, 90/94] Training Loss: 126.43 - Iteration Time: 0:00:01.253282
Testing - 2024-06-12 18:30:27.554273
[2/3, 2/16]
[2/3, 4/16]
[2/3, 6/16]
[2/3, 8/16]
[2/3, 10/16]
[2/3, 12/16]
[2/3, 14/16]
[2/3, 16/16]
Testing Loss: 109.36 - Epoch Time: 0:02:14.878143
Training - 2024-06-12 18:30:39.315411
[3/3, 10/94] Training Loss: 122.89 - Iteration Time: 0:00:01.304343
[3/3, 20/94] Training Loss: 123.51 - Iteration Time: 0:00:01.275529
[3/3, 30/94] Training Loss: 117.03 - Iteration Time: 0:00:01.261239
[3/3, 40/94] Training Loss: 118.44 - Iteration Time: 0:00:01.282086
[3/3, 50/94] Training Loss: 121.57 - Iteration Time: 0:00:01.291398
[3/3, 60/94] Training Loss: 117.14 - Iteration Time: 0:00:01.356867
[3/3, 70/94] Training Loss: 111.48 - Iteration Time: 0:00:01.523575
[3/3, 80/94] Training Loss: 112.22 - Iteration Time: 0:00:01.277116
[3/3, 90/94] Training Loss: 110.89 - Iteration Time: 0:00:01.285622
Testing - 2024-06-12 18:32:42.866354
[3/3, 2/16]
[3/3, 4/16]
[3/3, 6/16]
[3/3, 8/16]
[3/3, 10/16]
[3/3, 12/16]
[3/3, 14/16]
[3/3, 16/16]
Testing Loss: 109.02 - Epoch Time: 0:02:15.571394
Training and Testing Finished - Time: 0:06:41.351834
Assembling test data for t-sne projection
-- 10/16 --
Plotting Results Grid
WARNING    c:\users\lukea\documents\masters project code\msc_project\main.py:184: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
 [py.warnings]
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 1
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - {labels[input_index]}