Starting Sweep: Batch Size: 32, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 18000
Testing: 10000 -> 3000
Making Subsets
Training: 18000
Testing: 3000
Making Dataloaders
Defining network
2024-06-05 23:21:09.308830
Training!
0:00:00.009424
[1/9, 29/563] Training Loss: 1.0766072273254395
[1/9, 58/563] Training Loss: 1.0112900733947754
[1/9, 87/563] Training Loss: 0.9965663552284241
[1/9, 116/563] Training Loss: 0.9584885239601135
[1/9, 145/563] Training Loss: 0.946432888507843
[1/9, 174/563] Training Loss: 0.903574526309967
[1/9, 203/563] Training Loss: 0.8719881772994995
[1/9, 232/563] Training Loss: 0.8293259739875793
[1/9, 261/563] Training Loss: 0.821732759475708
[1/9, 290/563] Training Loss: 0.8000473976135254
[1/9, 319/563] Training Loss: 0.7817688584327698
[1/9, 348/563] Training Loss: 0.7674552798271179
[1/9, 377/563] Training Loss: 0.7597808241844177
[1/9, 406/563] Training Loss: 0.7377889752388
[1/9, 435/563] Training Loss: 0.7200274467468262
[1/9, 464/563] Training Loss: 0.723659336566925
[1/9, 493/563] Training Loss: 0.7092558145523071
[1/9, 522/563] Training Loss: 0.6880173683166504
[1/9, 551/563] Training Loss: 0.6774374842643738
Testing!
[1/9, 1/94]
[1/9, 6/94]
[1/9, 11/94]
[1/9, 16/94]
[1/9, 21/94]
[1/9, 26/94]
[1/9, 31/94]
[1/9, 36/94]
[1/9, 41/94]
[1/9, 46/94]
[1/9, 51/94]
[1/9, 56/94]
[1/9, 61/94]
[1/9, 66/94]
[1/9, 71/94]
[1/9, 76/94]
[1/9, 81/94]
[1/9, 86/94]
[1/9, 91/94]
Testing Loss: 0.6417250037193298
Training!
0:12:02.000794
[2/9, 29/563] Training Loss: 0.6716098189353943
[2/9, 58/563] Training Loss: 0.6434218883514404
[2/9, 87/563] Training Loss: 0.6451537013053894
[2/9, 116/563] Training Loss: 0.6403859257698059
[2/9, 145/563] Training Loss: 0.6385337710380554
[2/9, 174/563] Training Loss: 0.6108608841896057
[2/9, 203/563] Training Loss: 0.5972533226013184
[2/9, 232/563] Training Loss: 0.6014514565467834
[2/9, 261/563] Training Loss: 0.6016247272491455
[2/9, 290/563] Training Loss: 0.5816516280174255
[2/9, 319/563] Training Loss: 0.5666314363479614
[2/9, 348/563] Training Loss: 0.5527195334434509
[2/9, 377/563] Training Loss: 0.5694759488105774
[2/9, 406/563] Training Loss: 0.5609214901924133
[2/9, 435/563] Training Loss: 0.5422142744064331
[2/9, 464/563] Training Loss: 0.5392165184020996
[2/9, 493/563] Training Loss: 0.5305664539337158
[2/9, 522/563] Training Loss: 0.5389642715454102
[2/9, 551/563] Training Loss: 0.5259665846824646
Testing!
[2/9, 1/94]
[2/9, 6/94]
[2/9, 11/94]
[2/9, 16/94]
[2/9, 21/94]
[2/9, 26/94]
[2/9, 31/94]
[2/9, 36/94]
[2/9, 41/94]
[2/9, 46/94]
[2/9, 51/94]
[2/9, 56/94]
[2/9, 61/94]
[2/9, 66/94]
[2/9, 71/94]
[2/9, 76/94]
[2/9, 81/94]
[2/9, 86/94]
[2/9, 91/94]
Testing Loss: 0.5228500962257385
Training!
0:24:04.693992
[3/9, 29/563] Training Loss: 0.5225079655647278
[3/9, 58/563] Training Loss: 0.4889383316040039
[3/9, 87/563] Training Loss: 0.504365086555481
[3/9, 116/563] Training Loss: 0.48315149545669556
[3/9, 145/563] Training Loss: 0.4801200330257416
[3/9, 174/563] Training Loss: 0.47705772519111633
[3/9, 203/563] Training Loss: 0.47337785363197327
[3/9, 232/563] Training Loss: 0.4709443151950836
[3/9, 261/563] Training Loss: 0.4558326005935669
[3/9, 290/563] Training Loss: 0.45927339792251587
[3/9, 319/563] Training Loss: 0.45738768577575684
[3/9, 348/563] Training Loss: 0.4424785077571869
[3/9, 377/563] Training Loss: 0.44123971462249756
[3/9, 406/563] Training Loss: 0.43056178092956543
[3/9, 435/563] Training Loss: 0.43194684386253357
[3/9, 464/563] Training Loss: 0.4217863082885742
[3/9, 493/563] Training Loss: 0.42109209299087524
[3/9, 522/563] Training Loss: 0.41937199234962463
[3/9, 551/563] Training Loss: 0.41880491375923157
Testing!
[3/9, 1/94]
[3/9, 6/94]
[3/9, 11/94]
[3/9, 16/94]
[3/9, 21/94]
[3/9, 26/94]
[3/9, 31/94]
[3/9, 36/94]
[3/9, 41/94]
[3/9, 46/94]
[3/9, 51/94]
[3/9, 56/94]
[3/9, 61/94]
[3/9, 66/94]
[3/9, 71/94]
[3/9, 76/94]
[3/9, 81/94]
[3/9, 86/94]
[3/9, 91/94]
Testing Loss: 0.412433922290802
Training!
0:36:05.026439
[4/9, 29/563] Training Loss: 0.4227917790412903
[4/9, 58/563] Training Loss: 0.4110719859600067
[4/9, 87/563] Training Loss: 0.40027669072151184
[4/9, 116/563] Training Loss: 0.40368935465812683
[4/9, 145/563] Training Loss: 0.39910051226615906
[4/9, 174/563] Training Loss: 0.3882538676261902
[4/9, 203/563] Training Loss: 0.3833707869052887
[4/9, 232/563] Training Loss: 0.37372681498527527
[4/9, 261/563] Training Loss: 0.3839602768421173
[4/9, 290/563] Training Loss: 0.3739570677280426
[4/9, 319/563] Training Loss: 0.3712432384490967
[4/9, 348/563] Training Loss: 0.3629808723926544
[4/9, 377/563] Training Loss: 0.355509877204895
[4/9, 406/563] Training Loss: 0.3545967936515808
[4/9, 435/563] Training Loss: 0.357624888420105
[4/9, 464/563] Training Loss: 0.3455076515674591
[4/9, 493/563] Training Loss: 0.3407265841960907
[4/9, 522/563] Training Loss: 0.33914634585380554
[4/9, 551/563] Training Loss: 0.3444744944572449
Testing!
[4/9, 1/94]
[4/9, 6/94]
[4/9, 11/94]
[4/9, 16/94]
[4/9, 21/94]
[4/9, 26/94]
[4/9, 31/94]
[4/9, 36/94]
[4/9, 41/94]
[4/9, 46/94]
[4/9, 51/94]
[4/9, 56/94]
[4/9, 61/94]
[4/9, 66/94]
[4/9, 71/94]
[4/9, 76/94]
[4/9, 81/94]
[4/9, 86/94]
[4/9, 91/94]
Testing Loss: 0.3409094512462616
Training!
0:48:09.084587
[5/9, 29/563] Training Loss: 0.34633541107177734
[5/9, 58/563] Training Loss: 0.3339919447898865
[5/9, 87/563] Training Loss: 0.3340776264667511
[5/9, 116/563] Training Loss: 0.3323739171028137
[5/9, 145/563] Training Loss: 0.318134069442749
[5/9, 174/563] Training Loss: 0.3127603232860565
[5/9, 203/563] Training Loss: 0.31125175952911377
[5/9, 232/563] Training Loss: 0.3097551167011261
[5/9, 261/563] Training Loss: 0.30821704864501953
[5/9, 290/563] Training Loss: 0.3097321391105652
[5/9, 319/563] Training Loss: 0.3009119927883148
[5/9, 348/563] Training Loss: 0.3020658791065216
[5/9, 377/563] Training Loss: 0.3003796935081482
[5/9, 406/563] Training Loss: 0.30333712697029114
[5/9, 435/563] Training Loss: 0.2921020984649658
[5/9, 464/563] Training Loss: 0.2944599688053131
[5/9, 493/563] Training Loss: 0.29135969281196594
[5/9, 522/563] Training Loss: 0.29122212529182434
[5/9, 551/563] Training Loss: 0.2889419198036194
Testing!
[5/9, 1/94]
[5/9, 6/94]
[5/9, 11/94]
[5/9, 16/94]
[5/9, 21/94]
[5/9, 26/94]
[5/9, 31/94]
[5/9, 36/94]
[5/9, 41/94]
[5/9, 46/94]
[5/9, 51/94]
[5/9, 56/94]
[5/9, 61/94]
[5/9, 66/94]
[5/9, 71/94]
[5/9, 76/94]
[5/9, 81/94]
[5/9, 86/94]
[5/9, 91/94]
Testing Loss: 0.3002423346042633
Training!
1:00:09.948719
[6/9, 29/563] Training Loss: 0.2847834825515747
[6/9, 58/563] Training Loss: 0.2784653902053833
[6/9, 87/563] Training Loss: 0.28313976526260376
[6/9, 116/563] Training Loss: 0.27712613344192505
[6/9, 145/563] Training Loss: 0.2716313898563385
[6/9, 174/563] Training Loss: 0.2834640443325043
[6/9, 203/563] Training Loss: 0.27503716945648193
[6/9, 232/563] Training Loss: 0.27193987369537354
[6/9, 261/563] Training Loss: 0.2655610144138336
[6/9, 290/563] Training Loss: 0.26961323618888855
[6/9, 319/563] Training Loss: 0.2688165605068207
[6/9, 348/563] Training Loss: 0.26404324173927307
[6/9, 377/563] Training Loss: 0.2669042944908142
[6/9, 406/563] Training Loss: 0.2676142752170563
[6/9, 435/563] Training Loss: 0.26151132583618164
[6/9, 464/563] Training Loss: 0.25818341970443726
[6/9, 493/563] Training Loss: 0.26349499821662903
[6/9, 522/563] Training Loss: 0.2584359645843506
[6/9, 551/563] Training Loss: 0.2624310553073883
Testing!
[6/9, 1/94]
[6/9, 6/94]
[6/9, 11/94]
[6/9, 16/94]
[6/9, 21/94]
[6/9, 26/94]
[6/9, 31/94]
[6/9, 36/94]
[6/9, 41/94]
[6/9, 46/94]
[6/9, 51/94]
[6/9, 56/94]
[6/9, 61/94]
[6/9, 66/94]
[6/9, 71/94]
[6/9, 76/94]
[6/9, 81/94]
[6/9, 86/94]
[6/9, 91/94]
Testing Loss: 0.2415899783372879
Training!
1:12:11.940446
[7/9, 29/563] Training Loss: 0.2531217336654663
[7/9, 58/563] Training Loss: 0.254173219203949
[7/9, 87/563] Training Loss: 0.25748559832572937
[7/9, 116/563] Training Loss: 0.24694132804870605
[7/9, 145/563] Training Loss: 0.2508823871612549
[7/9, 174/563] Training Loss: 0.25159958004951477
[7/9, 203/563] Training Loss: 0.24662430584430695
[7/9, 232/563] Training Loss: 0.2478628009557724
[7/9, 261/563] Training Loss: 0.24747183918952942
[7/9, 290/563] Training Loss: 0.2478761076927185
[7/9, 319/563] Training Loss: 0.24522148072719574
[7/9, 348/563] Training Loss: 0.25122299790382385
[7/9, 377/563] Training Loss: 0.24998719990253448
[7/9, 406/563] Training Loss: 0.24113549292087555
[7/9, 435/563] Training Loss: 0.24317839741706848
[7/9, 464/563] Training Loss: 0.23783761262893677
[7/9, 493/563] Training Loss: 0.24750104546546936
[7/9, 522/563] Training Loss: 0.24205711483955383
[7/9, 551/563] Training Loss: 0.24272435903549194
Testing!
[7/9, 1/94]
[7/9, 6/94]
[7/9, 11/94]
[7/9, 16/94]
[7/9, 21/94]
[7/9, 26/94]
[7/9, 31/94]
[7/9, 36/94]
[7/9, 41/94]
[7/9, 46/94]
[7/9, 51/94]
[7/9, 56/94]
[7/9, 61/94]
[7/9, 66/94]
[7/9, 71/94]
[7/9, 76/94]
[7/9, 81/94]
[7/9, 86/94]
[7/9, 91/94]
Testing Loss: 0.22970305383205414
Training!
1:24:15.400481
[8/9, 29/563] Training Loss: 0.2403429001569748
[8/9, 58/563] Training Loss: 0.2397305965423584
[8/9, 87/563] Training Loss: 0.23162487149238586
[8/9, 116/563] Training Loss: 0.2378782033920288
[8/9, 145/563] Training Loss: 0.2364490032196045
[8/9, 174/563] Training Loss: 0.24095219373703003
[8/9, 203/563] Training Loss: 0.23479905724525452
[8/9, 232/563] Training Loss: 0.23355084657669067
[8/9, 261/563] Training Loss: 0.22855550050735474
[8/9, 290/563] Training Loss: 0.2286737561225891
[8/9, 319/563] Training Loss: 0.23396934568881989
[8/9, 348/563] Training Loss: 0.22421328723430634
[8/9, 377/563] Training Loss: 0.22736665606498718
[8/9, 406/563] Training Loss: 0.2281125783920288
[8/9, 435/563] Training Loss: 0.22920797765254974
[8/9, 464/563] Training Loss: 0.2303730696439743
[8/9, 493/563] Training Loss: 0.22501425445079803
[8/9, 522/563] Training Loss: 0.22525456547737122
[8/9, 551/563] Training Loss: 0.22188425064086914
Testing!
[8/9, 1/94]
[8/9, 6/94]
[8/9, 11/94]
[8/9, 16/94]
[8/9, 21/94]
[8/9, 26/94]
[8/9, 31/94]
[8/9, 36/94]
[8/9, 41/94]
[8/9, 46/94]
[8/9, 51/94]
[8/9, 56/94]
[8/9, 61/94]
[8/9, 66/94]
[8/9, 71/94]
[8/9, 76/94]
[8/9, 81/94]
[8/9, 86/94]
[8/9, 91/94]
Testing Loss: 0.22434227168560028
Training!
1:36:13.976825
[9/9, 29/563] Training Loss: 0.2225690484046936
[9/9, 58/563] Training Loss: 0.21813929080963135
[9/9, 87/563] Training Loss: 0.21413728594779968
[9/9, 116/563] Training Loss: 0.22055591642856598
[9/9, 145/563] Training Loss: 0.21802017092704773
[9/9, 174/563] Training Loss: 0.21869491040706635
[9/9, 203/563] Training Loss: 0.2144596129655838
[9/9, 232/563] Training Loss: 0.21807058155536652
[9/9, 261/563] Training Loss: 0.2173604816198349
[9/9, 290/563] Training Loss: 0.2128075659275055
[9/9, 319/563] Training Loss: 0.2127135843038559
[9/9, 348/563] Training Loss: 0.21715503931045532
[9/9, 377/563] Training Loss: 0.21273498237133026
[9/9, 406/563] Training Loss: 0.20831923186779022
[9/9, 435/563] Training Loss: 0.20947769284248352
[9/9, 464/563] Training Loss: 0.20987927913665771
[9/9, 493/563] Training Loss: 0.20927365124225616
[9/9, 522/563] Training Loss: 0.20814462006092072
[9/9, 551/563] Training Loss: 0.2071463167667389
Testing!
[9/9, 1/94]
[9/9, 6/94]
[9/9, 11/94]
[9/9, 16/94]
[9/9, 21/94]
[9/9, 26/94]
[9/9, 31/94]
[9/9, 36/94]
[9/9, 41/94]
[9/9, 46/94]
[9/9, 51/94]
[9/9, 56/94]
[9/9, 61/94]
[9/9, 66/94]
[9/9, 71/94]
[9/9, 76/94]
[9/9, 81/94]
[9/9, 86/94]
[9/9, 91/94]
Testing Loss: 0.19880874454975128
Training and Testing Finished
Assembling test data for t-sne projection
-- 1/94 --
-- 2/94 --
-- 3/94 --
-- 4/94 --
-- 5/94 --
-- 6/94 --
-- 7/94 --
-- 8/94 --
-- 9/94 --
-- 10/94 --
-- 11/94 --
-- 12/94 --
-- 13/94 --
-- 14/94 --
-- 15/94 --
-- 16/94 --
-- 17/94 --
-- 18/94 --
-- 19/94 --
-- 20/94 --
-- 21/94 --
-- 22/94 --
-- 23/94 --
-- 24/94 --
-- 25/94 --
-- 26/94 --
-- 27/94 --
-- 28/94 --
-- 29/94 --
-- 30/94 --
-- 31/94 --
-- 32/94 --
-- 33/94 --
-- 34/94 --
-- 35/94 --
-- 36/94 --
-- 37/94 --
-- 38/94 --
-- 39/94 --
-- 40/94 --
-- 41/94 --
-- 42/94 --
-- 43/94 --
-- 44/94 --
-- 45/94 --
-- 46/94 --
-- 47/94 --
-- 48/94 --
-- 49/94 --
-- 50/94 --
-- 51/94 --
-- 52/94 --
-- 53/94 --
-- 54/94 --
-- 55/94 --
-- 56/94 --
-- 57/94 --
-- 58/94 --
-- 59/94 --
-- 60/94 --
-- 61/94 --
-- 62/94 --
-- 63/94 --
-- 64/94 --
-- 65/94 --
-- 66/94 --
-- 67/94 --
-- 68/94 --
-- 69/94 --
-- 70/94 --
-- 71/94 --
-- 72/94 --
-- 73/94 --
-- 74/94 --
-- 75/94 --
-- 76/94 --
-- 77/94 --
-- 78/94 --
-- 79/94 --
-- 80/94 --
-- 81/94 --
-- 82/94 --
-- 83/94 --
-- 84/94 --
-- 85/94 --
-- 86/94 --
-- 87/94 --
-- 88/94 --
-- 89/94 --
-- 90/94 --
-- 91/94 --
-- 92/94 --
-- 93/94 --
-- 94/94 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation