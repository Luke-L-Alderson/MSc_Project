Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-18 01:32:38.658084
Scaler Value: 1000
Training - 2024-06-18 01:32:38.659076
[1/9, 10/94] Training Loss: 10289.4945 - Iteration Time: 0:00:01.456567
[1/9, 20/94] Training Loss: 10024.1756 - Iteration Time: 0:00:01.401561
[1/9, 30/94] Training Loss: 9857.0397 - Iteration Time: 0:00:01.394107
[1/9, 40/94] Training Loss: 9683.7354 - Iteration Time: 0:00:01.391172
[1/9, 50/94] Training Loss: 9549.4266 - Iteration Time: 0:00:01.378224
[1/9, 60/94] Training Loss: 9410.0364 - Iteration Time: 0:00:01.415909
[1/9, 70/94] Training Loss: 9297.2900 - Iteration Time: 0:00:01.389622
[1/9, 80/94] Training Loss: 9231.1503 - Iteration Time: 0:00:01.396113
[1/9, 90/94] Training Loss: 9185.5927 - Iteration Time: 0:00:01.442709
Testing - 2024-06-18 01:34:53.224505
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 9138.0645 - Epoch Time: 0:02:27.987416
Training - 2024-06-18 01:35:06.646492
[2/9, 10/94] Training Loss: 9117.4030 - Iteration Time: 0:00:01.378683
[2/9, 20/94] Training Loss: 9049.8011 - Iteration Time: 0:00:01.467558
[2/9, 30/94] Training Loss: 8979.9696 - Iteration Time: 0:00:01.380678
[2/9, 40/94] Training Loss: 8884.0217 - Iteration Time: 0:00:01.548851
[2/9, 50/94] Training Loss: 8749.8432 - Iteration Time: 0:00:01.405981
[2/9, 60/94] Training Loss: 8649.5784 - Iteration Time: 0:00:01.415976
[2/9, 70/94] Training Loss: 8559.2212 - Iteration Time: 0:00:01.359347
[2/9, 80/94] Training Loss: 8429.2952 - Iteration Time: 0:00:01.399027
[2/9, 90/94] Training Loss: 8316.1801 - Iteration Time: 0:00:01.406498
Testing - 2024-06-18 01:37:19.794007
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 8180.8887 - Epoch Time: 0:02:26.424322
Training - 2024-06-18 01:37:33.070814
[3/9, 10/94] Training Loss: 8137.9499 - Iteration Time: 0:00:01.378710
[3/9, 20/94] Training Loss: 8040.7749 - Iteration Time: 0:00:01.445652
[3/9, 30/94] Training Loss: 7977.2994 - Iteration Time: 0:00:01.398031
[3/9, 40/94] Training Loss: 7935.0782 - Iteration Time: 0:00:01.392156
[3/9, 50/94] Training Loss: 7888.6660 - Iteration Time: 0:00:01.426833
[3/9, 60/94] Training Loss: 7816.9580 - Iteration Time: 0:00:01.381190
[3/9, 70/94] Training Loss: 7735.8467 - Iteration Time: 0:00:01.394600
[3/9, 80/94] Training Loss: 7658.6824 - Iteration Time: 0:00:01.501262
[3/9, 90/94] Training Loss: 7599.2861 - Iteration Time: 0:00:01.408494
Testing - 2024-06-18 01:39:45.856294
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 7535.2878 - Epoch Time: 0:02:25.958162
Training - 2024-06-18 01:39:59.029983
[4/9, 10/94] Training Loss: 7511.5475 - Iteration Time: 0:00:01.429295
[4/9, 20/94] Training Loss: 7441.6648 - Iteration Time: 0:00:01.460619
[4/9, 30/94] Training Loss: 7382.3579 - Iteration Time: 0:00:01.381223
[4/9, 40/94] Training Loss: 7340.6552 - Iteration Time: 0:00:01.393619
[4/9, 50/94] Training Loss: 7243.0781 - Iteration Time: 0:00:01.423356
[4/9, 60/94] Training Loss: 7157.6108 - Iteration Time: 0:00:01.391693
[4/9, 70/94] Training Loss: 7084.7291 - Iteration Time: 0:00:01.395603
[4/9, 80/94] Training Loss: 7033.6984 - Iteration Time: 0:00:01.389598
[4/9, 90/94] Training Loss: 6984.3302 - Iteration Time: 0:00:01.400573
Testing - 2024-06-18 01:42:13.380020
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 6932.5396 - Epoch Time: 0:02:27.575018
Training - 2024-06-18 01:42:26.605001
[5/9, 10/94] Training Loss: 6916.5003 - Iteration Time: 0:00:01.388113
[5/9, 20/94] Training Loss: 6894.2767 - Iteration Time: 0:00:01.397577
[5/9, 30/94] Training Loss: 6843.0901 - Iteration Time: 0:00:01.482929
[5/9, 40/94] Training Loss: 6771.0850 - Iteration Time: 0:00:01.392604
[5/9, 50/94] Training Loss: 6683.6081 - Iteration Time: 0:00:01.371242
[5/9, 60/94] Training Loss: 6616.5722 - Iteration Time: 0:00:01.375221
[5/9, 70/94] Training Loss: 6530.0034 - Iteration Time: 0:00:01.477389
[5/9, 80/94] Training Loss: 6480.3046 - Iteration Time: 0:00:01.382151
[5/9, 90/94] Training Loss: 6410.3183 - Iteration Time: 0:00:01.381705
Testing - 2024-06-18 01:44:40.048202
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 6341.5542 - Epoch Time: 0:02:26.655371
Training - 2024-06-18 01:44:53.260372
[6/9, 10/94] Training Loss: 6301.1288 - Iteration Time: 0:00:01.427330
[6/9, 20/94] Training Loss: 6211.8139 - Iteration Time: 0:00:01.372763
[6/9, 30/94] Training Loss: 6112.6864 - Iteration Time: 0:00:01.379254
[6/9, 40/94] Training Loss: 6058.5106 - Iteration Time: 0:00:01.405046
[6/9, 50/94] Training Loss: 6008.5163 - Iteration Time: 0:00:01.423851
[6/9, 60/94] Training Loss: 5953.7462 - Iteration Time: 0:00:01.439732
[6/9, 70/94] Training Loss: 5900.1909 - Iteration Time: 0:00:01.393610
[6/9, 80/94] Training Loss: 5860.2483 - Iteration Time: 0:00:01.410451
[6/9, 90/94] Training Loss: 5837.0091 - Iteration Time: 0:00:01.491881
Testing - 2024-06-18 01:47:07.248040
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 5809.6223 - Epoch Time: 0:02:27.145449
Training - 2024-06-18 01:47:20.405821
[7/9, 10/94] Training Loss: 5781.8722 - Iteration Time: 0:00:01.390110
[7/9, 20/94] Training Loss: 5732.1720 - Iteration Time: 0:00:01.406468
[7/9, 30/94] Training Loss: 5693.2176 - Iteration Time: 0:00:01.421937
[7/9, 40/94] Training Loss: 5665.3376 - Iteration Time: 0:00:01.398114
[7/9, 50/94] Training Loss: 5637.1571 - Iteration Time: 0:00:01.422385
[7/9, 60/94] Training Loss: 5601.5992 - Iteration Time: 0:00:01.401539
[7/9, 70/94] Training Loss: 5585.1522 - Iteration Time: 0:00:01.494818
[7/9, 80/94] Training Loss: 5564.1566 - Iteration Time: 0:00:01.398071
[7/9, 90/94] Training Loss: 5555.7198 - Iteration Time: 0:00:01.410942
Testing - 2024-06-18 01:49:34.907600
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 5539.1465 - Epoch Time: 0:02:27.660987
Training - 2024-06-18 01:49:48.067305
[8/9, 10/94] Training Loss: 5543.1116 - Iteration Time: 0:00:01.427411
[8/9, 20/94] Training Loss: 5505.9629 - Iteration Time: 0:00:01.544474
[8/9, 30/94] Training Loss: 5494.4433 - Iteration Time: 0:00:01.407485
[8/9, 40/94] Training Loss: 5489.8312 - Iteration Time: 0:00:01.368850
[8/9, 50/94] Training Loss: 5449.3256 - Iteration Time: 0:00:01.417928
[8/9, 60/94] Training Loss: 5424.3360 - Iteration Time: 0:00:01.400574
[8/9, 70/94] Training Loss: 5391.3307 - Iteration Time: 0:00:01.419453
[8/9, 80/94] Training Loss: 5347.8397 - Iteration Time: 0:00:01.372287
[8/9, 90/94] Training Loss: 5329.8874 - Iteration Time: 0:00:01.377244
Testing - 2024-06-18 01:52:01.897820
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 5291.1409 - Epoch Time: 0:02:26.952516
Training - 2024-06-18 01:52:15.020317
[9/9, 10/94] Training Loss: 5296.8142 - Iteration Time: 0:00:01.393591
[9/9, 20/94] Training Loss: 5284.1484 - Iteration Time: 0:00:01.419174
[9/9, 30/94] Training Loss: 5275.5449 - Iteration Time: 0:00:01.367890
[9/9, 40/94] Training Loss: 5247.4739 - Iteration Time: 0:00:01.407026
[9/9, 50/94] Training Loss: 5234.3062 - Iteration Time: 0:00:01.716608
[9/9, 60/94] Training Loss: 5201.1446 - Iteration Time: 0:00:01.402505
[9/9, 70/94] Training Loss: 5178.6062 - Iteration Time: 0:00:01.426872
[9/9, 80/94] Training Loss: 5167.9249 - Iteration Time: 0:00:01.406540
[9/9, 90/94] Training Loss: 5161.8217 - Iteration Time: 0:00:01.395749
Testing - 2024-06-18 01:54:28.986904
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 5150.3223 - Epoch Time: 0:02:27.119941
Training and Testing Finished - Time: 0:22:03.482669
torch.Size([200, 64, 1, 28, 28])
torch.Size([200, 64, 1, 28, 28])
Assembling test data for t-sne projection
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 1/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 2/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 3/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 4/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 5/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 6/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 7/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 8/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 9/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 10/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 11/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 12/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 13/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 14/16 --
Code Layer Shape: torch.Size([200, 64, 100])
Code Layer Shape: torch.Size([64, 100])
-- 15/16 --
Code Layer Shape: torch.Size([200, 40, 100])
Code Layer Shape: torch.Size([40, 100])
-- 16/16 --
   Labels    0    1      2    3    4    5  ...   93   94   95   96   97     98   99
0       2  0.0  0.0  0.260  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.090  0.0
1       4  0.0  0.0  0.265  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.085  0.0
2       7  0.0  0.0  0.260  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.085  0.0
3       3  0.0  0.0  0.255  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.085  0.0
4       7  0.0  0.0  0.265  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.090  0.0
[5 rows x 101 columns]
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2
Applying UMAP