Starting Sweep: Batch Size: 32, Learning Rate: 0.001
Making datasets and defining subsets
Training: 60000 -> 36000
Testing: 10000 -> 6000
Making Subsets
Training: 36000
Testing: 6000
Making Dataloaders
Defining network
Training!
[1/3, 57/1125] Training Loss: 1.536170224348704
[1/3, 114/1125] Training Loss: 1.093341930916435
[1/3, 171/1125] Training Loss: 1.04768151881402
[1/3, 228/1125] Training Loss: 1.0466804274341517
[1/3, 285/1125] Training Loss: 0.9795580088046559
[1/3, 342/1125] Training Loss: 0.8903276711179499
[1/3, 399/1125] Training Loss: 0.8305771612284476
[1/3, 456/1125] Training Loss: 0.7810850551253871
[1/3, 513/1125] Training Loss: 0.7599922742759972
[1/3, 570/1125] Training Loss: 0.6939230268461662
[1/3, 627/1125] Training Loss: 0.6807197416037843
[1/3, 684/1125] Training Loss: 0.6413935381069518
[1/3, 741/1125] Training Loss: 0.6398532599733587
[1/3, 798/1125] Training Loss: 0.6360484549873754
[1/3, 855/1125] Training Loss: 0.6272315267930951
[1/3, 912/1125] Training Loss: 0.6264116314419529
[1/3, 969/1125] Training Loss: 0.6609411250080979
[1/3, 1026/1125] Training Loss: 0.636541676103023
[1/3, 1083/1125] Training Loss: 0.6269111643757737
Testing!
[1/3, 1/188]
[1/3, 11/188]
[1/3, 21/188]
[1/3, 31/188]
[1/3, 41/188]
[1/3, 51/188]
[1/3, 61/188]
[1/3, 71/188]
[1/3, 81/188]
[1/3, 91/188]
[1/3, 101/188]
[1/3, 111/188]
[1/3, 121/188]
[1/3, 131/188]
[1/3, 141/188]
[1/3, 151/188]
[1/3, 161/188]
[1/3, 171/188]
[1/3, 181/188]
Testing Loss: 0.5906326871169242
Training!
[2/3, 57/1125] Training Loss: 0.604930809715338
[2/3, 114/1125] Training Loss: 0.6062751255537334
[2/3, 171/1125] Training Loss: 0.60707110793967
[2/3, 228/1125] Training Loss: 0.5962389195174501
[2/3, 285/1125] Training Loss: 0.5993312283566123
[2/3, 342/1125] Training Loss: 0.5974128654128626
[2/3, 399/1125] Training Loss: 0.5798691087647488
[2/3, 456/1125] Training Loss: 0.585062609429945
[2/3, 513/1125] Training Loss: 0.5691876458494287
[2/3, 570/1125] Training Loss: 0.5590997139612833
[2/3, 627/1125] Training Loss: 0.5719304027264578
[2/3, 684/1125] Training Loss: 0.5607782497740629
[2/3, 741/1125] Training Loss: 0.5472909782016486
[2/3, 798/1125] Training Loss: 0.5447818438212076
[2/3, 855/1125] Training Loss: 0.5287643038389975
[2/3, 912/1125] Training Loss: 0.5384417808892434
[2/3, 969/1125] Training Loss: 0.5312135041805736
[2/3, 1026/1125] Training Loss: 0.5312422618531344
[2/3, 1083/1125] Training Loss: 0.5249913784495571
Testing!
[2/3, 1/188]
[2/3, 11/188]
[2/3, 21/188]
[2/3, 31/188]
[2/3, 41/188]
[2/3, 51/188]
[2/3, 61/188]
[2/3, 71/188]
[2/3, 81/188]
[2/3, 91/188]
[2/3, 101/188]
[2/3, 111/188]
[2/3, 121/188]
[2/3, 131/188]
[2/3, 141/188]
[2/3, 151/188]
[2/3, 161/188]
[2/3, 171/188]
[2/3, 181/188]
Testing Loss: 0.5451195548239507
Training!
[3/3, 57/1125] Training Loss: 0.5257742499050341
[3/3, 114/1125] Training Loss: 0.5217232625735434
[3/3, 171/1125] Training Loss: 0.5283682346343994
[3/3, 228/1125] Training Loss: 0.5291556872819599
[3/3, 285/1125] Training Loss: 0.5152408261048166
[3/3, 342/1125] Training Loss: 0.5175044285623651
[3/3, 399/1125] Training Loss: 0.5252077312845933
[3/3, 456/1125] Training Loss: 0.5244467823128951
[3/3, 513/1125] Training Loss: 0.5151481665017312
[3/3, 570/1125] Training Loss: 0.5199006581515596
[3/3, 627/1125] Training Loss: 0.5158709088961283
[3/3, 684/1125] Training Loss: 0.5172266709177118
[3/3, 741/1125] Training Loss: 0.5157323887473658
[3/3, 798/1125] Training Loss: 0.521492366205182
[3/3, 855/1125] Training Loss: 0.5141237362434989
[3/3, 912/1125] Training Loss: 0.5156468315082684
[3/3, 969/1125] Training Loss: 0.5156896888164052
[3/3, 1026/1125] Training Loss: 0.5112363842495701
[3/3, 1083/1125] Training Loss: 0.508579739353113
Testing!
[3/3, 1/188]
[3/3, 11/188]
[3/3, 21/188]
[3/3, 31/188]
[3/3, 41/188]
[3/3, 51/188]
[3/3, 61/188]
[3/3, 71/188]
[3/3, 81/188]
[3/3, 91/188]
[3/3, 101/188]
[3/3, 111/188]
[3/3, 121/188]
[3/3, 131/188]
[3/3, 141/188]
[3/3, 151/188]
[3/3, 161/188]
[3/3, 171/188]
[3/3, 181/188]
Testing Loss: 0.5283618291219075
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/188
Batch: 2/188
Batch: 3/188
Batch: 4/188
Batch: 5/188
Batch: 6/188
Batch: 7/188
Batch: 8/188
Batch: 9/188
Batch: 10/188
Batch: 11/188
Batch: 12/188
Batch: 13/188
Batch: 14/188
Batch: 15/188
Batch: 16/188
Batch: 17/188
Batch: 18/188
Batch: 19/188
Batch: 20/188
Batch: 21/188
Batch: 22/188
Batch: 23/188
Batch: 24/188
Batch: 25/188
Batch: 26/188
Batch: 27/188
Batch: 28/188
Batch: 29/188
Batch: 30/188
Batch: 31/188
Batch: 32/188
Batch: 33/188
Batch: 34/188
Batch: 35/188
Batch: 36/188
Batch: 37/188
Batch: 38/188
Batch: 39/188
Batch: 40/188
Batch: 41/188
Batch: 42/188
Batch: 43/188
Batch: 44/188
Batch: 45/188
Batch: 46/188
Batch: 47/188
Batch: 48/188
Batch: 49/188
Batch: 50/188
Batch: 51/188
Batch: 52/188
Batch: 53/188
Batch: 54/188
Batch: 55/188
Batch: 56/188
Batch: 57/188
Batch: 58/188
Batch: 59/188
Batch: 60/188
Batch: 61/188
Batch: 62/188
Batch: 63/188
Batch: 64/188
Batch: 65/188
Batch: 66/188
Batch: 67/188
Batch: 68/188
Batch: 69/188
Batch: 70/188
Batch: 71/188
Batch: 72/188
Batch: 73/188
Batch: 74/188
Batch: 75/188
Batch: 76/188
Batch: 77/188
Batch: 78/188
Batch: 79/188
Batch: 80/188
Batch: 81/188
Batch: 82/188
Batch: 83/188
Batch: 84/188
Batch: 85/188
Batch: 86/188
Batch: 87/188
Batch: 88/188
Batch: 89/188
Batch: 90/188
Batch: 91/188
Batch: 92/188
Batch: 93/188
Batch: 94/188
Batch: 95/188
Batch: 96/188
Batch: 97/188
Batch: 98/188
Batch: 99/188
Batch: 100/188
Batch: 101/188
Batch: 102/188
Batch: 103/188
Batch: 104/188
Batch: 105/188
Batch: 106/188
Batch: 107/188
Batch: 108/188
Batch: 109/188
Batch: 110/188
Batch: 111/188
Batch: 112/188
Batch: 113/188
Batch: 114/188
Batch: 115/188
Batch: 116/188
Batch: 117/188
Batch: 118/188
Batch: 119/188
Batch: 120/188
Batch: 121/188
Batch: 122/188
Batch: 123/188
Batch: 124/188
Batch: 125/188
Batch: 126/188
Batch: 127/188
Batch: 128/188
Batch: 129/188
Batch: 130/188
Batch: 131/188
Batch: 132/188
Batch: 133/188
Batch: 134/188
Batch: 135/188
Batch: 136/188
Batch: 137/188
Batch: 138/188
Batch: 139/188
Batch: 140/188
Batch: 141/188
Batch: 142/188
Batch: 143/188
Batch: 144/188
Batch: 145/188
Batch: 146/188
Batch: 147/188
Batch: 148/188
Batch: 149/188
Batch: 150/188
Batch: 151/188
Batch: 152/188
Batch: 153/188
Batch: 154/188
Batch: 155/188
Batch: 156/188
Batch: 157/188
Batch: 158/188
Batch: 159/188
Batch: 160/188
Batch: 161/188
Batch: 162/188
Batch: 163/188
Batch: 164/188
Batch: 165/188
Batch: 166/188
Batch: 167/188
Batch: 168/188
Batch: 169/188
Batch: 170/188
Batch: 171/188
Batch: 172/188
Batch: 173/188
Batch: 174/188
Batch: 175/188
Batch: 176/188
Batch: 177/188
Batch: 178/188
Batch: 179/188
Batch: 180/188
Batch: 181/188
Batch: 182/188
Batch: 183/188
Batch: 184/188
Batch: 185/188
Batch: 186/188
Batch: 187/188
Batch: 188/188
Features: (6000, 100)
All Labels: (6000,)
All Original Images: (6000, 28, 28)
All Reconstructed Images: (6000, 28, 28)
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
Plotting Results Grid
4 added
3 added
9 added
7 added
8 added
0 added
5 added
2 added
6 added
1 added
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation