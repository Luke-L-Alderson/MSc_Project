Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-18 14:30:25.573944
Scaler Value: 0.3571428571428571
Training - 2024-06-18 14:30:25.574440
[1/9, 10/94] Training Loss: 0.4638 - Iteration Time: 0:00:01.473027
[1/9, 20/94] Training Loss: 0.4507 - Iteration Time: 0:00:01.360884
[1/9, 30/94] Training Loss: 0.4523 - Iteration Time: 0:00:01.357463
[1/9, 40/94] Training Loss: 0.4536 - Iteration Time: 0:00:01.408007
[1/9, 50/94] Training Loss: 0.4515 - Iteration Time: 0:00:01.504261
[1/9, 60/94] Training Loss: 0.4479 - Iteration Time: 0:00:01.341549
[1/9, 70/94] Training Loss: 0.4455 - Iteration Time: 0:00:01.397652
[1/9, 80/94] Training Loss: 0.4482 - Iteration Time: 0:00:01.351530
[1/9, 90/94] Training Loss: 0.4474 - Iteration Time: 0:00:01.382751
Testing - 2024-06-18 14:32:39.774387
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 0.4318 - Epoch Time: 0:02:27.278077
Training - 2024-06-18 14:32:52.852517
[2/9, 10/94] Training Loss: 0.4497 - Iteration Time: 0:00:01.351456
[2/9, 20/94] Training Loss: 0.4407 - Iteration Time: 0:00:01.395595
[2/9, 30/94] Training Loss: 0.4414 - Iteration Time: 0:00:01.537108
[2/9, 40/94] Training Loss: 0.4420 - Iteration Time: 0:00:01.377333
[2/9, 50/94] Training Loss: 0.4384 - Iteration Time: 0:00:01.536076
[2/9, 60/94] Training Loss: 0.4421 - Iteration Time: 0:00:01.385225
[2/9, 70/94] Training Loss: 0.4383 - Iteration Time: 0:00:01.549935
[2/9, 80/94] Training Loss: 0.4328 - Iteration Time: 0:00:01.426908
[2/9, 90/94] Training Loss: 0.4366 - Iteration Time: 0:00:01.478987
Testing - 2024-06-18 14:35:07.147937
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 0.4292 - Epoch Time: 0:02:27.544514
Training - 2024-06-18 14:35:20.397526
[3/9, 10/94] Training Loss: 0.4352 - Iteration Time: 0:00:01.361921
[3/9, 20/94] Training Loss: 0.4372 - Iteration Time: 0:00:01.380854
[3/9, 30/94] Training Loss: 0.4324 - Iteration Time: 0:00:01.593059
[3/9, 40/94] Training Loss: 0.4318 - Iteration Time: 0:00:01.391312
[3/9, 50/94] Training Loss: 0.4337 - Iteration Time: 0:00:01.434176
[3/9, 60/94] Training Loss: 0.4322 - Iteration Time: 0:00:01.422152
[3/9, 70/94] Training Loss: 0.4300 - Iteration Time: 0:00:01.609539
[3/9, 80/94] Training Loss: 0.4335 - Iteration Time: 0:00:01.435796
[3/9, 90/94] Training Loss: 0.4313 - Iteration Time: 0:00:01.400737
Testing - 2024-06-18 14:37:36.418185
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 0.4154 - Epoch Time: 0:02:29.475060
Training - 2024-06-18 14:37:49.873082
[4/9, 10/94] Training Loss: 0.4262 - Iteration Time: 0:00:01.509068
[4/9, 20/94] Training Loss: 0.4276 - Iteration Time: 0:00:01.392022
[4/9, 30/94] Training Loss: 0.4273 - Iteration Time: 0:00:01.420253
[4/9, 40/94] Training Loss: 0.4274 - Iteration Time: 0:00:01.543757
[4/9, 50/94] Training Loss: 0.4309 - Iteration Time: 0:00:01.516315
[4/9, 60/94] Training Loss: 0.4283 - Iteration Time: 0:00:01.420246
[4/9, 70/94] Training Loss: 0.4304 - Iteration Time: 0:00:01.390252
[4/9, 80/94] Training Loss: 0.4300 - Iteration Time: 0:00:01.402429
[4/9, 90/94] Training Loss: 0.4240 - Iteration Time: 0:00:01.388885
Testing - 2024-06-18 14:40:05.971635
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 0.4165 - Epoch Time: 0:02:29.332960
Training - 2024-06-18 14:40:19.206042
[5/9, 10/94] Training Loss: 0.4238 - Iteration Time: 0:00:01.405085
[5/9, 20/94] Training Loss: 0.4332 - Iteration Time: 0:00:01.491531
[5/9, 30/94] Training Loss: 0.4304 - Iteration Time: 0:00:01.657590
[5/9, 40/94] Training Loss: 0.4258 - Iteration Time: 0:00:01.442240
[5/9, 50/94] Training Loss: 0.4274 - Iteration Time: 0:00:01.453092
[5/9, 60/94] Training Loss: 0.4234 - Iteration Time: 0:00:01.595391
[5/9, 70/94] Training Loss: 0.4246 - Iteration Time: 0:00:01.652243
[5/9, 80/94] Training Loss: 0.4199 - Iteration Time: 0:00:01.538013
[5/9, 90/94] Training Loss: 0.4230 - Iteration Time: 0:00:01.468056
Testing - 2024-06-18 14:42:38.930086
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 0.4132 - Epoch Time: 0:02:34.023474
Training - 2024-06-18 14:42:53.229516
[6/9, 10/94] Training Loss: 0.4228 - Iteration Time: 0:00:01.585834
[6/9, 20/94] Training Loss: 0.4183 - Iteration Time: 0:00:01.535582
[6/9, 30/94] Training Loss: 0.4224 - Iteration Time: 0:00:01.713468
[6/9, 40/94] Training Loss: 0.4272 - Iteration Time: 0:00:01.443003
[6/9, 50/94] Training Loss: 0.4252 - Iteration Time: 0:00:01.422761
[6/9, 60/94] Training Loss: 0.4229 - Iteration Time: 0:00:01.346660
[6/9, 70/94] Training Loss: 0.4229 - Iteration Time: 0:00:01.455825
[6/9, 80/94] Training Loss: 0.4195 - Iteration Time: 0:00:01.517664
[6/9, 90/94] Training Loss: 0.4219 - Iteration Time: 0:00:01.561301
Testing - 2024-06-18 14:45:20.817220
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 0.4034 - Epoch Time: 0:02:41.643299
Training - 2024-06-18 14:45:34.872815
[7/9, 10/94] Training Loss: 0.4205 - Iteration Time: 0:00:01.489068
[7/9, 20/94] Training Loss: 0.4187 - Iteration Time: 0:00:01.455447
[7/9, 30/94] Training Loss: 0.4180 - Iteration Time: 0:00:01.460847
[7/9, 40/94] Training Loss: 0.4173 - Iteration Time: 0:00:01.422760
[7/9, 50/94] Training Loss: 0.4153 - Iteration Time: 0:00:01.418211
[7/9, 60/94] Training Loss: 0.4093 - Iteration Time: 0:00:01.444792
[7/9, 70/94] Training Loss: 0.4161 - Iteration Time: 0:00:01.474319
[7/9, 80/94] Training Loss: 0.4155 - Iteration Time: 0:00:01.465198
[7/9, 90/94] Training Loss: 0.4147 - Iteration Time: 0:00:01.452180
Testing - 2024-06-18 14:47:56.493082
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 0.3992 - Epoch Time: 0:02:34.919434
Training - 2024-06-18 14:48:09.792743
[8/9, 10/94] Training Loss: 0.4200 - Iteration Time: 0:00:01.347565
[8/9, 20/94] Training Loss: 0.4126 - Iteration Time: 0:00:01.406021
[8/9, 30/94] Training Loss: 0.4099 - Iteration Time: 0:00:01.447146
[8/9, 40/94] Training Loss: 0.4090 - Iteration Time: 0:00:01.414665
[8/9, 50/94] Training Loss: 0.4097 - Iteration Time: 0:00:01.493909
[8/9, 60/94] Training Loss: 0.4102 - Iteration Time: 0:00:01.426603
[8/9, 70/94] Training Loss: 0.4098 - Iteration Time: 0:00:01.472743
[8/9, 80/94] Training Loss: 0.4098 - Iteration Time: 0:00:01.361166
[8/9, 90/94] Training Loss: 0.4074 - Iteration Time: 0:00:01.370811
Testing - 2024-06-18 14:50:26.543253
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 0.3948 - Epoch Time: 0:02:29.963366
Training - 2024-06-18 14:50:39.756605
[9/9, 10/94] Training Loss: 0.4073 - Iteration Time: 0:00:01.344256
[9/9, 20/94] Training Loss: 0.4067 - Iteration Time: 0:00:01.365359
[9/9, 30/94] Training Loss: 0.4065 - Iteration Time: 0:00:01.433890
[9/9, 40/94] Training Loss: 0.4012 - Iteration Time: 0:00:01.724828
[9/9, 50/94] Training Loss: 0.4005 - Iteration Time: 0:00:01.383927
[9/9, 60/94] Training Loss: 0.4015 - Iteration Time: 0:00:01.555127
[9/9, 70/94] Training Loss: 0.4004 - Iteration Time: 0:00:01.517779
[9/9, 80/94] Training Loss: 0.3990 - Iteration Time: 0:00:01.628343
[9/9, 90/94] Training Loss: 0.4033 - Iteration Time: 0:00:01.449040
Testing - 2024-06-18 14:52:55.907044
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 0.3857 - Epoch Time: 0:02:29.986947
Training and Testing Finished - Time: 0:22:44.170104
torch.Size([200, 64, 1, 28, 28])
torch.Size([200, 64, 1, 28, 28])
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
   Labels      0    1      2      3      4  ...   94   95   96     97     98   99
0       2  0.130  0.0  0.130  0.165  0.150  ...  0.0  0.0  0.0  0.085  0.085  0.0
1       4  0.150  0.0  0.100  0.165  0.115  ...  0.0  0.0  0.0  0.090  0.075  0.0
2       7  0.155  0.0  0.130  0.180  0.135  ...  0.0  0.0  0.0  0.100  0.075  0.0
3       3  0.130  0.0  0.095  0.190  0.140  ...  0.0  0.0  0.0  0.115  0.090  0.0
4       7  0.150  0.0  0.110  0.155  0.125  ...  0.0  0.0  0.0  0.080  0.075  0.0
[5 rows x 101 columns]
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2
Applying UMAP