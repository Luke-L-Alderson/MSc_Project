Starting Sweep: Batch Size: 32, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 18000
Testing: 10000 -> 3000
Making Subsets
Training: 18000
Testing: 3000
Making Dataloaders
Defining network
2024-06-06 01:11:02.973857
Training!
0:00:00.011907
[1/9, 29/563] Training Loss: 5.761723518371582
[1/9, 58/563] Training Loss: 1.00115168094635
[1/9, 87/563] Training Loss: 0.804107129573822
[1/9, 116/563] Training Loss: 0.7531419396400452
[1/9, 145/563] Training Loss: 0.7274892926216125
[1/9, 174/563] Training Loss: 0.7016352415084839
[1/9, 203/563] Training Loss: 0.6860324144363403
[1/9, 232/563] Training Loss: 0.6783165335655212
[1/9, 261/563] Training Loss: 0.6574565768241882
[1/9, 290/563] Training Loss: 0.6193790435791016
[1/9, 319/563] Training Loss: 0.6133588552474976
[1/9, 348/563] Training Loss: 0.5867732167243958
[1/9, 377/563] Training Loss: 0.5766780376434326
[1/9, 406/563] Training Loss: 0.5416490435600281
[1/9, 435/563] Training Loss: 0.5242452025413513
[1/9, 464/563] Training Loss: 0.514828622341156
[1/9, 493/563] Training Loss: 0.5072993636131287
[1/9, 522/563] Training Loss: 0.4870148301124573
[1/9, 551/563] Training Loss: 0.46987998485565186
Testing!
[1/9, 1/94]
[1/9, 6/94]
[1/9, 11/94]
[1/9, 16/94]
[1/9, 21/94]
[1/9, 26/94]
[1/9, 31/94]
[1/9, 36/94]
[1/9, 41/94]
[1/9, 46/94]
[1/9, 51/94]
[1/9, 56/94]
[1/9, 61/94]
[1/9, 66/94]
[1/9, 71/94]
[1/9, 76/94]
[1/9, 81/94]
[1/9, 86/94]
[1/9, 91/94]
Testing Loss: 0.4434986710548401
Training!
0:11:56.420068
[2/9, 29/563] Training Loss: 0.4488157629966736
[2/9, 58/563] Training Loss: 0.4388963282108307
[2/9, 87/563] Training Loss: 0.42893049120903015
[2/9, 116/563] Training Loss: 0.4146162271499634
[2/9, 145/563] Training Loss: 0.39927464723587036
[2/9, 174/563] Training Loss: 0.399656742811203
[2/9, 203/563] Training Loss: 0.3893318176269531
[2/9, 232/563] Training Loss: 0.38051837682724
[2/9, 261/563] Training Loss: 0.37340810894966125
[2/9, 290/563] Training Loss: 0.3623153269290924
[2/9, 319/563] Training Loss: 0.3605884909629822
[2/9, 348/563] Training Loss: 0.3548673987388611
[2/9, 377/563] Training Loss: 0.3515737056732178
[2/9, 406/563] Training Loss: 0.3424474895000458
[2/9, 435/563] Training Loss: 0.33473682403564453
[2/9, 464/563] Training Loss: 0.33408892154693604
[2/9, 493/563] Training Loss: 0.3295367360115051
[2/9, 522/563] Training Loss: 0.3187792897224426
[2/9, 551/563] Training Loss: 0.313371479511261
Testing!
[2/9, 1/94]
[2/9, 6/94]
[2/9, 11/94]
[2/9, 16/94]
[2/9, 21/94]
[2/9, 26/94]
[2/9, 31/94]
[2/9, 36/94]
[2/9, 41/94]
[2/9, 46/94]
[2/9, 51/94]
[2/9, 56/94]
[2/9, 61/94]
[2/9, 66/94]
[2/9, 71/94]
[2/9, 76/94]
[2/9, 81/94]
[2/9, 86/94]
[2/9, 91/94]
Testing Loss: 0.30178484320640564
Training!
0:23:53.529854
[3/9, 29/563] Training Loss: 0.3052341043949127
[3/9, 58/563] Training Loss: 0.2946637272834778
[3/9, 87/563] Training Loss: 0.29356661438941956
[3/9, 116/563] Training Loss: 0.2983852028846741
[3/9, 145/563] Training Loss: 0.2951565682888031
[3/9, 174/563] Training Loss: 0.2821862995624542
[3/9, 203/563] Training Loss: 0.2918376922607422
[3/9, 232/563] Training Loss: 0.2878067195415497
[3/9, 261/563] Training Loss: 0.2782391905784607
[3/9, 290/563] Training Loss: 0.27752143144607544
[3/9, 319/563] Training Loss: 0.2744305729866028
[3/9, 348/563] Training Loss: 0.27035996317863464
[3/9, 377/563] Training Loss: 0.2669341266155243
[3/9, 406/563] Training Loss: 0.2688937783241272
[3/9, 435/563] Training Loss: 0.26635628938674927
[3/9, 464/563] Training Loss: 0.2715459167957306
[3/9, 493/563] Training Loss: 0.2630375027656555
[3/9, 522/563] Training Loss: 0.26034820079803467
[3/9, 551/563] Training Loss: 0.2593802213668823
Testing!
[3/9, 1/94]
[3/9, 6/94]
[3/9, 11/94]
[3/9, 16/94]
[3/9, 21/94]
[3/9, 26/94]
[3/9, 31/94]
[3/9, 36/94]
[3/9, 41/94]
[3/9, 46/94]
[3/9, 51/94]
[3/9, 56/94]
[3/9, 61/94]
[3/9, 66/94]
[3/9, 71/94]
[3/9, 76/94]
[3/9, 81/94]
[3/9, 86/94]
[3/9, 91/94]
Testing Loss: 0.262165367603302
Training!
0:35:50.491788
[4/9, 29/563] Training Loss: 0.25808951258659363
[4/9, 58/563] Training Loss: 0.2543995976448059
[4/9, 87/563] Training Loss: 0.25014349818229675
[4/9, 116/563] Training Loss: 0.2508035898208618
[4/9, 145/563] Training Loss: 0.24840852618217468
[4/9, 174/563] Training Loss: 0.24828028678894043
[4/9, 203/563] Training Loss: 0.24407771229743958
[4/9, 232/563] Training Loss: 0.2484978586435318
[4/9, 261/563] Training Loss: 0.2404351532459259
[4/9, 290/563] Training Loss: 0.2438209503889084
[4/9, 319/563] Training Loss: 0.23951822519302368
[4/9, 348/563] Training Loss: 0.2453365921974182
[4/9, 377/563] Training Loss: 0.23813004791736603
[4/9, 406/563] Training Loss: 0.24107299745082855
[4/9, 435/563] Training Loss: 0.23579470813274384
[4/9, 464/563] Training Loss: 0.23059514164924622
[4/9, 493/563] Training Loss: 0.23494333028793335
[4/9, 522/563] Training Loss: 0.23537783324718475
[4/9, 551/563] Training Loss: 0.23562610149383545
Testing!
[4/9, 1/94]
[4/9, 6/94]
[4/9, 11/94]
[4/9, 16/94]
[4/9, 21/94]
[4/9, 26/94]
[4/9, 31/94]
[4/9, 36/94]
[4/9, 41/94]
[4/9, 46/94]
[4/9, 51/94]
[4/9, 56/94]
[4/9, 61/94]
[4/9, 66/94]
[4/9, 71/94]
[4/9, 76/94]
[4/9, 81/94]
[4/9, 86/94]
[4/9, 91/94]
Testing Loss: 0.22932183742523193
Training!
0:47:48.211897
[5/9, 29/563] Training Loss: 0.23499883711338043
[5/9, 58/563] Training Loss: 0.22915861010551453
[5/9, 87/563] Training Loss: 0.22557248175144196
[5/9, 116/563] Training Loss: 0.22852550446987152
[5/9, 145/563] Training Loss: 0.23215506970882416
[5/9, 174/563] Training Loss: 0.2302912324666977
[5/9, 203/563] Training Loss: 0.22345216572284698
[5/9, 232/563] Training Loss: 0.22355496883392334
[5/9, 261/563] Training Loss: 0.2193860113620758
[5/9, 290/563] Training Loss: 0.2250833660364151
[5/9, 319/563] Training Loss: 0.22174623608589172
[5/9, 348/563] Training Loss: 0.21945029497146606
[5/9, 377/563] Training Loss: 0.22498062252998352
[5/9, 406/563] Training Loss: 0.21643270552158356
[5/9, 435/563] Training Loss: 0.2178780734539032
[5/9, 464/563] Training Loss: 0.217538520693779
[5/9, 493/563] Training Loss: 0.21534095704555511
[5/9, 522/563] Training Loss: 0.21124081313610077
[5/9, 551/563] Training Loss: 0.21478068828582764
Testing!
[5/9, 1/94]
[5/9, 6/94]
[5/9, 11/94]
[5/9, 16/94]
[5/9, 21/94]
[5/9, 26/94]
[5/9, 31/94]
[5/9, 36/94]
[5/9, 41/94]
[5/9, 46/94]
[5/9, 51/94]
[5/9, 56/94]
[5/9, 61/94]
[5/9, 66/94]
[5/9, 71/94]
[5/9, 76/94]
[5/9, 81/94]
[5/9, 86/94]
[5/9, 91/94]
Testing Loss: 0.20042863488197327
Training!
0:59:49.106814
[6/9, 29/563] Training Loss: 0.2122146189212799
[6/9, 58/563] Training Loss: 0.20953738689422607
[6/9, 87/563] Training Loss: 0.21074046194553375
[6/9, 116/563] Training Loss: 0.20823383331298828
[6/9, 145/563] Training Loss: 0.2073655128479004
[6/9, 174/563] Training Loss: 0.20700573921203613
[6/9, 203/563] Training Loss: 0.21431919932365417
[6/9, 232/563] Training Loss: 0.20691265165805817
[6/9, 261/563] Training Loss: 0.20658011734485626
[6/9, 290/563] Training Loss: 0.20767349004745483
[6/9, 319/563] Training Loss: 0.20838670432567596
[6/9, 348/563] Training Loss: 0.19971954822540283
[6/9, 377/563] Training Loss: 0.19901831448078156
[6/9, 406/563] Training Loss: 0.1983662098646164
[6/9, 435/563] Training Loss: 0.2037169486284256
[6/9, 464/563] Training Loss: 0.19794225692749023
[6/9, 493/563] Training Loss: 0.20096537470817566
[6/9, 522/563] Training Loss: 0.20088739693164825
[6/9, 551/563] Training Loss: 0.19565676152706146
Testing!
[6/9, 1/94]
[6/9, 6/94]
[6/9, 11/94]
[6/9, 16/94]
[6/9, 21/94]
[6/9, 26/94]
[6/9, 31/94]
[6/9, 36/94]
[6/9, 41/94]
[6/9, 46/94]
[6/9, 51/94]
[6/9, 56/94]
[6/9, 61/94]
[6/9, 66/94]
[6/9, 71/94]
[6/9, 76/94]
[6/9, 81/94]
[6/9, 86/94]
[6/9, 91/94]
Testing Loss: 0.19475917518138885
Training!
1:11:45.090008
[7/9, 29/563] Training Loss: 0.20188799500465393
[7/9, 58/563] Training Loss: 0.19668056070804596
[7/9, 87/563] Training Loss: 0.19940632581710815
[7/9, 116/563] Training Loss: 0.1989765763282776
[7/9, 145/563] Training Loss: 0.1979045271873474
[7/9, 174/563] Training Loss: 0.19439393281936646
[7/9, 203/563] Training Loss: 0.19056975841522217
[7/9, 232/563] Training Loss: 0.19840745627880096
[7/9, 261/563] Training Loss: 0.19547373056411743
[7/9, 290/563] Training Loss: 0.19367371499538422
[7/9, 319/563] Training Loss: 0.1935066282749176
[7/9, 348/563] Training Loss: 0.19318993389606476
[7/9, 377/563] Training Loss: 0.19176051020622253
[7/9, 406/563] Training Loss: 0.19140884280204773
[7/9, 435/563] Training Loss: 0.1916208416223526
[7/9, 464/563] Training Loss: 0.18761976063251495
[7/9, 493/563] Training Loss: 0.18820370733737946
[7/9, 522/563] Training Loss: 0.1904786378145218
[7/9, 551/563] Training Loss: 0.18857912719249725
Testing!
[7/9, 1/94]
[7/9, 6/94]
[7/9, 11/94]
[7/9, 16/94]
[7/9, 21/94]
[7/9, 26/94]
[7/9, 31/94]
[7/9, 36/94]
[7/9, 41/94]
[7/9, 46/94]
[7/9, 51/94]
[7/9, 56/94]
[7/9, 61/94]
[7/9, 66/94]
[7/9, 71/94]
[7/9, 76/94]
[7/9, 81/94]
[7/9, 86/94]
[7/9, 91/94]
Testing Loss: 0.19063334167003632
Training!
1:23:42.319690
[8/9, 29/563] Training Loss: 0.18718324601650238
[8/9, 58/563] Training Loss: 0.1857670247554779
[8/9, 87/563] Training Loss: 0.18697284162044525
[8/9, 116/563] Training Loss: 0.18764549493789673
[8/9, 145/563] Training Loss: 0.18647807836532593
[8/9, 174/563] Training Loss: 0.18700413405895233
[8/9, 203/563] Training Loss: 0.18948137760162354
[8/9, 232/563] Training Loss: 0.18762218952178955
[8/9, 261/563] Training Loss: 0.18762114644050598
[8/9, 290/563] Training Loss: 0.18430645763874054
[8/9, 319/563] Training Loss: 0.18122616410255432
[8/9, 348/563] Training Loss: 0.18513411283493042
[8/9, 377/563] Training Loss: 0.17843136191368103
[8/9, 406/563] Training Loss: 0.18208767473697662
[8/9, 435/563] Training Loss: 0.18210288882255554
[8/9, 464/563] Training Loss: 0.18013714253902435
[8/9, 493/563] Training Loss: 0.18166077136993408
[8/9, 522/563] Training Loss: 0.18370018899440765
[8/9, 551/563] Training Loss: 0.17989316582679749
Testing!
[8/9, 1/94]
[8/9, 6/94]
[8/9, 11/94]
[8/9, 16/94]
[8/9, 21/94]
[8/9, 26/94]
[8/9, 31/94]
[8/9, 36/94]
[8/9, 41/94]
[8/9, 46/94]
[8/9, 51/94]
[8/9, 56/94]
[8/9, 61/94]
[8/9, 66/94]
[8/9, 71/94]
[8/9, 76/94]
[8/9, 81/94]
[8/9, 86/94]
[8/9, 91/94]
Testing Loss: 0.18308646976947784
Training!
1:35:41.764854
[9/9, 29/563] Training Loss: 0.18054866790771484
[9/9, 58/563] Training Loss: 0.17926190793514252
[9/9, 87/563] Training Loss: 0.18032069504261017
[9/9, 116/563] Training Loss: 0.17845061421394348
[9/9, 145/563] Training Loss: 0.17732635140419006
[9/9, 174/563] Training Loss: 0.17594477534294128
[9/9, 203/563] Training Loss: 0.17374743521213531
[9/9, 232/563] Training Loss: 0.1741657853126526
[9/9, 261/563] Training Loss: 0.17678967118263245
[9/9, 290/563] Training Loss: 0.17482902109622955
[9/9, 319/563] Training Loss: 0.17395785450935364
[9/9, 348/563] Training Loss: 0.17644093930721283
[9/9, 377/563] Training Loss: 0.17425639927387238
[9/9, 406/563] Training Loss: 0.17513583600521088
[9/9, 435/563] Training Loss: 0.17769387364387512
[9/9, 464/563] Training Loss: 0.16882768273353577
[9/9, 493/563] Training Loss: 0.1722431182861328
[9/9, 522/563] Training Loss: 0.17138896882534027
[9/9, 551/563] Training Loss: 0.17567558586597443
Testing!
[9/9, 1/94]
[9/9, 6/94]
[9/9, 11/94]
[9/9, 16/94]
[9/9, 21/94]
[9/9, 26/94]
[9/9, 31/94]
[9/9, 36/94]
[9/9, 41/94]
[9/9, 46/94]
[9/9, 51/94]
[9/9, 56/94]
[9/9, 61/94]
[9/9, 66/94]
[9/9, 71/94]
[9/9, 76/94]
[9/9, 81/94]
[9/9, 86/94]
[9/9, 91/94]
Testing Loss: 0.17683544754981995
Training and Testing Finished
Assembling test data for t-sne projection
-- 1/94 --
-- 2/94 --
-- 3/94 --
-- 4/94 --
-- 5/94 --
-- 6/94 --
-- 7/94 --
-- 8/94 --
-- 9/94 --
-- 10/94 --
-- 11/94 --
-- 12/94 --
-- 13/94 --
-- 14/94 --
-- 15/94 --
-- 16/94 --
-- 17/94 --
-- 18/94 --
-- 19/94 --
-- 20/94 --
-- 21/94 --
-- 22/94 --
-- 23/94 --
-- 24/94 --
-- 25/94 --
-- 26/94 --
-- 27/94 --
-- 28/94 --
-- 29/94 --
-- 30/94 --
-- 31/94 --
-- 32/94 --
-- 33/94 --
-- 34/94 --
-- 35/94 --
-- 36/94 --
-- 37/94 --
-- 38/94 --
-- 39/94 --
-- 40/94 --
-- 41/94 --
-- 42/94 --
-- 43/94 --
-- 44/94 --
-- 45/94 --
-- 46/94 --
-- 47/94 --
-- 48/94 --
-- 49/94 --
-- 50/94 --
-- 51/94 --
-- 52/94 --
-- 53/94 --
-- 54/94 --
-- 55/94 --
-- 56/94 --
-- 57/94 --
-- 58/94 --
-- 59/94 --
-- 60/94 --
-- 61/94 --
-- 62/94 --
-- 63/94 --
-- 64/94 --
-- 65/94 --
-- 66/94 --
-- 67/94 --
-- 68/94 --
-- 69/94 --
-- 70/94 --
-- 71/94 --
-- 72/94 --
-- 73/94 --
-- 74/94 --
-- 75/94 --
-- 76/94 --
-- 77/94 --
-- 78/94 --
-- 79/94 --
-- 80/94 --
-- 81/94 --
-- 82/94 --
-- 83/94 --
-- 84/94 --
-- 85/94 --
-- 86/94 --
-- 87/94 --
-- 88/94 --
-- 89/94 --
-- 90/94 --
-- 91/94 --
-- 92/94 --
-- 93/94 --
-- 94/94 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation