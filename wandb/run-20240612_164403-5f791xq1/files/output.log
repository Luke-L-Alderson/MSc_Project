Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-12 16:44:04.183306
Training - 2024-06-12 16:44:04.183801
[1/9, 10/94] Training Loss: 71.23 - Iteration Time: 0:00:01.189239
[1/9, 20/94] Training Loss: 67.68 - Iteration Time: 0:00:01.205633
[1/9, 30/94] Training Loss: 67.98 - Iteration Time: 0:00:01.199577
[1/9, 40/94] Training Loss: 67.81 - Iteration Time: 0:00:01.216480
[1/9, 50/94] Training Loss: 66.56 - Iteration Time: 0:00:01.165431
[1/9, 60/94] Training Loss: 64.82 - Iteration Time: 0:00:01.310308
[1/9, 70/94] Training Loss: 62.79 - Iteration Time: 0:00:01.263691
[1/9, 80/94] Training Loss: 62.95 - Iteration Time: 0:00:01.262169
[1/9, 90/94] Training Loss: 60.92 - Iteration Time: 0:00:01.308857
Testing - 2024-06-12 16:46:04.059132
[1/9, 2/16]
[1/9, 4/16]
[1/9, 6/16]
[1/9, 8/16]
[1/9, 10/16]
[1/9, 12/16]
[1/9, 14/16]
[1/9, 16/16]
Testing Loss: 58.24 - Epoch Time: 0:02:11.961693
Training - 2024-06-12 16:46:16.145990
[2/9, 10/94] Training Loss: 60.76 - Iteration Time: 0:00:01.269663
[2/9, 20/94] Training Loss: 56.96 - Iteration Time: 0:00:01.317746
[2/9, 30/94] Training Loss: 56.60 - Iteration Time: 0:00:01.340569
[2/9, 40/94] Training Loss: 55.30 - Iteration Time: 0:00:01.268560
[2/9, 50/94] Training Loss: 53.66 - Iteration Time: 0:00:01.300803
[2/9, 60/94] Training Loss: 53.73 - Iteration Time: 0:00:01.271024
[2/9, 70/94] Training Loss: 51.94 - Iteration Time: 0:00:01.273555
[2/9, 80/94] Training Loss: 49.70 - Iteration Time: 0:00:01.408485
[2/9, 90/94] Training Loss: 49.71 - Iteration Time: 0:00:01.298835
Testing - 2024-06-12 16:48:19.797295
[2/9, 2/16]
[2/9, 4/16]
[2/9, 6/16]
[2/9, 8/16]
[2/9, 10/16]
[2/9, 12/16]
[2/9, 14/16]
[2/9, 16/16]
Testing Loss: 42.48 - Epoch Time: 0:02:15.747375
Training - 2024-06-12 16:48:31.893365
[3/9, 10/94] Training Loss: 47.90 - Iteration Time: 0:00:01.276599
[3/9, 20/94] Training Loss: 48.08 - Iteration Time: 0:00:01.280952
[3/9, 30/94] Training Loss: 46.23 - Iteration Time: 0:00:01.314713
[3/9, 40/94] Training Loss: 46.12 - Iteration Time: 0:00:01.232918
[3/9, 50/94] Training Loss: 46.05 - Iteration Time: 0:00:01.263037
[3/9, 60/94] Training Loss: 45.15 - Iteration Time: 0:00:01.286964
[3/9, 70/94] Training Loss: 43.34 - Iteration Time: 0:00:01.564796
[3/9, 80/94] Training Loss: 43.31 - Iteration Time: 0:00:01.332134
[3/9, 90/94] Training Loss: 42.28 - Iteration Time: 0:00:01.260258
Testing - 2024-06-12 16:50:36.150535
[3/9, 2/16]
[3/9, 4/16]
[3/9, 6/16]
[3/9, 8/16]
[3/9, 10/16]
[3/9, 12/16]
[3/9, 14/16]
[3/9, 16/16]
Testing Loss: 42.10 - Epoch Time: 0:02:16.244534
Training - 2024-06-12 16:50:48.137899
[4/9, 10/94] Training Loss: 40.26 - Iteration Time: 0:00:01.378715
[4/9, 20/94] Training Loss: 41.10 - Iteration Time: 0:00:01.242768
[4/9, 30/94] Training Loss: 39.48 - Iteration Time: 0:00:01.240505
[4/9, 40/94] Training Loss: 38.51 - Iteration Time: 0:00:01.297948
[4/9, 50/94] Training Loss: 38.10 - Iteration Time: 0:00:01.249289
[4/9, 60/94] Training Loss: 37.00 - Iteration Time: 0:00:01.250710
[4/9, 70/94] Training Loss: 37.47 - Iteration Time: 0:00:01.253591
[4/9, 80/94] Training Loss: 36.69 - Iteration Time: 0:00:01.236917
[4/9, 90/94] Training Loss: 35.55 - Iteration Time: 0:00:01.266635
Testing - 2024-06-12 16:52:50.442433
[4/9, 2/16]
[4/9, 4/16]
[4/9, 6/16]
[4/9, 8/16]
[4/9, 10/16]
[4/9, 12/16]
[4/9, 14/16]
[4/9, 16/16]
Testing Loss: 35.85 - Epoch Time: 0:02:14.739298
Training - 2024-06-12 16:53:02.877693
[5/9, 10/94] Training Loss: 34.89 - Iteration Time: 0:00:01.281168
[5/9, 20/94] Training Loss: 34.79 - Iteration Time: 0:00:01.281487
[5/9, 30/94] Training Loss: 33.02 - Iteration Time: 0:00:01.242378
[5/9, 40/94] Training Loss: 32.73 - Iteration Time: 0:00:01.288942
[5/9, 50/94] Training Loss: 32.44 - Iteration Time: 0:00:01.372275
[5/9, 60/94] Training Loss: 31.74 - Iteration Time: 0:00:01.254292
[5/9, 70/94] Training Loss: 31.02 - Iteration Time: 0:00:01.425462
[5/9, 80/94] Training Loss: 29.66 - Iteration Time: 0:00:01.245349
[5/9, 90/94] Training Loss: 29.42 - Iteration Time: 0:00:01.345390
Testing - 2024-06-12 16:55:06.727611
[5/9, 2/16]
[5/9, 4/16]
[5/9, 6/16]
[5/9, 8/16]
[5/9, 10/16]
[5/9, 12/16]
[5/9, 14/16]
[5/9, 16/16]
Testing Loss: 28.77 - Epoch Time: 0:02:15.384235
Training - 2024-06-12 16:55:18.262425
[6/9, 10/94] Training Loss: 28.88 - Iteration Time: 0:00:01.168881
[6/9, 20/94] Training Loss: 27.96 - Iteration Time: 0:00:01.200127
[6/9, 30/94] Training Loss: 27.80 - Iteration Time: 0:00:01.208164
[6/9, 40/94] Training Loss: 27.56 - Iteration Time: 0:00:01.290430
[6/9, 50/94] Training Loss: 27.13 - Iteration Time: 0:00:01.301402
[6/9, 60/94] Training Loss: 26.85 - Iteration Time: 0:00:01.280540
[6/9, 70/94] Training Loss: 26.86 - Iteration Time: 0:00:01.250731
[6/9, 80/94] Training Loss: 26.59 - Iteration Time: 0:00:01.519185
[6/9, 90/94] Training Loss: 26.49 - Iteration Time: 0:00:01.279044
Testing - 2024-06-12 16:57:21.160114
[6/9, 2/16]
[6/9, 4/16]
[6/9, 6/16]
[6/9, 8/16]
[6/9, 10/16]
[6/9, 12/16]
[6/9, 14/16]
[6/9, 16/16]
Testing Loss: 26.07 - Epoch Time: 0:02:17.102951
Training - 2024-06-12 16:57:35.365872
[7/9, 10/94] Training Loss: 25.38 - Iteration Time: 0:00:01.400151
[7/9, 20/94] Training Loss: 25.10 - Iteration Time: 0:00:01.320257
[7/9, 30/94] Training Loss: 24.88 - Iteration Time: 0:00:01.271548
[7/9, 40/94] Training Loss: 24.32 - Iteration Time: 0:00:01.271165
[7/9, 50/94] Training Loss: 24.00 - Iteration Time: 0:00:01.292993
[7/9, 60/94] Training Loss: 22.56 - Iteration Time: 0:00:01.405108
[7/9, 70/94] Training Loss: 22.84 - Iteration Time: 0:00:01.452633
[7/9, 80/94] Training Loss: 22.34 - Iteration Time: 0:00:01.524673
[7/9, 90/94] Training Loss: 22.19 - Iteration Time: 0:00:01.483956
Testing - 2024-06-12 16:59:47.336376
[7/9, 2/16]
[7/9, 4/16]
[7/9, 6/16]
[7/9, 8/16]
[7/9, 10/16]
[7/9, 12/16]
[7/9, 14/16]
[7/9, 16/16]
Testing Loss: 22.09 - Epoch Time: 0:02:25.420477
Training - 2024-06-12 17:00:00.786845
[8/9, 10/94] Training Loss: 22.09 - Iteration Time: 0:00:01.420955
[8/9, 20/94] Training Loss: 21.23 - Iteration Time: 0:00:01.325151
[8/9, 30/94] Training Loss: 20.86 - Iteration Time: 0:00:01.311848
[8/9, 40/94] Training Loss: 20.18 - Iteration Time: 0:00:01.334202
[8/9, 50/94] Training Loss: 19.67 - Iteration Time: 0:00:01.305812
[8/9, 60/94] Training Loss: 19.55 - Iteration Time: 0:00:01.359454
[8/9, 70/94] Training Loss: 19.11 - Iteration Time: 0:00:01.422415
[8/9, 80/94] Training Loss: 18.82 - Iteration Time: 0:00:01.458625
[8/9, 90/94] Training Loss: 18.13 - Iteration Time: 0:00:01.622860
Testing - 2024-06-12 17:02:14.007831
[8/9, 2/16]
[8/9, 4/16]
[8/9, 6/16]
[8/9, 8/16]
[8/9, 10/16]
[8/9, 12/16]
[8/9, 14/16]
[8/9, 16/16]
Testing Loss: 18.29 - Epoch Time: 0:02:26.575501
Training - 2024-06-12 17:02:27.362842
[9/9, 10/94] Training Loss: 17.51 - Iteration Time: 0:00:01.551915
[9/9, 20/94] Training Loss: 17.40 - Iteration Time: 0:00:01.550475
[9/9, 30/94] Training Loss: 17.17 - Iteration Time: 0:00:01.571310
[9/9, 40/94] Training Loss: 16.69 - Iteration Time: 0:00:01.429794
[9/9, 50/94] Training Loss: 16.35 - Iteration Time: 0:00:01.631242
[9/9, 60/94] Training Loss: 16.51 - Iteration Time: 0:00:02.010802
[9/9, 70/94] Training Loss: 16.18 - Iteration Time: 0:00:02.481474
[9/9, 80/94] Training Loss: 16.06 - Iteration Time: 0:00:01.767272
[9/9, 90/94] Training Loss: 16.70 - Iteration Time: 0:00:02.289053
Testing - 2024-06-12 17:05:05.739932
[9/9, 2/16]
[9/9, 4/16]
[9/9, 6/16]
[9/9, 8/16]
[9/9, 10/16]
[9/9, 12/16]
[9/9, 14/16]
[9/9, 16/16]
Testing Loss: 16.44 - Epoch Time: 0:02:56.312882
Training and Testing Finished - Time: 0:21:19.492915
Assembling test data for t-sne projection
-- 10/16 --
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
WARNING    c:\users\lukea\documents\masters project code\msc_project\main.py:181: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
 [py.warnings]
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - {labels[input_index]}