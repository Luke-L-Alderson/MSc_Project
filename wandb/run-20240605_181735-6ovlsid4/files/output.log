Starting Sweep: Batch Size: 16, Learning Rate: 0.1
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Subsets
Training: 6000
Testing: 1000
(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.7725, 0.2118, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0824, 0.9294, 0.3451, 0.0353, 0.0353, 0.0353, 0.0353,
          0.3922, 0.4314, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.6588, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0627, 0.9294, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0784, 0.9765, 0.9961, 0.6745, 0.5373, 0.2627, 0.2627, 0.2627,
          0.2627, 0.9137, 0.7529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3255, 0.3216, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.8863, 0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0275, 0.8980, 0.5686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.2157, 0.9961, 0.4941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.4824, 0.9961, 0.3765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.5961, 0.9961, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.8706, 0.9961, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1059,
          0.9373, 0.8000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588,
          0.9961, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.9333,
          0.9961, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9961,
          0.9961, 0.6196, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7020, 0.9961,
          0.9961, 0.8627, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.8824, 0.9961,
          0.9373, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9961, 0.9961,
          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7020, 0.9961, 0.7686,
          0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 0.9961, 0.2588,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]]), 7)
<class 'tuple'>
2
Making Dataloaders
Defining network
2024-06-05 18:17:37.219874
Training!
0:00:00.083965
[1/6, 19/375] Training Loss: 2.1029675006866455
[1/6, 38/375] Training Loss: 1.1904810667037964
[1/6, 57/375] Training Loss: 1.1824140548706055
[1/6, 76/375] Training Loss: 1.1802325248718262
[1/6, 95/375] Training Loss: 1.1752468347549438
[1/6, 114/375] Training Loss: 1.1774214506149292
[1/6, 133/375] Training Loss: 1.2038568258285522
[1/6, 152/375] Training Loss: 1.166131854057312
[1/6, 171/375] Training Loss: 1.1885957717895508
[1/6, 190/375] Training Loss: 1.1770696640014648
[1/6, 209/375] Training Loss: 1.1868128776550293
[1/6, 228/375] Training Loss: 1.2444041967391968
[1/6, 247/375] Training Loss: 1.1706874370574951
[1/6, 266/375] Training Loss: 1.1627085208892822
[1/6, 285/375] Training Loss: 1.1941542625427246
[1/6, 304/375] Training Loss: 1.207457184791565
[1/6, 323/375] Training Loss: 1.1844695806503296
[1/6, 342/375] Training Loss: 1.2053532600402832
[1/6, 361/375] Training Loss: 1.1886765956878662
Testing!
[1/6, 1/63]
[1/6, 5/63]
[1/6, 9/63]
[1/6, 13/63]
[1/6, 17/63]
[1/6, 21/63]
[1/6, 25/63]
[1/6, 29/63]
[1/6, 33/63]
[1/6, 37/63]
[1/6, 41/63]
[1/6, 45/63]
[1/6, 49/63]
[1/6, 53/63]
[1/6, 57/63]
[1/6, 61/63]
Testing Loss: 1.330104112625122
Training!
0:07:29.257325
[2/6, 19/375] Training Loss: 1.204115867614746
[2/6, 38/375] Training Loss: 1.1893508434295654
[2/6, 57/375] Training Loss: 1.19076406955719
[2/6, 76/375] Training Loss: 1.1654096841812134
[2/6, 95/375] Training Loss: 1.1716923713684082
[2/6, 114/375] Training Loss: 1.2231113910675049
[2/6, 133/375] Training Loss: 1.1658695936203003
[2/6, 152/375] Training Loss: 1.173783779144287
[2/6, 171/375] Training Loss: 1.2636220455169678
[2/6, 190/375] Training Loss: 1.1510546207427979
[2/6, 209/375] Training Loss: 1.189340353012085
[2/6, 228/375] Training Loss: 1.1362279653549194
[2/6, 247/375] Training Loss: 1.2042794227600098
[2/6, 266/375] Training Loss: 1.1486263275146484
[2/6, 285/375] Training Loss: 1.2115283012390137
[2/6, 304/375] Training Loss: 1.154478669166565
[2/6, 323/375] Training Loss: 1.1692960262298584
[2/6, 342/375] Training Loss: 1.1795698404312134
[2/6, 361/375] Training Loss: 1.1415185928344727
Testing!
[2/6, 1/63]
[2/6, 5/63]
[2/6, 9/63]
[2/6, 13/63]
[2/6, 17/63]
[2/6, 21/63]
[2/6, 25/63]
[2/6, 29/63]
[2/6, 33/63]
[2/6, 37/63]
[2/6, 41/63]
[2/6, 45/63]
[2/6, 49/63]
[2/6, 53/63]
[2/6, 57/63]
[2/6, 61/63]
Testing Loss: 1.2952463626861572
Training!
0:14:50.618738
[3/6, 19/375] Training Loss: 1.1834344863891602
[3/6, 38/375] Training Loss: 1.1863596439361572
[3/6, 57/375] Training Loss: 1.173190712928772
[3/6, 76/375] Training Loss: 1.1914963722229004
[3/6, 95/375] Training Loss: 1.1580160856246948
[3/6, 114/375] Training Loss: 1.177383303642273
[3/6, 133/375] Training Loss: 1.166324496269226
[3/6, 152/375] Training Loss: 1.196950912475586
[3/6, 171/375] Training Loss: 1.2109602689743042
[3/6, 190/375] Training Loss: 1.1848812103271484
[3/6, 209/375] Training Loss: 1.1418110132217407
[3/6, 228/375] Training Loss: 1.213029146194458
[3/6, 247/375] Training Loss: 1.1053831577301025
[3/6, 266/375] Training Loss: 1.2034871578216553
[3/6, 285/375] Training Loss: 1.1796778440475464
[3/6, 304/375] Training Loss: 1.1361970901489258
[3/6, 323/375] Training Loss: 1.2462542057037354
[3/6, 342/375] Training Loss: 1.1935492753982544
[3/6, 361/375] Training Loss: 1.2088640928268433
Testing!
[3/6, 1/63]
[3/6, 5/63]
[3/6, 9/63]
[3/6, 13/63]
[3/6, 17/63]
[3/6, 21/63]
[3/6, 25/63]
[3/6, 29/63]
[3/6, 33/63]
[3/6, 37/63]
[3/6, 41/63]
[3/6, 45/63]
[3/6, 49/63]
[3/6, 53/63]
[3/6, 57/63]
[3/6, 61/63]
Testing Loss: 1.2269566059112549
Training!
0:22:11.415236
[4/6, 19/375] Training Loss: 1.1385542154312134
[4/6, 38/375] Training Loss: 1.1551352739334106
[4/6, 57/375] Training Loss: 1.1741224527359009
[4/6, 76/375] Training Loss: 1.1828559637069702
[4/6, 95/375] Training Loss: 1.1708110570907593
[4/6, 114/375] Training Loss: 1.204352855682373
[4/6, 133/375] Training Loss: 1.1835570335388184
[4/6, 152/375] Training Loss: 1.2004427909851074
[4/6, 171/375] Training Loss: 1.1819000244140625
[4/6, 190/375] Training Loss: 1.207648515701294
[4/6, 209/375] Training Loss: 1.178238034248352
[4/6, 228/375] Training Loss: 1.2010364532470703
[4/6, 247/375] Training Loss: 1.1995710134506226
[4/6, 266/375] Training Loss: 1.1837306022644043
[4/6, 285/375] Training Loss: 1.1716997623443604
[4/6, 304/375] Training Loss: 1.1787471771240234
[4/6, 323/375] Training Loss: 1.2346919775009155
[4/6, 342/375] Training Loss: 1.2039586305618286
[4/6, 361/375] Training Loss: 1.221910834312439
Testing!
[4/6, 1/63]
[4/6, 5/63]
[4/6, 9/63]
[4/6, 13/63]
[4/6, 17/63]
[4/6, 21/63]
[4/6, 25/63]
[4/6, 29/63]
[4/6, 33/63]
[4/6, 37/63]
[4/6, 41/63]
[4/6, 45/63]
[4/6, 49/63]
[4/6, 53/63]
[4/6, 57/63]
[4/6, 61/63]
Testing Loss: 1.158110499382019
Training!
0:29:32.908317
[5/6, 19/375] Training Loss: 1.194923996925354
[5/6, 38/375] Training Loss: 1.1889573335647583
[5/6, 57/375] Training Loss: 1.1944682598114014
[5/6, 76/375] Training Loss: 1.1892638206481934
[5/6, 95/375] Training Loss: 1.2247275114059448
[5/6, 114/375] Training Loss: 1.199471354484558
[5/6, 133/375] Training Loss: 1.1791391372680664
[5/6, 152/375] Training Loss: 1.140561580657959
[5/6, 171/375] Training Loss: 1.1998002529144287
[5/6, 190/375] Training Loss: 1.1502552032470703
[5/6, 209/375] Training Loss: 1.1702568531036377
[5/6, 228/375] Training Loss: 1.203550100326538
[5/6, 247/375] Training Loss: 1.1894711256027222
[5/6, 266/375] Training Loss: 1.2273410558700562
[5/6, 285/375] Training Loss: 1.189829707145691
[5/6, 304/375] Training Loss: 1.2104347944259644
[5/6, 323/375] Training Loss: 1.2025728225708008
[5/6, 342/375] Training Loss: 1.1535228490829468
[5/6, 361/375] Training Loss: 1.194752812385559
Testing!
[5/6, 1/63]
[5/6, 5/63]
[5/6, 9/63]
[5/6, 13/63]
[5/6, 17/63]
[5/6, 21/63]
[5/6, 25/63]
[5/6, 29/63]
[5/6, 33/63]
[5/6, 37/63]
[5/6, 41/63]
[5/6, 45/63]
[5/6, 49/63]
[5/6, 53/63]
[5/6, 57/63]
[5/6, 61/63]
Testing Loss: 1.2595056295394897
Training!
0:36:54.765113
[6/6, 19/375] Training Loss: 1.2212015390396118
[6/6, 38/375] Training Loss: 1.1825942993164062
[6/6, 57/375] Training Loss: 1.1920536756515503
[6/6, 76/375] Training Loss: 1.1982089281082153
[6/6, 95/375] Training Loss: 1.1606390476226807
[6/6, 114/375] Training Loss: 1.1719881296157837
[6/6, 133/375] Training Loss: 1.1990587711334229
[6/6, 152/375] Training Loss: 1.1492465734481812
[6/6, 171/375] Training Loss: 1.1758536100387573
[6/6, 190/375] Training Loss: 1.1556832790374756
[6/6, 209/375] Training Loss: 1.19419527053833
[6/6, 228/375] Training Loss: 1.179144024848938
[6/6, 247/375] Training Loss: 1.1681714057922363
[6/6, 266/375] Training Loss: 1.175522804260254
[6/6, 285/375] Training Loss: 1.1950477361679077
[6/6, 304/375] Training Loss: 1.169251561164856
[6/6, 323/375] Training Loss: 1.2040348052978516
[6/6, 342/375] Training Loss: 1.2228859663009644
[6/6, 361/375] Training Loss: 1.1640132665634155
Testing!
[6/6, 1/63]
[6/6, 5/63]
[6/6, 9/63]
[6/6, 13/63]
[6/6, 17/63]
[6/6, 21/63]
[6/6, 25/63]
[6/6, 29/63]
[6/6, 33/63]
[6/6, 37/63]
[6/6, 41/63]
[6/6, 45/63]
[6/6, 49/63]
[6/6, 53/63]
[6/6, 57/63]
[6/6, 61/63]
Testing Loss: 1.2139968872070312
Training and Testing Finished
Assembling test data for t-sne projection
-- 1/63 --
-- 2/63 --
-- 3/63 --
-- 4/63 --
-- 5/63 --
-- 6/63 --
-- 7/63 --
-- 8/63 --
-- 9/63 --
-- 10/63 --
-- 11/63 --
-- 12/63 --
-- 13/63 --
-- 14/63 --
-- 15/63 --
-- 16/63 --
-- 17/63 --
-- 18/63 --
-- 19/63 --
-- 20/63 --
-- 21/63 --
-- 22/63 --
-- 23/63 --
-- 24/63 --
-- 25/63 --
-- 26/63 --
-- 27/63 --
-- 28/63 --
-- 29/63 --
-- 30/63 --
-- 31/63 --
-- 32/63 --
-- 33/63 --
-- 34/63 --
-- 35/63 --
-- 36/63 --
-- 37/63 --
-- 38/63 --
-- 39/63 --
-- 40/63 --
-- 41/63 --
-- 42/63 --
-- 43/63 --
-- 44/63 --
-- 45/63 --
-- 46/63 --
-- 47/63 --
-- 48/63 --
-- 49/63 --
-- 50/63 --
-- 51/63 --
-- 52/63 --
-- 53/63 --
-- 54/63 --
-- 55/63 --
-- 56/63 --
-- 57/63 --
-- 58/63 --
-- 59/63 --
-- 60/63 --
-- 61/63 --
-- 62/63 --
-- 63/63 --
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\sklearn\decomposition\_pca.py:640: RuntimeWarning: invalid value encountered in divide
  self.explained_variance_ratio_ = self.explained_variance_ / total_var
 [py.warnings]
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\sklearn\manifold\_t_sne.py:987: RuntimeWarning: invalid value encountered in divide
  X_embedded = X_embedded / np.std(X_embedded[:, 0]) * 1e-4
