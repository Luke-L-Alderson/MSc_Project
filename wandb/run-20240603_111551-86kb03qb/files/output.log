Starting Sweep: Batch Size: 64, Learning Rate: 0.001
Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/1, 10/938] Training Loss: 3.9809163749217986
[1/1, 20/938] Training Loss: 1.0514630258083344
[1/1, 30/938] Training Loss: 1.0510678350925446
[1/1, 40/938] Training Loss: 1.0352711021900176
[1/1, 50/938] Training Loss: 0.9823350548744202
[1/1, 60/938] Training Loss: 1.013880693912506
[1/1, 70/938] Training Loss: 0.9720891416072845
[1/1, 80/938] Training Loss: 0.9867373824119567
[1/1, 90/938] Training Loss: 0.9744165182113648
[1/1, 100/938] Training Loss: 0.9766546964645386
[1/1, 110/938] Training Loss: 0.9441365718841552
[1/1, 120/938] Training Loss: 0.9354695081710815
[1/1, 130/938] Training Loss: 0.968502688407898
[1/1, 140/938] Training Loss: 0.9466968595981597
[1/1, 150/938] Training Loss: 0.9370763301849365
[1/1, 160/938] Training Loss: 0.9017513692378998
[1/1, 170/938] Training Loss: 0.8814500093460083
[1/1, 180/938] Training Loss: 0.9167596578598023
[1/1, 190/938] Training Loss: 0.871945196390152
[1/1, 200/938] Training Loss: 0.9120018899440765
[1/1, 210/938] Training Loss: 0.8957838535308837
[1/1, 220/938] Training Loss: 0.8526082575321198
[1/1, 230/938] Training Loss: 0.8258615791797638
[1/1, 240/938] Training Loss: 0.8398462355136871
[1/1, 250/938] Training Loss: 0.836083048582077
[1/1, 260/938] Training Loss: 0.8390765368938446
[1/1, 270/938] Training Loss: 0.821953558921814
[1/1, 280/938] Training Loss: 0.8369763433933258
[1/1, 290/938] Training Loss: 0.8490425229072571
[1/1, 300/938] Training Loss: 0.824485319852829
[1/1, 310/938] Training Loss: 0.8324391007423401
[1/1, 320/938] Training Loss: 0.8586303055286407
[1/1, 330/938] Training Loss: 0.8061696469783783
[1/1, 340/938] Training Loss: 0.821747463941574
[1/1, 350/938] Training Loss: 0.8003351330757141
[1/1, 360/938] Training Loss: 0.8029249727725982
[1/1, 370/938] Training Loss: 0.7994967877864838
[1/1, 380/938] Training Loss: 0.7986936926841736
[1/1, 390/938] Training Loss: 0.7978353142738343
[1/1, 400/938] Training Loss: 0.7756339907646179
[1/1, 410/938] Training Loss: 0.8000777304172516
[1/1, 420/938] Training Loss: 0.7768224954605103
[1/1, 430/938] Training Loss: 0.797572910785675
[1/1, 440/938] Training Loss: 0.7618285953998566
[1/1, 450/938] Training Loss: 0.7489500224590302
[1/1, 460/938] Training Loss: 0.7645623505115509
[1/1, 470/938] Training Loss: 0.7467195808887481
[1/1, 480/938] Training Loss: 0.7538168072700501
[1/1, 490/938] Training Loss: 0.7410694777965545
[1/1, 500/938] Training Loss: 0.7327845573425293
[1/1, 510/938] Training Loss: 0.7237011551856994
[1/1, 520/938] Training Loss: 0.7056842505931854
[1/1, 530/938] Training Loss: 0.7138534963130951
[1/1, 540/938] Training Loss: 0.7381662428379059
[1/1, 550/938] Training Loss: 0.7115449249744416
[1/1, 560/938] Training Loss: 0.7328290939331055
[1/1, 570/938] Training Loss: 0.7308694839477539
[1/1, 580/938] Training Loss: 0.70164715051651
[1/1, 590/938] Training Loss: 0.7272309541702271
[1/1, 600/938] Training Loss: 0.7231838941574097
[1/1, 610/938] Training Loss: 0.7158728063106536
[1/1, 620/938] Training Loss: 0.7034651756286621
[1/1, 630/938] Training Loss: 0.7014845132827758
[1/1, 640/938] Training Loss: 0.6883956313133239
[1/1, 650/938] Training Loss: 0.6939452886581421
[1/1, 660/938] Training Loss: 0.6881375074386596
[1/1, 670/938] Training Loss: 0.6778862595558166
[1/1, 680/938] Training Loss: 0.673334950208664
[1/1, 690/938] Training Loss: 0.6487361013889312
[1/1, 700/938] Training Loss: 0.6756734251976013
[1/1, 710/938] Training Loss: 0.6637900650501252
[1/1, 720/938] Training Loss: 0.6713958561420441
[1/1, 730/938] Training Loss: 0.6404569327831269
[1/1, 740/938] Training Loss: 0.6577921092510224
[1/1, 750/938] Training Loss: 0.6416942954063416
[1/1, 760/938] Training Loss: 0.6602049350738526
[1/1, 770/938] Training Loss: 0.6613242864608765
[1/1, 780/938] Training Loss: 0.6806680858135223
[1/1, 790/938] Training Loss: 0.6444944500923157
[1/1, 800/938] Training Loss: 0.654084587097168
[1/1, 810/938] Training Loss: 0.6756985306739807
[1/1, 820/938] Training Loss: 0.6380348801612854
[1/1, 830/938] Training Loss: 0.6479597330093384
[1/1, 840/938] Training Loss: 0.6413630247116089
[1/1, 850/938] Training Loss: 0.6327098727226257
[1/1, 860/938] Training Loss: 0.6615355610847473
[1/1, 870/938] Training Loss: 0.6037398278713226
[1/1, 880/938] Training Loss: 0.6448513269424438
[1/1, 890/938] Training Loss: 0.6310230553150177
[1/1, 900/938] Training Loss: 0.6516509532928467
[1/1, 910/938] Training Loss: 0.6341285049915314
[1/1, 920/938] Training Loss: 0.6105745673179627
[1/1, 930/938] Training Loss: 0.5998004853725434
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.6162395564715069
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
7 added
0 added
4 added
2 added
3 added
5 added
1 added
6 added
9 added
8 added