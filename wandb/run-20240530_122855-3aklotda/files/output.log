Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/938] Training Loss: 1.5162796795368194
[1/3, 20/938] Training Loss: 1.0440765917301178
[1/3, 30/938] Training Loss: 1.0266044080257415
[1/3, 40/938] Training Loss: 1.002770435810089
[1/3, 50/938] Training Loss: 0.9511718451976776
[1/3, 60/938] Training Loss: 0.9786489605903625
[1/3, 70/938] Training Loss: 0.9317245781421661
[1/3, 80/938] Training Loss: 0.9389783322811127
[1/3, 90/938] Training Loss: 0.9213830232620239
[1/3, 100/938] Training Loss: 0.9290744185447692
[1/3, 110/938] Training Loss: 0.8934170246124268
[1/3, 120/938] Training Loss: 0.8742414355278015
[1/3, 130/938] Training Loss: 0.895583713054657
[1/3, 140/938] Training Loss: 0.8760287463665009
[1/3, 150/938] Training Loss: 0.8702572703361511
[1/3, 160/938] Training Loss: 0.8340715646743775
[1/3, 170/938] Training Loss: 0.8231932163238526
[1/3, 180/938] Training Loss: 0.8551722884178161
[1/3, 190/938] Training Loss: 0.8147155821323395
[1/3, 200/938] Training Loss: 0.845467209815979
[1/3, 210/938] Training Loss: 0.825642329454422
[1/3, 220/938] Training Loss: 0.7874617040157318
[1/3, 230/938] Training Loss: 0.7585189402103424
[1/3, 240/938] Training Loss: 0.7747532367706299
[1/3, 250/938] Training Loss: 0.7591143786907196
[1/3, 260/938] Training Loss: 0.7501541674137115
[1/3, 270/938] Training Loss: 0.7347194433212281
[1/3, 280/938] Training Loss: 0.7409635424613953
[1/3, 290/938] Training Loss: 0.748293536901474
[1/3, 300/938] Training Loss: 0.7318974554538726
[1/3, 310/938] Training Loss: 0.7272791683673858
[1/3, 320/938] Training Loss: 0.709535813331604
[1/3, 330/938] Training Loss: 0.6965683996677399
[1/3, 340/938] Training Loss: 0.7108986020088196
[1/3, 350/938] Training Loss: 0.6962889432907104
[1/3, 360/938] Training Loss: 0.6988450288772583
[1/3, 370/938] Training Loss: 0.7497570514678955
[1/3, 380/938] Training Loss: 0.7185148000717163
[1/3, 390/938] Training Loss: 0.697792512178421
[1/3, 400/938] Training Loss: 0.6727804183959961
[1/3, 410/938] Training Loss: 0.6891414582729339
[1/3, 420/938] Training Loss: 0.6586983919143676
[1/3, 430/938] Training Loss: 0.6700134396553039
[1/3, 440/938] Training Loss: 0.6425920903682709
[1/3, 450/938] Training Loss: 0.6347899258136749
[1/3, 460/938] Training Loss: 0.6448401749134064
[1/3, 470/938] Training Loss: 0.6411734104156495
[1/3, 480/938] Training Loss: 0.6527170538902283
[1/3, 490/938] Training Loss: 0.628363311290741
[1/3, 500/938] Training Loss: 0.6125457584857941
[1/3, 510/938] Training Loss: 0.6220109701156616
[1/3, 520/938] Training Loss: 0.5939390063285828
[1/3, 530/938] Training Loss: 0.5907969415187836
[1/3, 540/938] Training Loss: 0.606592720746994
[1/3, 550/938] Training Loss: 0.5871710062026978
[1/3, 560/938] Training Loss: 0.6123747229576111
[1/3, 570/938] Training Loss: 0.6109108805656434
[1/3, 580/938] Training Loss: 0.5805073976516724
[1/3, 590/938] Training Loss: 0.6004433870315552
[1/3, 600/938] Training Loss: 0.5984822630882263
[1/3, 610/938] Training Loss: 0.5854004144668579
[1/3, 620/938] Training Loss: 0.5739816904067994
[1/3, 630/938] Training Loss: 0.5721286237239838
[1/3, 640/938] Training Loss: 0.5572364628314972
[1/3, 650/938] Training Loss: 0.5736340045928955
[1/3, 660/938] Training Loss: 0.5739408314228058
[1/3, 670/938] Training Loss: 0.5576333940029145
[1/3, 680/938] Training Loss: 0.5504932552576065
[1/3, 690/938] Training Loss: 0.5274375170469284
[1/3, 700/938] Training Loss: 0.550139844417572
[1/3, 710/938] Training Loss: 0.5384591221809387
[1/3, 720/938] Training Loss: 0.5426024436950684
[1/3, 730/938] Training Loss: 0.5236968100070953
[1/3, 740/938] Training Loss: 0.5388537019491195
[1/3, 750/938] Training Loss: 0.5209618330001831
[1/3, 760/938] Training Loss: 0.5303395748138428
[1/3, 770/938] Training Loss: 0.530392411351204
[1/3, 780/938] Training Loss: 0.5575618207454681
[1/3, 790/938] Training Loss: 0.5329408049583435
[1/3, 800/938] Training Loss: 0.5336954325437546
[1/3, 810/938] Training Loss: 0.5474322140216827
[1/3, 820/938] Training Loss: 0.5189730048179626
[1/3, 830/938] Training Loss: 0.5221323758363724
[1/3, 840/938] Training Loss: 0.5168017774820328
[1/3, 850/938] Training Loss: 0.5108351886272431
[1/3, 860/938] Training Loss: 0.5286720752716064
[1/3, 870/938] Training Loss: 0.4868564188480377
[1/3, 880/938] Training Loss: 0.5195934236049652
[1/3, 890/938] Training Loss: 0.5023114085197449
[1/3, 900/938] Training Loss: 0.5148096710443497
[1/3, 910/938] Training Loss: 0.5110965639352798
[1/3, 920/938] Training Loss: 0.5188026130199432
[1/3, 930/938] Training Loss: 0.49324126839637755
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.503318014740944
5/5: Training network...
[2/3, 10/938] Training Loss: 0.5084300696849823
[2/3, 20/938] Training Loss: 0.5004209220409394
[2/3, 30/938] Training Loss: 0.4963749319314957
[2/3, 40/938] Training Loss: 0.49986112117767334
[2/3, 50/938] Training Loss: 0.503567236661911
[2/3, 60/938] Training Loss: 0.4913310557603836
[2/3, 70/938] Training Loss: 0.4847593456506729
[2/3, 80/938] Training Loss: 0.47908838987350466
[2/3, 90/938] Training Loss: 0.47791844606399536
[2/3, 100/938] Training Loss: 0.47197296023368834
[2/3, 110/938] Training Loss: 0.4650776952505112
[2/3, 120/938] Training Loss: 0.4797851949930191
[2/3, 130/938] Training Loss: 0.47932700514793397
[2/3, 140/938] Training Loss: 0.45604985356330874
[2/3, 150/938] Training Loss: 0.4555528074502945
[2/3, 160/938] Training Loss: 0.47083854377269746
[2/3, 170/938] Training Loss: 0.4833718866109848
[2/3, 180/938] Training Loss: 0.4759210288524628
[2/3, 190/938] Training Loss: 0.46727199852466583
[2/3, 200/938] Training Loss: 0.46643213331699374
[2/3, 210/938] Training Loss: 0.46272110641002656
[2/3, 220/938] Training Loss: 0.44562724232673645
[2/3, 230/938] Training Loss: 0.43495602905750275
[2/3, 240/938] Training Loss: 0.4615240812301636
[2/3, 250/938] Training Loss: 0.45239525437355044
[2/3, 260/938] Training Loss: 0.461622616648674
[2/3, 270/938] Training Loss: 0.44735791981220246
[2/3, 280/938] Training Loss: 0.44767544865608216
[2/3, 290/938] Training Loss: 0.4400662213563919
[2/3, 300/938] Training Loss: 0.43808040022850037
[2/3, 310/938] Training Loss: 0.4391838043928146
[2/3, 320/938] Training Loss: 0.4331490695476532
[2/3, 330/938] Training Loss: 0.41709923148155215
[2/3, 340/938] Training Loss: 0.42530795335769656
[2/3, 350/938] Training Loss: 0.41360802948474884
[2/3, 360/938] Training Loss: 0.42237668633461
[2/3, 370/938] Training Loss: 0.4252642810344696
[2/3, 380/938] Training Loss: 0.4263093024492264
[2/3, 390/938] Training Loss: 0.4189868688583374
[2/3, 400/938] Training Loss: 0.4093610107898712
[2/3, 410/938] Training Loss: 0.4104930251836777
[2/3, 420/938] Training Loss: 0.39907086789608004
[2/3, 430/938] Training Loss: 0.4052471548318863
[2/3, 440/938] Training Loss: 0.39431188106536863
[2/3, 450/938] Training Loss: 0.37972200512886045
[2/3, 460/938] Training Loss: 0.3905777305364609
[2/3, 470/938] Training Loss: 0.39502675235271456
[2/3, 480/938] Training Loss: 0.39158807694911957
[2/3, 490/938] Training Loss: 0.3802270472049713
[2/3, 500/938] Training Loss: 0.3843358278274536
[2/3, 510/938] Training Loss: 0.40616068840026853
[2/3, 520/938] Training Loss: 0.41406815946102143
[2/3, 530/938] Training Loss: 0.3934656232595444
[2/3, 540/938] Training Loss: 0.3834991276264191
[2/3, 550/938] Training Loss: 0.37122614979743956
[2/3, 560/938] Training Loss: 0.3662480294704437
[2/3, 570/938] Training Loss: 0.3819724261760712
[2/3, 580/938] Training Loss: 0.36756775081157683
[2/3, 590/938] Training Loss: 0.3644909650087357
[2/3, 600/938] Training Loss: 0.3502987205982208
[2/3, 610/938] Training Loss: 0.34170243740081785
[2/3, 620/938] Training Loss: 0.33978030681610105
[2/3, 630/938] Training Loss: 0.3482558250427246
[2/3, 640/938] Training Loss: 0.34691556692123415
[2/3, 650/938] Training Loss: 0.3387933999300003
[2/3, 660/938] Training Loss: 0.34292404651641845
[2/3, 670/938] Training Loss: 0.33841938972473146
[2/3, 680/938] Training Loss: 0.33628931939601897
[2/3, 690/938] Training Loss: 0.3173505932092667
[2/3, 700/938] Training Loss: 0.33538084030151366
[2/3, 710/938] Training Loss: 0.32715240120887756
[2/3, 720/938] Training Loss: 0.32660748064517975
[2/3, 730/938] Training Loss: 0.3290164589881897
[2/3, 740/938] Training Loss: 0.32389419972896577
[2/3, 750/938] Training Loss: 0.31367231607437135
[2/3, 760/938] Training Loss: 0.31699852645397186
[2/3, 770/938] Training Loss: 0.31755329966545104
[2/3, 780/938] Training Loss: 0.3230745315551758
[2/3, 790/938] Training Loss: 0.3142493814229965
[2/3, 800/938] Training Loss: 0.30858156085014343
[2/3, 810/938] Training Loss: 0.30852909088134767
[2/3, 820/938] Training Loss: 0.29814873933792113
[2/3, 830/938] Training Loss: 0.3088932245969772
[2/3, 840/938] Training Loss: 0.3074115693569183
[2/3, 850/938] Training Loss: 0.3050947904586792
[2/3, 860/938] Training Loss: 0.3051361918449402
[2/3, 870/938] Training Loss: 0.295232230424881
[2/3, 880/938] Training Loss: 0.29134797751903535
[2/3, 890/938] Training Loss: 0.2882977992296219
[2/3, 900/938] Training Loss: 0.29356718361377715
[2/3, 910/938] Training Loss: 0.2912407577037811
[2/3, 920/938] Training Loss: 0.2938435643911362
[2/3, 930/938] Training Loss: 0.2958485037088394
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.3947269939879576
5/5: Training network...
[3/3, 10/938] Training Loss: 0.28980253338813783
[3/3, 20/938] Training Loss: 0.28894367814064026
[3/3, 30/938] Training Loss: 0.28154416680336
[3/3, 40/938] Training Loss: 0.29504447877407075
[3/3, 50/938] Training Loss: 0.28692429065704345
[3/3, 60/938] Training Loss: 0.28167387992143633
[3/3, 70/938] Training Loss: 0.2824273109436035
[3/3, 80/938] Training Loss: 0.28392775505781176
[3/3, 90/938] Training Loss: 0.27713173627853394
[3/3, 100/938] Training Loss: 0.28505114614963534
[3/3, 110/938] Training Loss: 0.28235994577407836
[3/3, 120/938] Training Loss: 0.3052284061908722
[3/3, 130/938] Training Loss: 0.2903125762939453
[3/3, 140/938] Training Loss: 0.28674396574497224
[3/3, 150/938] Training Loss: 0.2715224653482437
[3/3, 160/938] Training Loss: 0.2650381147861481
[3/3, 170/938] Training Loss: 0.2697407051920891
[3/3, 180/938] Training Loss: 0.2952307671308517
[3/3, 190/938] Training Loss: 0.3002289175987244
[3/3, 200/938] Training Loss: 0.2839803546667099
[3/3, 210/938] Training Loss: 0.27140297144651415
[3/3, 220/938] Training Loss: 0.25404333621263503
[3/3, 230/938] Training Loss: 0.26958624869585035
[3/3, 240/938] Training Loss: 0.2647016108036041
[3/3, 250/938] Training Loss: 0.261190102994442
[3/3, 260/938] Training Loss: 0.25779661238193513
[3/3, 270/938] Training Loss: 0.25795255303382875
[3/3, 280/938] Training Loss: 0.25549761205911636
[3/3, 290/938] Training Loss: 0.24931009858846664
[3/3, 300/938] Training Loss: 0.2652561664581299
[3/3, 310/938] Training Loss: 0.25990522652864456
[3/3, 320/938] Training Loss: 0.24974550753831865
[3/3, 330/938] Training Loss: 0.25258102267980576
[3/3, 340/938] Training Loss: 0.24469098150730134
[3/3, 350/938] Training Loss: 0.24715408235788344
[3/3, 360/938] Training Loss: 0.24290921837091445
[3/3, 370/938] Training Loss: 0.24797566831111909
[3/3, 380/938] Training Loss: 0.26626117527484894
[3/3, 390/938] Training Loss: 0.2697799429297447
[3/3, 400/938] Training Loss: 0.2579406097531319
[3/3, 410/938] Training Loss: 0.25402176678180693
[3/3, 420/938] Training Loss: 0.24834526926279069
[3/3, 430/938] Training Loss: 0.2738952785730362
[3/3, 440/938] Training Loss: 0.274995718896389
[3/3, 450/938] Training Loss: 0.24332693666219712
[3/3, 460/938] Training Loss: 0.24636884182691574
[3/3, 470/938] Training Loss: 0.24906659871339798
[3/3, 480/938] Training Loss: 0.24055306166410445
[3/3, 490/938] Training Loss: 0.23773809969425203
[3/3, 500/938] Training Loss: 0.24706557095050813
[3/3, 510/938] Training Loss: 0.24680893421173095
[3/3, 520/938] Training Loss: 0.24154503345489503
[3/3, 530/938] Training Loss: 0.2548025995492935
[3/3, 540/938] Training Loss: 0.24067229479551316
[3/3, 550/938] Training Loss: 0.24620064347982407
[3/3, 560/938] Training Loss: 0.24467599242925644
[3/3, 570/938] Training Loss: 0.24733862578868865
[3/3, 580/938] Training Loss: 0.25477551072835924
[3/3, 590/938] Training Loss: 0.2516566440463066
[3/3, 600/938] Training Loss: 0.23907597810029985
[3/3, 610/938] Training Loss: 0.23869623243808746
[3/3, 620/938] Training Loss: 0.22558349072933198
[3/3, 630/938] Training Loss: 0.22154322862625123
[3/3, 640/938] Training Loss: 0.23146145939826965
[3/3, 650/938] Training Loss: 0.2312946140766144
[3/3, 660/938] Training Loss: 0.22416987419128417
[3/3, 670/938] Training Loss: 0.2289110541343689
[3/3, 680/938] Training Loss: 0.2427964463829994
[3/3, 690/938] Training Loss: 0.2580125957727432
[3/3, 700/938] Training Loss: 0.25162575840950013
[3/3, 710/938] Training Loss: 0.23695233315229416
[3/3, 720/938] Training Loss: 0.22556161880493164
[3/3, 730/938] Training Loss: 0.22577717751264573
[3/3, 740/938] Training Loss: 0.2267645075917244
[3/3, 750/938] Training Loss: 0.2213783785700798
[3/3, 760/938] Training Loss: 0.21598582565784455
[3/3, 770/938] Training Loss: 0.21982991844415664
[3/3, 780/938] Training Loss: 0.2060595452785492
[3/3, 790/938] Training Loss: 0.22217681109905243
[3/3, 800/938] Training Loss: 0.23587244600057602
[3/3, 810/938] Training Loss: 0.21649651378393173
[3/3, 820/938] Training Loss: 0.2175476610660553
[3/3, 830/938] Training Loss: 0.21389144361019136
[3/3, 840/938] Training Loss: 0.21901122182607652
[3/3, 850/938] Training Loss: 0.20822333097457885
[3/3, 860/938] Training Loss: 0.2116354838013649
[3/3, 870/938] Training Loss: 0.20827734619379043
[3/3, 880/938] Training Loss: 0.20225965678691865
[3/3, 890/938] Training Loss: 0.20518849343061446
[3/3, 900/938] Training Loss: 0.20632995218038558
[3/3, 910/938] Training Loss: 0.20192171782255172
[3/3, 920/938] Training Loss: 0.20300494581460954
[3/3, 930/938] Training Loss: 0.20577806383371353
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.3304343312647608
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
0 added
3 added
6 added
7 added
1 added
9 added
8 added
4 added
5 added
2 added
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(