Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/469] Training Loss: 4.229673945903778
[1/3, 20/469] Training Loss: 1.0328459799289704
[1/3, 30/469] Training Loss: 1.019911539554596
[1/3, 40/469] Training Loss: 1.009653377532959
[1/3, 50/469] Training Loss: 0.9618186533451081
[1/3, 60/469] Training Loss: 0.9734718024730682
[1/3, 70/469] Training Loss: 0.9559670805931091
[1/3, 80/469] Training Loss: 0.941928893327713
[1/3, 90/469] Training Loss: 0.9480946362018585
[1/3, 100/469] Training Loss: 0.948622065782547
[1/3, 110/469] Training Loss: 0.9198862493038178
[1/3, 120/469] Training Loss: 0.8848740994930268
[1/3, 130/469] Training Loss: 0.8921543955802917
[1/3, 140/469] Training Loss: 0.9026321291923523
[1/3, 150/469] Training Loss: 0.8979202508926392
[1/3, 160/469] Training Loss: 0.872645902633667
[1/3, 170/469] Training Loss: 0.8854802012443542
[1/3, 180/469] Training Loss: 0.8268114328384399
[1/3, 190/469] Training Loss: 0.864655488729477
[1/3, 200/469] Training Loss: 0.8495218336582184
[1/3, 210/469] Training Loss: 0.8267486095428467
[1/3, 220/469] Training Loss: 0.8119610786437989
[1/3, 230/469] Training Loss: 0.8123879432678223
[1/3, 240/469] Training Loss: 0.800579959154129
[1/3, 250/469] Training Loss: 0.8002459466457367
[1/3, 260/469] Training Loss: 0.7942573845386505
[1/3, 270/469] Training Loss: 0.7733964085578918
[1/3, 280/469] Training Loss: 0.7672742307186127
[1/3, 290/469] Training Loss: 0.7372919142246246
[1/3, 300/469] Training Loss: 0.7690899014472962
[1/3, 310/469] Training Loss: 0.7486600518226624
[1/3, 320/469] Training Loss: 0.7334203481674194
[1/3, 330/469] Training Loss: 0.7867734968662262
[1/3, 340/469] Training Loss: 0.7576213121414185
[1/3, 350/469] Training Loss: 0.7356819987297059
[1/3, 360/469] Training Loss: 0.7219544053077698
[1/3, 370/469] Training Loss: 0.6823982059955597
[1/3, 380/469] Training Loss: 0.7087186515331269
[1/3, 390/469] Training Loss: 0.7135390520095826
[1/3, 400/469] Training Loss: 0.682339072227478
[1/3, 410/469] Training Loss: 0.6426470637321472
[1/3, 420/469] Training Loss: 0.6688541412353516
[1/3, 430/469] Training Loss: 0.6709048449993134
[1/3, 440/469] Training Loss: 0.6591019093990326
[1/3, 450/469] Training Loss: 0.6664397537708282
[1/3, 460/469] Training Loss: 0.6421066582202911
5/5: Testing network...
Testing: 1/79
Testing: 2/79
Testing: 3/79
Testing: 4/79
Testing: 5/79
Testing: 6/79
Testing: 7/79
Testing: 8/79
Testing: 9/79
Testing: 10/79
Testing: 11/79
Testing: 12/79
Testing: 13/79
Testing: 14/79
Testing: 15/79
Testing: 16/79
Testing: 17/79
Testing: 18/79
Testing: 19/79
Testing: 20/79
Testing: 21/79
Testing: 22/79
Testing: 23/79
Testing: 24/79
Testing: 25/79
Testing: 26/79
Testing: 27/79
Testing: 28/79
Testing: 29/79
Testing: 30/79
Testing: 31/79
Testing: 32/79
Testing: 33/79
Testing: 34/79
Testing: 35/79
Testing: 36/79
Testing: 37/79
Testing: 38/79
Testing: 39/79
Testing: 40/79
Testing: 41/79
Testing: 42/79
Testing: 43/79
Testing: 44/79
Testing: 45/79
Testing: 46/79
Testing: 47/79
Testing: 48/79
Testing: 49/79
Testing: 50/79
Testing: 51/79
Testing: 52/79
Testing: 53/79
Testing: 54/79
Testing: 55/79
Testing: 56/79
Testing: 57/79
Testing: 58/79
Testing: 59/79
Testing: 60/79
Testing: 61/79
Testing: 62/79
Testing: 63/79
Testing: 64/79
Testing: 65/79
Testing: 66/79
Testing: 67/79
Testing: 68/79
Testing: 69/79
Testing: 70/79
Testing: 71/79
Testing: 72/79
Testing: 73/79
Testing: 74/79
Testing: 75/79
Testing: 76/79
Testing: 77/79
Testing: 78/79
Testing: 79/79
Testing Loss: 0.6346279357160841
5/5: Training network...
[2/3, 10/469] Training Loss: 0.6455887377262115
[2/3, 20/469] Training Loss: 0.6131947576999665
[2/3, 30/469] Training Loss: 0.6332403540611267
[2/3, 40/469] Training Loss: 0.6181923031806946
[2/3, 50/469] Training Loss: 0.6170821249485016
[2/3, 60/469] Training Loss: 0.6183070480823517
[2/3, 70/469] Training Loss: 0.6512327671051026
[2/3, 80/469] Training Loss: 0.6433228015899658
[2/3, 90/469] Training Loss: 0.6137479424476624
[2/3, 100/469] Training Loss: 0.6031992793083191
[2/3, 110/469] Training Loss: 0.606564724445343
[2/3, 120/469] Training Loss: 0.5944728851318359
[2/3, 130/469] Training Loss: 0.5899367749691009
[2/3, 140/469] Training Loss: 0.5806948482990265
[2/3, 150/469] Training Loss: 0.5638970494270324
[2/3, 160/469] Training Loss: 0.5799717545509339
[2/3, 170/469] Training Loss: 0.5620622754096984
[2/3, 180/469] Training Loss: 0.5649907886981964
[2/3, 190/469] Training Loss: 0.5749749362468719
[2/3, 200/469] Training Loss: 0.5719348788261414
[2/3, 210/469] Training Loss: 0.5570418000221252
[2/3, 220/469] Training Loss: 0.5699779748916626
[2/3, 230/469] Training Loss: 0.5663062632083893
[2/3, 240/469] Training Loss: 0.5683296203613282
[2/3, 250/469] Training Loss: 0.5477132678031922
[2/3, 260/469] Training Loss: 0.5578655481338501
[2/3, 270/469] Training Loss: 0.571163272857666
[2/3, 280/469] Training Loss: 0.5568637073040008
[2/3, 290/469] Training Loss: 0.5456282198429108
[2/3, 300/469] Training Loss: 0.5492468476295471
[2/3, 310/469] Training Loss: 0.5841440707445145
[2/3, 320/469] Training Loss: 0.6472578287124634
[2/3, 330/469] Training Loss: 0.6113327205181122
[2/3, 340/469] Training Loss: 0.5691468894481659
[2/3, 350/469] Training Loss: 0.55100217461586
[2/3, 360/469] Training Loss: 0.5563122272491455
[2/3, 370/469] Training Loss: 0.5476182371377945
[2/3, 380/469] Training Loss: 0.5544502139091492
[2/3, 390/469] Training Loss: 0.5438830435276032
[2/3, 400/469] Training Loss: 0.5858898252248764
[2/3, 410/469] Training Loss: 0.7375664710998535
[2/3, 420/469] Training Loss: 0.6808290541172027
[2/3, 430/469] Training Loss: 0.6313467144966125
[2/3, 440/469] Training Loss: 0.573567795753479
[2/3, 450/469] Training Loss: 0.5820087552070617
[2/3, 460/469] Training Loss: 0.5869062602519989
5/5: Testing network...
Testing: 1/79
Testing: 2/79
Testing: 3/79
Testing: 4/79
Testing: 5/79
Testing: 6/79
Testing: 7/79
Testing: 8/79
Testing: 9/79
Testing: 10/79
Testing: 11/79
Testing: 12/79
Testing: 13/79
Testing: 14/79
Testing: 15/79
Testing: 16/79
Testing: 17/79
Testing: 18/79
Testing: 19/79
Testing: 20/79
Testing: 21/79
Testing: 22/79
Testing: 23/79
Testing: 24/79
Testing: 25/79
Testing: 26/79
Testing: 27/79
Testing: 28/79
Testing: 29/79
Testing: 30/79
Testing: 31/79
Testing: 32/79
Testing: 33/79
Testing: 34/79
Testing: 35/79
Testing: 36/79
Testing: 37/79
Testing: 38/79
Testing: 39/79
Testing: 40/79
Testing: 41/79
Testing: 42/79
Testing: 43/79
Testing: 44/79
Testing: 45/79
Testing: 46/79
Testing: 47/79
Testing: 48/79
Testing: 49/79
Testing: 50/79
Testing: 51/79
Testing: 52/79
Testing: 53/79
Testing: 54/79
Testing: 55/79
Testing: 56/79
Testing: 57/79
Testing: 58/79
Testing: 59/79
Testing: 60/79
Testing: 61/79
Testing: 62/79
Testing: 63/79
Testing: 64/79
Testing: 65/79
Testing: 66/79
Testing: 67/79
Testing: 68/79
Testing: 69/79
Testing: 70/79
Testing: 71/79
Testing: 72/79
Testing: 73/79
Testing: 74/79
Testing: 75/79
Testing: 76/79
Testing: 77/79
Testing: 78/79
Testing: 79/79
Testing Loss: 0.6045376643538475
5/5: Training network...
[3/3, 10/469] Training Loss: 0.5744745314121247
[3/3, 20/469] Training Loss: 0.5746053814888
[3/3, 30/469] Training Loss: 0.5474236369132995
[3/3, 40/469] Training Loss: 0.5557738065719604
[3/3, 50/469] Training Loss: 0.559109365940094
[3/3, 60/469] Training Loss: 0.5374484419822693
[3/3, 70/469] Training Loss: 0.5376381158828736
[3/3, 80/469] Training Loss: 0.5382961362600327
[3/3, 90/469] Training Loss: 0.5416485905647278
[3/3, 100/469] Training Loss: 0.5357440352439881
[3/3, 110/469] Training Loss: 0.533498877286911
[3/3, 120/469] Training Loss: 0.5401899933815002
[3/3, 130/469] Training Loss: 0.5459924459457397
[3/3, 140/469] Training Loss: 0.5401057839393616
[3/3, 150/469] Training Loss: 0.5357070356607437
[3/3, 160/469] Training Loss: 0.5175329178571701
[3/3, 170/469] Training Loss: 0.5355831742286682
[3/3, 180/469] Training Loss: 0.535486051440239
[3/3, 190/469] Training Loss: 0.6292406380176544
[3/3, 200/469] Training Loss: 0.5847537696361542
[3/3, 210/469] Training Loss: 0.5179241240024567
[3/3, 220/469] Training Loss: 0.5254395544528961
[3/3, 230/469] Training Loss: 0.5194309115409851
[3/3, 240/469] Training Loss: 0.5288729548454285
[3/3, 250/469] Training Loss: 0.49437623620033266
[3/3, 260/469] Training Loss: 0.5241274207830429
[3/3, 270/469] Training Loss: 0.5125835925340653
[3/3, 280/469] Training Loss: 0.5276249766349792
[3/3, 290/469] Training Loss: 0.5097110360860825
[3/3, 300/469] Training Loss: 0.5115422546863556
[3/3, 310/469] Training Loss: 0.5179967790842056
[3/3, 320/469] Training Loss: 0.5005989760160446
[3/3, 330/469] Training Loss: 0.5020131707191468
[3/3, 340/469] Training Loss: 0.5753752231597901
[3/3, 350/469] Training Loss: 0.6625485956668854
[3/3, 360/469] Training Loss: 0.6100938260555268
[3/3, 370/469] Training Loss: 0.584313052892685
[3/3, 380/469] Training Loss: 0.5626477777957917
[3/3, 390/469] Training Loss: 0.543749338388443
[3/3, 400/469] Training Loss: 0.5425713717937469
[3/3, 410/469] Training Loss: 0.52885120511055
[3/3, 420/469] Training Loss: 0.5371446311473846
[3/3, 430/469] Training Loss: 0.5375081986188889
[3/3, 440/469] Training Loss: 0.5253517091274261
[3/3, 450/469] Training Loss: 0.5313431113958359
[3/3, 460/469] Training Loss: 0.5225401222705841
5/5: Testing network...
Testing: 1/79
Testing: 2/79
Testing: 3/79
Testing: 4/79
Testing: 5/79
Testing: 6/79
Testing: 7/79
Testing: 8/79
Testing: 9/79
Testing: 10/79
Testing: 11/79
Testing: 12/79
Testing: 13/79
Testing: 14/79
Testing: 15/79
Testing: 16/79
Testing: 17/79
Testing: 18/79
Testing: 19/79
Testing: 20/79
Testing: 21/79
Testing: 22/79
Testing: 23/79
Testing: 24/79
Testing: 25/79
Testing: 26/79
Testing: 27/79
Testing: 28/79
Testing: 29/79
Testing: 30/79
Testing: 31/79
Testing: 32/79
Testing: 33/79
Testing: 34/79
Testing: 35/79
Testing: 36/79
Testing: 37/79
Testing: 38/79
Testing: 39/79
Testing: 40/79
Testing: 41/79
Testing: 42/79
Testing: 43/79
Testing: 44/79
Testing: 45/79
Testing: 46/79
Testing: 47/79
Testing: 48/79
Testing: 49/79
Testing: 50/79
Testing: 51/79
Testing: 52/79
Testing: 53/79
Testing: 54/79
Testing: 55/79
Testing: 56/79
Testing: 57/79
Testing: 58/79
Testing: 59/79
Testing: 60/79
Testing: 61/79
Testing: 62/79
Testing: 63/79
Testing: 64/79
Testing: 65/79
Testing: 66/79
Testing: 67/79
Testing: 68/79
Testing: 69/79
Testing: 70/79
Testing: 71/79
Testing: 72/79
Testing: 73/79
Testing: 74/79
Testing: 75/79
Testing: 76/79
Testing: 77/79
Testing: 78/79
Testing: 79/79
Testing Loss: 0.5750363088789441
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/79
Batch: 2/79
Batch: 3/79
Batch: 4/79
Batch: 5/79
Batch: 6/79
Batch: 7/79
Batch: 8/79
Batch: 9/79
Batch: 10/79
Batch: 11/79
Batch: 12/79
Batch: 13/79
Batch: 14/79
Batch: 15/79
Batch: 16/79
Batch: 17/79
Batch: 18/79
Batch: 19/79
Batch: 20/79
Batch: 21/79
Batch: 22/79
Batch: 23/79
Batch: 24/79
Batch: 25/79
Batch: 26/79
Batch: 27/79
Batch: 28/79
Batch: 29/79
Batch: 30/79
Batch: 31/79
Batch: 32/79
Batch: 33/79
Batch: 34/79
Batch: 35/79
Batch: 36/79
Batch: 37/79
Batch: 38/79
Batch: 39/79
Batch: 40/79
Batch: 41/79
Batch: 42/79
Batch: 43/79
Batch: 44/79
Batch: 45/79
Batch: 46/79
Batch: 47/79
Batch: 48/79
Batch: 49/79
Batch: 50/79
Batch: 51/79
Batch: 52/79
Batch: 53/79
Batch: 54/79
Batch: 55/79
Batch: 56/79
Batch: 57/79
Batch: 58/79
Batch: 59/79
Batch: 60/79
Batch: 61/79
Batch: 62/79
Batch: 63/79
Batch: 64/79
Batch: 65/79
Batch: 66/79
Batch: 67/79
Batch: 68/79
Batch: 69/79
Batch: 70/79
Batch: 71/79
Batch: 72/79
Batch: 73/79
Batch: 74/79
Batch: 75/79
Batch: 76/79
Batch: 77/79
Batch: 78/79
Batch: 79/79
9 added
2 added
7 added
3 added
1 added
4 added
8 added
5 added
0 added
6 added
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
Applying t-SNE