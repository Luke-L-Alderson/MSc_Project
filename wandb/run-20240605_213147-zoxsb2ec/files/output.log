Starting Sweep: Batch Size: 32, Learning Rate: 0.01
Making datasets and defining subsets
Training: 60000 -> 18000
Testing: 10000 -> 3000
Making Subsets
Training: 18000
Testing: 3000
Making Dataloaders
Defining network
2024-06-05 21:31:49.602813
Training!
0:00:00.010416
[1/9, 29/563] Training Loss: 2.3983349800109863
[1/9, 58/563] Training Loss: 1.1674515008926392
[1/9, 87/563] Training Loss: 1.1330851316452026
[1/9, 116/563] Training Loss: 1.1279510259628296
[1/9, 145/563] Training Loss: 1.1369909048080444
[1/9, 174/563] Training Loss: 1.1538447141647339
[1/9, 203/563] Training Loss: 1.1583274602890015
[1/9, 232/563] Training Loss: 1.1388686895370483
[1/9, 261/563] Training Loss: 1.1461331844329834
[1/9, 290/563] Training Loss: 1.164871335029602
[1/9, 319/563] Training Loss: 1.161407470703125
[1/9, 348/563] Training Loss: 1.1543693542480469
[1/9, 377/563] Training Loss: 1.1364411115646362
[1/9, 406/563] Training Loss: 1.1318979263305664
[1/9, 435/563] Training Loss: 1.123400092124939
[1/9, 464/563] Training Loss: 1.0953657627105713
[1/9, 493/563] Training Loss: 1.0639281272888184
[1/9, 522/563] Training Loss: 1.063563346862793
[1/9, 551/563] Training Loss: 1.061525821685791
Testing!
[1/9, 1/94]
[1/9, 6/94]
[1/9, 11/94]
[1/9, 16/94]
[1/9, 21/94]
[1/9, 26/94]
[1/9, 31/94]
[1/9, 36/94]
[1/9, 41/94]
[1/9, 46/94]
[1/9, 51/94]
[1/9, 56/94]
[1/9, 61/94]
[1/9, 66/94]
[1/9, 71/94]
[1/9, 76/94]
[1/9, 81/94]
[1/9, 86/94]
[1/9, 91/94]
Testing Loss: 1.0656412839889526
Training!
0:12:01.573001
[2/9, 29/563] Training Loss: 1.050918459892273
[2/9, 58/563] Training Loss: 1.049440860748291
[2/9, 87/563] Training Loss: 1.035327434539795
[2/9, 116/563] Training Loss: 1.0491844415664673
[2/9, 145/563] Training Loss: 1.0480942726135254
[2/9, 174/563] Training Loss: 1.0519556999206543
[2/9, 203/563] Training Loss: 1.0780888795852661
[2/9, 232/563] Training Loss: 1.056484580039978
[2/9, 261/563] Training Loss: 1.060534954071045
[2/9, 290/563] Training Loss: 1.0567715167999268
[2/9, 319/563] Training Loss: 1.0394597053527832
[2/9, 348/563] Training Loss: 1.0524623394012451
[2/9, 377/563] Training Loss: 1.0350556373596191
[2/9, 406/563] Training Loss: 1.0600379705429077
[2/9, 435/563] Training Loss: 1.0548423528671265
[2/9, 464/563] Training Loss: 1.043577790260315
[2/9, 493/563] Training Loss: 1.0346168279647827
[2/9, 522/563] Training Loss: 1.0534918308258057
[2/9, 551/563] Training Loss: 1.0532052516937256
Testing!
[2/9, 1/94]
[2/9, 6/94]
[2/9, 11/94]
[2/9, 16/94]
[2/9, 21/94]
[2/9, 26/94]
[2/9, 31/94]
[2/9, 36/94]
[2/9, 41/94]
[2/9, 46/94]
[2/9, 51/94]
[2/9, 56/94]
[2/9, 61/94]
[2/9, 66/94]
[2/9, 71/94]
[2/9, 76/94]
[2/9, 81/94]
[2/9, 86/94]
[2/9, 91/94]
Testing Loss: 1.016581416130066
Training!
0:24:00.062410
[3/9, 29/563] Training Loss: 1.0334885120391846
[3/9, 58/563] Training Loss: 1.0749565362930298
[3/9, 87/563] Training Loss: 1.0302824974060059
[3/9, 116/563] Training Loss: 1.048083782196045
[3/9, 145/563] Training Loss: 1.0688860416412354
[3/9, 174/563] Training Loss: 1.0388038158416748
[3/9, 203/563] Training Loss: 1.0492600202560425
[3/9, 232/563] Training Loss: 1.0391461849212646
[3/9, 261/563] Training Loss: 1.0575072765350342
[34m[1mwandb[39m[22m: Network error (ConnectionError), entering retry loop.
[3/9, 290/563] Training Loss: 1.0526291131973267
[3/9, 319/563] Training Loss: 1.0522552728652954
[3/9, 348/563] Training Loss: 1.025687336921692
[3/9, 377/563] Training Loss: 1.0408730506896973
[3/9, 406/563] Training Loss: 1.0406572818756104
[3/9, 435/563] Training Loss: 1.0624117851257324
[3/9, 464/563] Training Loss: 1.0476425886154175
[3/9, 493/563] Training Loss: 1.0407425165176392
[3/9, 522/563] Training Loss: 1.060459017753601
[3/9, 551/563] Training Loss: 1.0424020290374756
Testing!
[3/9, 1/94]
[3/9, 6/94]
[3/9, 11/94]
[3/9, 16/94]
[3/9, 21/94]
[3/9, 26/94]
[3/9, 31/94]
[3/9, 36/94]
[3/9, 41/94]
[3/9, 46/94]
[3/9, 51/94]
[3/9, 56/94]
[3/9, 61/94]
[3/9, 66/94]
[3/9, 71/94]
[3/9, 76/94]
[3/9, 81/94]
[3/9, 86/94]
[3/9, 91/94]
Testing Loss: 1.0728907585144043
Training!
0:35:59.164269
[4/9, 29/563] Training Loss: 1.0496501922607422
[4/9, 58/563] Training Loss: 1.0396493673324585
[4/9, 87/563] Training Loss: 1.056248664855957
[4/9, 116/563] Training Loss: 1.0534265041351318
[4/9, 145/563] Training Loss: 1.0595206022262573
[4/9, 174/563] Training Loss: 1.040703296661377
[4/9, 203/563] Training Loss: 1.034712553024292
[4/9, 232/563] Training Loss: 1.0319998264312744
[4/9, 261/563] Training Loss: 1.0572324991226196
[4/9, 290/563] Training Loss: 1.054828405380249
[4/9, 319/563] Training Loss: 1.0299406051635742
[4/9, 348/563] Training Loss: 1.0379290580749512
[4/9, 377/563] Training Loss: 1.051058292388916
[4/9, 406/563] Training Loss: 1.0259987115859985
[4/9, 435/563] Training Loss: 1.0259313583374023
[34m[1mwandb[39m[22m: Network error (ConnectionError), entering retry loop.
[4/9, 464/563] Training Loss: 1.0229382514953613
[4/9, 493/563] Training Loss: 1.0572621822357178
[4/9, 522/563] Training Loss: 1.0460957288742065
[4/9, 551/563] Training Loss: 1.0352343320846558
Testing!
[4/9, 1/94]
[4/9, 6/94]
[4/9, 11/94]
[4/9, 16/94]
[4/9, 21/94]
[4/9, 26/94]
[4/9, 31/94]
[4/9, 36/94]
[4/9, 41/94]
[4/9, 46/94]
[4/9, 51/94]
[4/9, 56/94]
[4/9, 61/94]
[4/9, 66/94]
[4/9, 71/94]
[4/9, 76/94]
[4/9, 81/94]
[4/9, 86/94]
[4/9, 91/94]
Testing Loss: 1.0632452964782715
Training!
0:47:57.737197
[5/9, 29/563] Training Loss: 1.0337852239608765
[5/9, 58/563] Training Loss: 1.040443778038025
[5/9, 87/563] Training Loss: 1.0217342376708984
[5/9, 116/563] Training Loss: 1.035843849182129
[5/9, 145/563] Training Loss: 1.0334160327911377
[5/9, 174/563] Training Loss: 1.0497452020645142
[5/9, 203/563] Training Loss: 1.0591175556182861
[5/9, 232/563] Training Loss: 1.0713818073272705
[5/9, 261/563] Training Loss: 1.0288727283477783
[5/9, 290/563] Training Loss: 1.058168888092041
[5/9, 319/563] Training Loss: 1.0273301601409912
[5/9, 348/563] Training Loss: 1.0310029983520508
[5/9, 377/563] Training Loss: 1.0383342504501343
[5/9, 406/563] Training Loss: 1.0473638772964478
[5/9, 435/563] Training Loss: 1.0059128999710083
[5/9, 464/563] Training Loss: 1.058443307876587
[5/9, 493/563] Training Loss: 1.0160963535308838
[5/9, 522/563] Training Loss: 0.9989429712295532
[5/9, 551/563] Training Loss: 1.0502638816833496
Testing!
[5/9, 1/94]
[5/9, 6/94]
[5/9, 11/94]
[5/9, 16/94]
[5/9, 21/94]
[5/9, 26/94]
[5/9, 31/94]
[5/9, 36/94]
[5/9, 41/94]
[5/9, 46/94]
[5/9, 51/94]
[5/9, 56/94]
[5/9, 61/94]
[5/9, 66/94]
[5/9, 71/94]
[5/9, 76/94]
[5/9, 81/94]
[5/9, 86/94]
[5/9, 91/94]
Testing Loss: 1.0293227434158325
Training!
0:59:54.074549
[6/9, 29/563] Training Loss: 1.0874967575073242
[6/9, 58/563] Training Loss: 1.174430012702942
[6/9, 87/563] Training Loss: 1.1568284034729004
[6/9, 116/563] Training Loss: 1.1527085304260254
[6/9, 145/563] Training Loss: 1.1553560495376587
[6/9, 174/563] Training Loss: 1.1455392837524414
[6/9, 203/563] Training Loss: 1.153693675994873
[6/9, 232/563] Training Loss: 1.155809760093689
[6/9, 261/563] Training Loss: 1.146498680114746
[6/9, 290/563] Training Loss: 1.1736479997634888
[6/9, 319/563] Training Loss: 1.1359801292419434
[6/9, 348/563] Training Loss: 1.130881905555725
[6/9, 377/563] Training Loss: 1.134932518005371
[6/9, 406/563] Training Loss: 1.1708701848983765
[6/9, 435/563] Training Loss: 1.145784854888916
[6/9, 464/563] Training Loss: 1.1373496055603027
[6/9, 493/563] Training Loss: 1.135265827178955
[6/9, 522/563] Training Loss: 1.1479239463806152
[6/9, 551/563] Training Loss: 1.130430817604065
Testing!
[6/9, 1/94]
[6/9, 6/94]
[6/9, 11/94]
[6/9, 16/94]
[6/9, 21/94]
[6/9, 26/94]
[6/9, 31/94]
[6/9, 36/94]
[6/9, 41/94]
[6/9, 46/94]
[6/9, 51/94]
[6/9, 56/94]
[6/9, 61/94]
[6/9, 66/94]
[6/9, 71/94]
[6/9, 76/94]
[6/9, 81/94]
[6/9, 86/94]
[6/9, 91/94]
Testing Loss: 1.125575065612793
Training!
1:11:53.435658
[7/9, 29/563] Training Loss: 1.160155177116394
[7/9, 58/563] Training Loss: 1.1453938484191895
[7/9, 87/563] Training Loss: 1.0939844846725464
[7/9, 116/563] Training Loss: 1.1519163846969604
[7/9, 145/563] Training Loss: 1.151184320449829
[34m[1mwandb[39m[22m: Network error (ConnectionError), entering retry loop.
[7/9, 174/563] Training Loss: 1.1462031602859497
[7/9, 203/563] Training Loss: 1.1593319177627563
[7/9, 232/563] Training Loss: 1.1486129760742188
[7/9, 261/563] Training Loss: 1.1306904554367065
[7/9, 290/563] Training Loss: 1.13259756565094
[7/9, 319/563] Training Loss: 1.1479315757751465
[7/9, 348/563] Training Loss: 1.1140090227127075
[7/9, 377/563] Training Loss: 1.1408567428588867
[7/9, 406/563] Training Loss: 1.1197123527526855
[7/9, 435/563] Training Loss: 1.1686530113220215
[7/9, 464/563] Training Loss: 1.1295565366744995
[7/9, 493/563] Training Loss: 1.1687207221984863
[7/9, 522/563] Training Loss: 1.140083909034729
[7/9, 551/563] Training Loss: 1.1538053750991821
Testing!
[7/9, 1/94]
[7/9, 6/94]
[7/9, 11/94]
[7/9, 16/94]
[7/9, 21/94]
[7/9, 26/94]
[7/9, 31/94]
[7/9, 36/94]
[7/9, 41/94]
[7/9, 46/94]
[7/9, 51/94]
[7/9, 56/94]
[7/9, 61/94]
[7/9, 66/94]
[7/9, 71/94]
[7/9, 76/94]
[7/9, 81/94]
[7/9, 86/94]
[7/9, 91/94]
Testing Loss: 1.2117135524749756
Training!
1:23:50.999382
[8/9, 29/563] Training Loss: 1.1412060260772705
[8/9, 58/563] Training Loss: 1.118133544921875
[8/9, 87/563] Training Loss: 1.1425879001617432
[8/9, 116/563] Training Loss: 1.18163001537323
[8/9, 145/563] Training Loss: 1.1199190616607666
[8/9, 174/563] Training Loss: 1.1617313623428345
[8/9, 203/563] Training Loss: 1.1285974979400635
[8/9, 232/563] Training Loss: 1.1379308700561523
[8/9, 261/563] Training Loss: 1.1585379838943481
[8/9, 290/563] Training Loss: 1.1154568195343018
[8/9, 319/563] Training Loss: 1.1389801502227783
[8/9, 348/563] Training Loss: 1.136061429977417
[8/9, 377/563] Training Loss: 1.1550883054733276
[8/9, 406/563] Training Loss: 1.17303466796875
[8/9, 435/563] Training Loss: 1.1456053256988525
[8/9, 464/563] Training Loss: 1.1454793214797974
[8/9, 493/563] Training Loss: 1.182314395904541
[8/9, 522/563] Training Loss: 1.1737935543060303
[8/9, 551/563] Training Loss: 1.1575396060943604
Testing!
[8/9, 1/94]
[8/9, 6/94]
[8/9, 11/94]
[8/9, 16/94]
[8/9, 21/94]
[8/9, 26/94]
[8/9, 31/94]
[8/9, 36/94]
[8/9, 41/94]
[8/9, 46/94]
[8/9, 51/94]
[8/9, 56/94]
[8/9, 61/94]
[8/9, 66/94]
[8/9, 71/94]
[8/9, 76/94]
[8/9, 81/94]
[8/9, 86/94]
[8/9, 91/94]
Testing Loss: 1.1984862089157104
Training!
1:35:50.388699
[9/9, 29/563] Training Loss: 1.135353446006775
[9/9, 58/563] Training Loss: 1.1452940702438354
[9/9, 87/563] Training Loss: 1.1573460102081299
[9/9, 116/563] Training Loss: 1.1501885652542114
[9/9, 145/563] Training Loss: 1.1241858005523682
[9/9, 174/563] Training Loss: 1.1245168447494507
[9/9, 203/563] Training Loss: 1.1352381706237793
[9/9, 232/563] Training Loss: 1.1523953676223755
[9/9, 261/563] Training Loss: 1.1500710248947144
[9/9, 290/563] Training Loss: 1.1500250101089478
[9/9, 319/563] Training Loss: 1.1270208358764648
[9/9, 348/563] Training Loss: 1.1376651525497437
[9/9, 377/563] Training Loss: 1.1427942514419556
[9/9, 406/563] Training Loss: 1.1469637155532837
[9/9, 435/563] Training Loss: 1.1454808712005615
[9/9, 464/563] Training Loss: 1.1576858758926392
[9/9, 493/563] Training Loss: 1.144637107849121
[9/9, 522/563] Training Loss: 1.1509054899215698
[9/9, 551/563] Training Loss: 1.1667159795761108
Testing!
[9/9, 1/94]
[9/9, 6/94]
[9/9, 11/94]
[9/9, 16/94]
[9/9, 21/94]
[9/9, 26/94]
[9/9, 31/94]
[9/9, 36/94]
[9/9, 41/94]
[9/9, 46/94]
[9/9, 51/94]
[9/9, 56/94]
[9/9, 61/94]
[9/9, 66/94]
[9/9, 71/94]
[9/9, 76/94]
[9/9, 81/94]
[9/9, 86/94]
[9/9, 91/94]
Testing Loss: 1.1035906076431274
Training and Testing Finished
Assembling test data for t-sne projection
-- 1/94 --
-- 2/94 --
-- 3/94 --
-- 4/94 --
-- 5/94 --
-- 6/94 --
-- 7/94 --
-- 8/94 --
-- 9/94 --
-- 10/94 --
-- 11/94 --
-- 12/94 --
-- 13/94 --
-- 14/94 --
-- 15/94 --
-- 16/94 --
-- 17/94 --
-- 18/94 --
-- 19/94 --
-- 20/94 --
-- 21/94 --
-- 22/94 --
-- 23/94 --
-- 24/94 --
-- 25/94 --
-- 26/94 --
-- 27/94 --
-- 28/94 --
-- 29/94 --
-- 30/94 --
-- 31/94 --
-- 32/94 --
-- 33/94 --
-- 34/94 --
-- 35/94 --
-- 36/94 --
-- 37/94 --
-- 38/94 --
-- 39/94 --
-- 40/94 --
-- 41/94 --
-- 42/94 --
-- 43/94 --
-- 44/94 --
-- 45/94 --
-- 46/94 --
-- 47/94 --
-- 48/94 --
-- 49/94 --
-- 50/94 --
-- 51/94 --
-- 52/94 --
-- 53/94 --
-- 54/94 --
-- 55/94 --
-- 56/94 --
-- 57/94 --
-- 58/94 --
-- 59/94 --
-- 60/94 --
-- 61/94 --
-- 62/94 --
-- 63/94 --
-- 64/94 --
-- 65/94 --
-- 66/94 --
-- 67/94 --
-- 68/94 --
-- 69/94 --
-- 70/94 --
-- 71/94 --
-- 72/94 --
-- 73/94 --
-- 74/94 --
-- 75/94 --
-- 76/94 --
-- 77/94 --
-- 78/94 --
-- 79/94 --
-- 80/94 --
-- 81/94 --
-- 82/94 --
-- 83/94 --
-- 84/94 --
-- 85/94 --
-- 86/94 --
-- 87/94 --
-- 88/94 --
-- 89/94 --
-- 90/94 --
-- 91/94 --
-- 92/94 --
-- 93/94 --
-- 94/94 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation