Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/938] Training Loss: 3.9809163749217986
[1/3, 20/938] Training Loss: 1.0514630258083344
[1/3, 30/938] Training Loss: 1.0510678350925446
[1/3, 40/938] Training Loss: 1.0352711021900176
[1/3, 50/938] Training Loss: 0.9823350548744202
[1/3, 60/938] Training Loss: 1.013880693912506
[1/3, 70/938] Training Loss: 0.9720891416072845
[1/3, 80/938] Training Loss: 0.9867373824119567
[1/3, 90/938] Training Loss: 0.9744165182113648
[1/3, 100/938] Training Loss: 0.9766546964645386
[1/3, 110/938] Training Loss: 0.9441365718841552
[1/3, 120/938] Training Loss: 0.9354695081710815
[1/3, 130/938] Training Loss: 0.968502688407898
[1/3, 140/938] Training Loss: 0.9466968595981597
[1/3, 150/938] Training Loss: 0.9370763301849365
[1/3, 160/938] Training Loss: 0.9017513692378998
[1/3, 170/938] Training Loss: 0.8814500093460083
[1/3, 180/938] Training Loss: 0.9167596578598023
[1/3, 190/938] Training Loss: 0.871945196390152
[1/3, 200/938] Training Loss: 0.9120018899440765
[1/3, 210/938] Training Loss: 0.8957838535308837
[1/3, 220/938] Training Loss: 0.8526082575321198
[1/3, 230/938] Training Loss: 0.8258615791797638
[1/3, 240/938] Training Loss: 0.8398462355136871
[1/3, 250/938] Training Loss: 0.836083048582077
[1/3, 260/938] Training Loss: 0.8390765368938446
[1/3, 270/938] Training Loss: 0.821953558921814
[1/3, 280/938] Training Loss: 0.8369763433933258
[1/3, 290/938] Training Loss: 0.8490425229072571
[1/3, 300/938] Training Loss: 0.824485319852829
[1/3, 310/938] Training Loss: 0.8324391007423401
[1/3, 320/938] Training Loss: 0.8586303055286407
[1/3, 330/938] Training Loss: 0.8061696469783783
[1/3, 340/938] Training Loss: 0.821747463941574
[1/3, 350/938] Training Loss: 0.8003351330757141
[1/3, 360/938] Training Loss: 0.8029249727725982
[1/3, 370/938] Training Loss: 0.7994967877864838
[1/3, 380/938] Training Loss: 0.7986936926841736
[1/3, 390/938] Training Loss: 0.7978353142738343
[1/3, 400/938] Training Loss: 0.7756339907646179
[1/3, 410/938] Training Loss: 0.8000777304172516
[1/3, 420/938] Training Loss: 0.7768224954605103
[1/3, 430/938] Training Loss: 0.797572910785675
[1/3, 440/938] Training Loss: 0.7618285953998566
[1/3, 450/938] Training Loss: 0.7489500224590302
[1/3, 460/938] Training Loss: 0.7645623505115509
[1/3, 470/938] Training Loss: 0.7467195808887481
[1/3, 480/938] Training Loss: 0.7538168072700501
[1/3, 490/938] Training Loss: 0.7410694777965545
[1/3, 500/938] Training Loss: 0.7327845573425293
[1/3, 510/938] Training Loss: 0.7237011551856994
[1/3, 520/938] Training Loss: 0.7056842505931854
[1/3, 530/938] Training Loss: 0.7138534963130951
[1/3, 540/938] Training Loss: 0.7381662428379059
[1/3, 550/938] Training Loss: 0.7115449249744416
[1/3, 560/938] Training Loss: 0.7328290939331055
[1/3, 570/938] Training Loss: 0.7308694839477539
[1/3, 580/938] Training Loss: 0.70164715051651
[1/3, 590/938] Training Loss: 0.7272309541702271
[1/3, 600/938] Training Loss: 0.7231838941574097
[1/3, 610/938] Training Loss: 0.7158728063106536
[1/3, 620/938] Training Loss: 0.7034651756286621
[1/3, 630/938] Training Loss: 0.7014845132827758
[1/3, 640/938] Training Loss: 0.6883956313133239
[1/3, 650/938] Training Loss: 0.6939452886581421
[1/3, 660/938] Training Loss: 0.6881375074386596
[1/3, 670/938] Training Loss: 0.6778862595558166
[1/3, 680/938] Training Loss: 0.673334950208664
[1/3, 690/938] Training Loss: 0.6487361013889312
[1/3, 700/938] Training Loss: 0.6756734251976013
[1/3, 710/938] Training Loss: 0.6637900650501252
[1/3, 720/938] Training Loss: 0.6713958561420441
[1/3, 730/938] Training Loss: 0.6404569327831269
[1/3, 740/938] Training Loss: 0.6577921092510224
[1/3, 750/938] Training Loss: 0.6416942954063416
[1/3, 760/938] Training Loss: 0.6602049350738526
[1/3, 770/938] Training Loss: 0.6613242864608765
[1/3, 780/938] Training Loss: 0.6806680858135223
[1/3, 790/938] Training Loss: 0.6444944500923157
[1/3, 800/938] Training Loss: 0.654084587097168
[1/3, 810/938] Training Loss: 0.6756985306739807
[1/3, 820/938] Training Loss: 0.6380348801612854
[1/3, 830/938] Training Loss: 0.6479597330093384
[1/3, 840/938] Training Loss: 0.6413630247116089
[1/3, 850/938] Training Loss: 0.6327098727226257
[1/3, 860/938] Training Loss: 0.6615355610847473
[1/3, 870/938] Training Loss: 0.6037398278713226
[1/3, 880/938] Training Loss: 0.6448513269424438
[1/3, 890/938] Training Loss: 0.6310230553150177
[1/3, 900/938] Training Loss: 0.6516509532928467
[1/3, 910/938] Training Loss: 0.6341285049915314
[1/3, 920/938] Training Loss: 0.6105745673179627
[1/3, 930/938] Training Loss: 0.5998004853725434
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.6162395564715069
5/5: Training network...
[2/3, 10/938] Training Loss: 0.6225097954273224
[2/3, 20/938] Training Loss: 0.6130608856678009
[2/3, 30/938] Training Loss: 0.6022462725639344
[2/3, 40/938] Training Loss: 0.6044385075569153
[2/3, 50/938] Training Loss: 0.6035371243953704
[2/3, 60/938] Training Loss: 0.5982090711593628
[2/3, 70/938] Training Loss: 0.5865873515605926
[2/3, 80/938] Training Loss: 0.5789414584636688
[2/3, 90/938] Training Loss: 0.5780663073062897
[2/3, 100/938] Training Loss: 0.578093308210373
[2/3, 110/938] Training Loss: 0.5721008241176605
[2/3, 120/938] Training Loss: 0.5942803502082825
[2/3, 130/938] Training Loss: 0.5967836618423462
[2/3, 140/938] Training Loss: 0.5683714985847473
[2/3, 150/938] Training Loss: 0.5740137100219727
[2/3, 160/938] Training Loss: 0.574063491821289
[2/3, 170/938] Training Loss: 0.5782927334308624
[2/3, 180/938] Training Loss: 0.5733408093452453
[2/3, 190/938] Training Loss: 0.5829437971115112
[2/3, 200/938] Training Loss: 0.5823479652404785
[2/3, 210/938] Training Loss: 0.5801992177963257
[2/3, 220/938] Training Loss: 0.5619382500648499
[2/3, 230/938] Training Loss: 0.5455021023750305
[2/3, 240/938] Training Loss: 0.5670754671096802
[2/3, 250/938] Training Loss: 0.5544797629117966
[2/3, 260/938] Training Loss: 0.5712220370769501
[2/3, 270/938] Training Loss: 0.5574623048305511
[2/3, 280/938] Training Loss: 0.5573508501052856
[2/3, 290/938] Training Loss: 0.5475320816040039
[2/3, 300/938] Training Loss: 0.5475614845752717
[2/3, 310/938] Training Loss: 0.5476777672767639
[2/3, 320/938] Training Loss: 0.5461352914571762
[2/3, 330/938] Training Loss: 0.5314999848604203
[2/3, 340/938] Training Loss: 0.548076742887497
[2/3, 350/938] Training Loss: 0.524908035993576
[2/3, 360/938] Training Loss: 0.5348804026842118
[2/3, 370/938] Training Loss: 0.5377404510974884
[2/3, 380/938] Training Loss: 0.5411183536052704
[2/3, 390/938] Training Loss: 0.536022612452507
[2/3, 400/938] Training Loss: 0.5375604450702667
[2/3, 410/938] Training Loss: 0.535557034611702
[2/3, 420/938] Training Loss: 0.5147388190031051
[2/3, 430/938] Training Loss: 0.5268974959850311
[2/3, 440/938] Training Loss: 0.5182557314634323
[2/3, 450/938] Training Loss: 0.49811365008354186
[2/3, 460/938] Training Loss: 0.49619806110858916
[2/3, 470/938] Training Loss: 0.521382263302803
[2/3, 480/938] Training Loss: 0.5263589888811111
[2/3, 490/938] Training Loss: 0.512466761469841
[2/3, 500/938] Training Loss: 0.5187519967556
[2/3, 510/938] Training Loss: 0.5227520138025283
[2/3, 520/938] Training Loss: 0.5149944424629211
[2/3, 530/938] Training Loss: 0.5063890039920806
[2/3, 540/938] Training Loss: 0.5166550368070603
[2/3, 550/938] Training Loss: 0.5089528799057007
[2/3, 560/938] Training Loss: 0.5036079257726669
[2/3, 570/938] Training Loss: 0.5224076271057129
[2/3, 580/938] Training Loss: 0.5100948244333268
[2/3, 590/938] Training Loss: 0.5214282423257828
[2/3, 600/938] Training Loss: 0.5049363821744919
[2/3, 610/938] Training Loss: 0.49560861885547636
[2/3, 620/938] Training Loss: 0.4987803786993027
[2/3, 630/938] Training Loss: 0.5150594770908355
[2/3, 640/938] Training Loss: 0.5100502580404281
[2/3, 650/938] Training Loss: 0.5017700225114823
[2/3, 660/938] Training Loss: 0.5050778597593307
[2/3, 670/938] Training Loss: 0.49394646286964417
[2/3, 680/938] Training Loss: 0.495336252450943
[2/3, 690/938] Training Loss: 0.46669227480888364
[2/3, 700/938] Training Loss: 0.49483183920383456
[2/3, 710/938] Training Loss: 0.4842316389083862
[2/3, 720/938] Training Loss: 0.49144274890422823
[2/3, 730/938] Training Loss: 0.48927209079265593
[2/3, 740/938] Training Loss: 0.48455371260643004
[2/3, 750/938] Training Loss: 0.4636342734098434
[2/3, 760/938] Training Loss: 0.46681608259677887
[2/3, 770/938] Training Loss: 0.4592789888381958
[2/3, 780/938] Training Loss: 0.4651968151330948
[2/3, 790/938] Training Loss: 0.4536906212568283
[2/3, 800/938] Training Loss: 0.4498718291521072
[2/3, 810/938] Training Loss: 0.4511859089136124
[2/3, 820/938] Training Loss: 0.43682539761066436
[2/3, 830/938] Training Loss: 0.450314000248909
[2/3, 840/938] Training Loss: 0.4454204261302948
[2/3, 850/938] Training Loss: 0.45269418358802793
[2/3, 860/938] Training Loss: 0.4449802964925766
[2/3, 870/938] Training Loss: 0.42822275757789613
[2/3, 880/938] Training Loss: 0.42249487042427064
[2/3, 890/938] Training Loss: 0.4171104818582535
[2/3, 900/938] Training Loss: 0.42109454870224
[2/3, 910/938] Training Loss: 0.4170419365167618
[2/3, 920/938] Training Loss: 0.41780968606472013
[2/3, 930/938] Training Loss: 0.41440227031707766
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.5064298415184021
5/5: Training network...
[3/3, 10/938] Training Loss: 0.39994186758995054
[3/3, 20/938] Training Loss: 0.4000866174697876
[3/3, 30/938] Training Loss: 0.3858305186033249
[3/3, 40/938] Training Loss: 0.39092342257499696
[3/3, 50/938] Training Loss: 0.378918519616127
[3/3, 60/938] Training Loss: 0.3716091841459274
[3/3, 70/938] Training Loss: 0.36707628667354586
[3/3, 80/938] Training Loss: 0.36254598796367643
[3/3, 90/938] Training Loss: 0.3547927111387253
[3/3, 100/938] Training Loss: 0.3631988376379013
[3/3, 110/938] Training Loss: 0.35540972352027894
[3/3, 120/938] Training Loss: 0.35831725001335146
[3/3, 130/938] Training Loss: 0.3549846887588501
[3/3, 140/938] Training Loss: 0.35968223214149475
[3/3, 150/938] Training Loss: 0.34944204390048983
[3/3, 160/938] Training Loss: 0.3396772474050522
[3/3, 170/938] Training Loss: 0.3432728499174118
[3/3, 180/938] Training Loss: 0.34526516795158385
[3/3, 190/938] Training Loss: 0.34470481872558595
[3/3, 200/938] Training Loss: 0.3422016739845276
[3/3, 210/938] Training Loss: 0.3337625563144684
[3/3, 220/938] Training Loss: 0.3168917238712311
[3/3, 230/938] Training Loss: 0.3384786158800125
[3/3, 240/938] Training Loss: 0.3329434424638748
[3/3, 250/938] Training Loss: 0.32325955033302306
[3/3, 260/938] Training Loss: 0.32137617766857146
[3/3, 270/938] Training Loss: 0.3206574201583862
[3/3, 280/938] Training Loss: 0.3168686181306839
[3/3, 290/938] Training Loss: 0.3066142201423645
[3/3, 300/938] Training Loss: 0.3228100389242172
[3/3, 310/938] Training Loss: 0.3124991744756699
[3/3, 320/938] Training Loss: 0.2989808708429337
[3/3, 330/938] Training Loss: 0.3039046138525009
[3/3, 340/938] Training Loss: 0.2947924941778183
[3/3, 350/938] Training Loss: 0.2972946435213089
[3/3, 360/938] Training Loss: 0.2904693931341171
[3/3, 370/938] Training Loss: 0.2918571323156357
[3/3, 380/938] Training Loss: 0.3015016704797745
[3/3, 390/938] Training Loss: 0.2917969286441803
[3/3, 400/938] Training Loss: 0.2904499024152756
[3/3, 410/938] Training Loss: 0.2922482043504715
[3/3, 420/938] Training Loss: 0.28030369579792025
[3/3, 430/938] Training Loss: 0.28458426594734193
[3/3, 440/938] Training Loss: 0.2943125545978546
[3/3, 450/938] Training Loss: 0.27273987978696823
[3/3, 460/938] Training Loss: 0.2805817753076553
[3/3, 470/938] Training Loss: 0.28312933892011644
[3/3, 480/938] Training Loss: 0.27805631756782534
[3/3, 490/938] Training Loss: 0.2711707890033722
[3/3, 500/938] Training Loss: 0.2744885623455048
[3/3, 510/938] Training Loss: 0.2713380306959152
[3/3, 520/938] Training Loss: 0.26620119661092756
[3/3, 530/938] Training Loss: 0.28825852572917937
[3/3, 540/938] Training Loss: 0.2709780052304268
[3/3, 550/938] Training Loss: 0.27716327011585234
[3/3, 560/938] Training Loss: 0.27417537569999695
[3/3, 570/938] Training Loss: 0.26524687856435775
[3/3, 580/938] Training Loss: 0.26234743744134903
[3/3, 590/938] Training Loss: 0.2656278669834137
[3/3, 600/938] Training Loss: 0.2616798907518387
[3/3, 610/938] Training Loss: 0.26545688658952715
[3/3, 620/938] Training Loss: 0.25093148499727247
[3/3, 630/938] Training Loss: 0.24924377351999283
[3/3, 640/938] Training Loss: 0.2593706905841827
[3/3, 650/938] Training Loss: 0.2624554201960564
[3/3, 660/938] Training Loss: 0.2577821001410484
[3/3, 670/938] Training Loss: 0.2576230302453041
[3/3, 680/938] Training Loss: 0.25721877068281174
[3/3, 690/938] Training Loss: 0.25644796192646024
[3/3, 700/938] Training Loss: 0.253055265545845
[3/3, 710/938] Training Loss: 0.25129143595695497
[3/3, 720/938] Training Loss: 0.24723624736070632
[3/3, 730/938] Training Loss: 0.25301777571439743
[3/3, 740/938] Training Loss: 0.2597458451986313
[3/3, 750/938] Training Loss: 0.25120729207992554
[3/3, 760/938] Training Loss: 0.24654526263475418
[3/3, 770/938] Training Loss: 0.25521091669797896
[3/3, 780/938] Training Loss: 0.24255325943231582
[3/3, 790/938] Training Loss: 0.24072617739439012
[3/3, 800/938] Training Loss: 0.24263832122087478
[3/3, 810/938] Training Loss: 0.23832019418478012
[3/3, 820/938] Training Loss: 0.24791272431612016
[3/3, 830/938] Training Loss: 0.24684616029262543
[3/3, 840/938] Training Loss: 0.2543910101056099
[3/3, 850/938] Training Loss: 0.23959712833166122
[3/3, 860/938] Training Loss: 0.24647749960422516
[3/3, 870/938] Training Loss: 0.24226294308900834
[3/3, 880/938] Training Loss: 0.23540420979261398
[3/3, 890/938] Training Loss: 0.23972563147544862
[3/3, 900/938] Training Loss: 0.2413426622748375
[3/3, 910/938] Training Loss: 0.23596690893173217
[3/3, 920/938] Training Loss: 0.2360421821475029
[3/3, 930/938] Training Loss: 0.23880578130483626
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.4156109203894934
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
0 added
3 added
6 added
7 added
1 added
9 added
8 added
4 added
5 added
2 added
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(