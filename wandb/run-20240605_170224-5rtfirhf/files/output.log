Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Subsets
Training: 6000
Testing: 1000
(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.7725, 0.2118, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0824, 0.9294, 0.3451, 0.0353, 0.0353, 0.0353, 0.0353,
          0.3922, 0.4314, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.6588, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0627, 0.9294, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961, 0.9961,
          0.9961, 0.9961, 0.5529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0784, 0.9765, 0.9961, 0.6745, 0.5373, 0.2627, 0.2627, 0.2627,
          0.2627, 0.9137, 0.7529, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.3255, 0.3216, 0.0549, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.8863, 0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0275, 0.8980, 0.5686, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.2157, 0.9961, 0.4941, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.4824, 0.9961, 0.3765, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.5961, 0.9961, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.8706, 0.9961, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1059,
          0.9373, 0.8000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6588,
          0.9961, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1020, 0.9333,
          0.9961, 0.1137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9961,
          0.9961, 0.6196, 0.2745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7020, 0.9961,
          0.9961, 0.8627, 0.0392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0471, 0.8824, 0.9961,
          0.9373, 0.1255, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.9961, 0.9961,
          0.2706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7020, 0.9961, 0.7686,
          0.0471, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8196, 0.9961, 0.2588,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000],
         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000, 0.0000, 0.0000]]]), 7)
<class 'tuple'>
2
Making Dataloaders
Defining network
2024-06-05 17:02:26.932937
Training!
0:00:00.037710
[1/3, 5/94] Training Loss: 12.010342597961426
[1/3, 10/94] Training Loss: 5.279742240905762
[1/3, 15/94] Training Loss: 2.6306190490722656
[1/3, 20/94] Training Loss: 1.751363754272461
[1/3, 25/94] Training Loss: 1.3350794315338135
[1/3, 30/94] Training Loss: 1.1404885053634644
[1/3, 35/94] Training Loss: 1.0992424488067627
[1/3, 40/94] Training Loss: 1.0971143245697021
[1/3, 45/94] Training Loss: 0.9731976389884949
[1/3, 50/94] Training Loss: 1.0235413312911987
[1/3, 55/94] Training Loss: 0.98687744140625
[1/3, 60/94] Training Loss: 1.0123590230941772
[1/3, 65/94] Training Loss: 0.9714421629905701
[1/3, 70/94] Training Loss: 0.9820316433906555
[1/3, 75/94] Training Loss: 0.9651031494140625
[1/3, 80/94] Training Loss: 0.9259598851203918
[1/3, 85/94] Training Loss: 0.9347138404846191
[1/3, 90/94] Training Loss: 0.9824018478393555
Testing!
[1/3, 1/16]
[1/3, 2/16]
[1/3, 3/16]
[1/3, 4/16]
[1/3, 5/16]
[1/3, 6/16]
[1/3, 7/16]
[1/3, 8/16]
[1/3, 9/16]
[1/3, 10/16]
[1/3, 11/16]
[1/3, 12/16]
[1/3, 13/16]
[1/3, 14/16]
[1/3, 15/16]
[1/3, 16/16]
Testing Loss: 0.9877264499664307
Training!
0:02:06.027985
[2/3, 5/94] Training Loss: 0.960220992565155
[2/3, 10/94] Training Loss: 0.9954237937927246
[2/3, 15/94] Training Loss: 0.9323472380638123
[2/3, 20/94] Training Loss: 0.9316217303276062
[2/3, 25/94] Training Loss: 0.9639245867729187
[2/3, 30/94] Training Loss: 0.9465533494949341
[2/3, 35/94] Training Loss: 0.9252759218215942
[2/3, 40/94] Training Loss: 0.9835085868835449
[2/3, 45/94] Training Loss: 0.9976333975791931
[2/3, 50/94] Training Loss: 0.9569582343101501
[2/3, 55/94] Training Loss: 0.9552454948425293
[2/3, 60/94] Training Loss: 0.9842190146446228
[2/3, 65/94] Training Loss: 0.9591972231864929
[2/3, 70/94] Training Loss: 0.9775727391242981
[2/3, 75/94] Training Loss: 0.9595531821250916
[2/3, 80/94] Training Loss: 0.922676682472229
[2/3, 85/94] Training Loss: 0.9367641806602478
[2/3, 90/94] Training Loss: 0.9640769958496094
Testing!
[2/3, 1/16]
[2/3, 2/16]
[2/3, 3/16]
[2/3, 4/16]
[2/3, 5/16]
[2/3, 6/16]
[2/3, 7/16]
[2/3, 8/16]
[2/3, 9/16]
[2/3, 10/16]
[2/3, 11/16]
[2/3, 12/16]
[2/3, 13/16]
[2/3, 14/16]
[2/3, 15/16]
[2/3, 16/16]
Testing Loss: 1.096240758895874
Training!
0:04:09.428738
[3/3, 5/94] Training Loss: 0.9419581294059753
[3/3, 10/94] Training Loss: 0.9599453806877136
[3/3, 15/94] Training Loss: 0.8783196806907654
[3/3, 20/94] Training Loss: 0.936034619808197
[3/3, 25/94] Training Loss: 0.9364797472953796
[3/3, 30/94] Training Loss: 0.9244005084037781
[3/3, 35/94] Training Loss: 0.9366105198860168
[3/3, 40/94] Training Loss: 0.9570396542549133
[3/3, 45/94] Training Loss: 0.8758034706115723
[3/3, 50/94] Training Loss: 0.9132549166679382
[3/3, 55/94] Training Loss: 0.9392523765563965
[3/3, 60/94] Training Loss: 0.8948888778686523
[3/3, 65/94] Training Loss: 0.9184736609458923
[3/3, 70/94] Training Loss: 0.9126296043395996
[3/3, 75/94] Training Loss: 0.8869265913963318
[3/3, 80/94] Training Loss: 0.9944101572036743
[3/3, 85/94] Training Loss: 0.8991861343383789
[3/3, 90/94] Training Loss: 0.9648860096931458
Testing!
[3/3, 1/16]
[3/3, 2/16]
[3/3, 3/16]
[3/3, 4/16]
[3/3, 5/16]
[3/3, 6/16]
[3/3, 7/16]
[3/3, 8/16]
[3/3, 9/16]
[3/3, 10/16]
[3/3, 11/16]
[3/3, 12/16]
[3/3, 13/16]
[3/3, 14/16]
[3/3, 15/16]
[3/3, 16/16]
Testing Loss: 0.9892669320106506
Training and Testing Finished
0.9892669320106506
<class 'float'>
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(
-- 16/16 --
Applying t-SNE
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation