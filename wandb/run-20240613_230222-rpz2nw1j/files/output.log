Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 6000
Testing: 10000 -> 1000
Making Dataloaders
Defining network
2024-06-13 23:02:25.482143
Scaler Value: 0.010101010101010102
Training - 2024-06-13 23:02:25.483135
[1/9, 10/94] Training Loss: 0.07 - Iteration Time: 0:00:01.268256
[1/9, 20/94] Training Loss: 0.07 - Iteration Time: 0:00:01.356909
[1/9, 30/94] Training Loss: 0.07 - Iteration Time: 0:00:01.312253
[1/9, 40/94] Training Loss: 0.07 - Iteration Time: 0:00:01.331812
[1/9, 50/94] Training Loss: 0.07 - Iteration Time: 0:00:01.314828
[1/9, 60/94] Training Loss: 0.07 - Iteration Time: 0:00:01.298382
[1/9, 70/94] Training Loss: 0.06 - Iteration Time: 0:00:01.297322
[1/9, 80/94] Training Loss: 0.07 - Iteration Time: 0:00:01.272507
[1/9, 90/94] Training Loss: 0.06 - Iteration Time: 0:00:01.265640
Testing - 2024-06-13 23:04:31.338126
[1/9, 10/16]
Testing Loss: 0.06 - Epoch Time: 0:02:17.746822
Training - 2024-06-13 23:04:43.230452
[2/9, 10/94] Training Loss: 0.06 - Iteration Time: 0:00:01.278585
[2/9, 20/94] Training Loss: 0.06 - Iteration Time: 0:00:01.303875
[2/9, 30/94] Training Loss: 0.06 - Iteration Time: 0:00:01.289929
[2/9, 40/94] Training Loss: 0.06 - Iteration Time: 0:00:01.290982
[2/9, 50/94] Training Loss: 0.06 - Iteration Time: 0:00:01.258634
[2/9, 60/94] Training Loss: 0.06 - Iteration Time: 0:00:01.431394
[2/9, 70/94] Training Loss: 0.06 - Iteration Time: 0:00:01.288923
[2/9, 80/94] Training Loss: 0.06 - Iteration Time: 0:00:01.294455
[2/9, 90/94] Training Loss: 0.06 - Iteration Time: 0:00:01.288943
Testing - 2024-06-13 23:06:47.790354
[2/9, 10/16]
Testing Loss: 0.06 - Epoch Time: 0:02:16.399103
Training - 2024-06-13 23:06:59.630050
[3/9, 10/94] Training Loss: 0.06 - Iteration Time: 0:00:01.290987
[3/9, 20/94] Training Loss: 0.06 - Iteration Time: 0:00:01.276620
[3/9, 30/94] Training Loss: 0.06 - Iteration Time: 0:00:01.274035
[3/9, 40/94] Training Loss: 0.06 - Iteration Time: 0:00:01.294422
[3/9, 50/94] Training Loss: 0.06 - Iteration Time: 0:00:01.283975
[3/9, 60/94] Training Loss: 0.06 - Iteration Time: 0:00:01.280605
[3/9, 70/94] Training Loss: 0.06 - Iteration Time: 0:00:01.367300
[3/9, 80/94] Training Loss: 0.06 - Iteration Time: 0:00:01.349002
[3/9, 90/94] Training Loss: 0.06 - Iteration Time: 0:00:01.282485
Testing - 2024-06-13 23:09:05.497547
[3/9, 10/16]
Testing Loss: 0.05 - Epoch Time: 0:02:17.855810
Training - 2024-06-13 23:09:17.486852
[4/9, 10/94] Training Loss: 0.05 - Iteration Time: 0:00:01.303388
[4/9, 20/94] Training Loss: 0.05 - Iteration Time: 0:00:01.262237
[4/9, 30/94] Training Loss: 0.05 - Iteration Time: 0:00:01.293923
[4/9, 40/94] Training Loss: 0.05 - Iteration Time: 0:00:01.358364
[4/9, 50/94] Training Loss: 0.05 - Iteration Time: 0:00:01.291411
[4/9, 60/94] Training Loss: 0.05 - Iteration Time: 0:00:01.291512
[4/9, 70/94] Training Loss: 0.05 - Iteration Time: 0:00:01.298384
[4/9, 80/94] Training Loss: 0.05 - Iteration Time: 0:00:01.402664
[4/9, 90/94] Training Loss: 0.05 - Iteration Time: 0:00:01.291869
Testing - 2024-06-13 23:11:23.099234
[4/9, 10/16]
Testing Loss: 0.05 - Epoch Time: 0:02:17.508865
Training - 2024-06-13 23:11:34.996213
[5/9, 10/94] Training Loss: 0.05 - Iteration Time: 0:00:01.282489
[5/9, 20/94] Training Loss: 0.05 - Iteration Time: 0:00:01.452189
[5/9, 30/94] Training Loss: 0.05 - Iteration Time: 0:00:01.286960
[5/9, 40/94] Training Loss: 0.05 - Iteration Time: 0:00:01.298927
[5/9, 50/94] Training Loss: 0.05 - Iteration Time: 0:00:01.303271
[5/9, 60/94] Training Loss: 0.05 - Iteration Time: 0:00:01.303828
[5/9, 70/94] Training Loss: 0.05 - Iteration Time: 0:00:01.345525
[5/9, 80/94] Training Loss: 0.05 - Iteration Time: 0:00:01.262232
[5/9, 90/94] Training Loss: 0.05 - Iteration Time: 0:00:01.347989
Testing - 2024-06-13 23:13:40.439093
[5/9, 10/16]
Testing Loss: 0.05 - Epoch Time: 0:02:17.310873
Training - 2024-06-13 23:13:52.307581
[6/9, 10/94] Training Loss: 0.05 - Iteration Time: 0:00:01.299318
[6/9, 20/94] Training Loss: 0.05 - Iteration Time: 0:00:01.288945
[6/9, 30/94] Training Loss: 0.05 - Iteration Time: 0:00:01.318760
[6/9, 40/94] Training Loss: 0.05 - Iteration Time: 0:00:01.315291
[6/9, 50/94] Training Loss: 0.05 - Iteration Time: 0:00:01.291351
[6/9, 60/94] Training Loss: 0.05 - Iteration Time: 0:00:01.341574
[6/9, 70/94] Training Loss: 0.04 - Iteration Time: 0:00:01.311376
[6/9, 80/94] Training Loss: 0.04 - Iteration Time: 0:00:01.280483
[6/9, 90/94] Training Loss: 0.04 - Iteration Time: 0:00:01.298432
Testing - 2024-06-13 23:15:58.115109
[6/9, 10/16]
Testing Loss: 0.04 - Epoch Time: 0:02:18.091760
Training - 2024-06-13 23:16:10.400333
[7/9, 10/94] Training Loss: 0.04 - Iteration Time: 0:00:01.320286
[7/9, 20/94] Training Loss: 0.04 - Iteration Time: 0:00:01.286996
[7/9, 30/94] Training Loss: 0.04 - Iteration Time: 0:00:01.272629
[7/9, 40/94] Training Loss: 0.04 - Iteration Time: 0:00:01.599015
[7/9, 50/94] Training Loss: 0.04 - Iteration Time: 0:00:01.338644
[7/9, 60/94] Training Loss: 0.04 - Iteration Time: 0:00:01.309266
[7/9, 70/94] Training Loss: 0.04 - Iteration Time: 0:00:01.298378
[7/9, 80/94] Training Loss: 0.04 - Iteration Time: 0:00:01.304316
[7/9, 90/94] Training Loss: 0.04 - Iteration Time: 0:00:01.339089
Testing - 2024-06-13 23:18:17.013578
[7/9, 10/16]
Testing Loss: 0.04 - Epoch Time: 0:02:18.589924
Training - 2024-06-13 23:18:28.990753
[8/9, 10/94] Training Loss: 0.04 - Iteration Time: 0:00:01.290447
[8/9, 20/94] Training Loss: 0.04 - Iteration Time: 0:00:01.312797
[8/9, 30/94] Training Loss: 0.04 - Iteration Time: 0:00:01.294030
[8/9, 40/94] Training Loss: 0.04 - Iteration Time: 0:00:01.310271
[8/9, 50/94] Training Loss: 0.04 - Iteration Time: 0:00:01.299889
[8/9, 60/94] Training Loss: 0.04 - Iteration Time: 0:00:01.304841
[8/9, 70/94] Training Loss: 0.04 - Iteration Time: 0:00:01.389764
[8/9, 80/94] Training Loss: 0.04 - Iteration Time: 0:00:01.314934
[8/9, 90/94] Training Loss: 0.04 - Iteration Time: 0:00:01.347541
Testing - 2024-06-13 23:20:34.862285
[8/9, 10/16]
Testing Loss: 0.04 - Epoch Time: 0:02:17.712237
Training - 2024-06-13 23:20:46.703485
[9/9, 10/94] Training Loss: 0.04 - Iteration Time: 0:00:01.287997
[9/9, 20/94] Training Loss: 0.04 - Iteration Time: 0:00:01.301336
[9/9, 30/94] Training Loss: 0.04 - Iteration Time: 0:00:01.277576
[9/9, 40/94] Training Loss: 0.04 - Iteration Time: 0:00:01.309704
[9/9, 50/94] Training Loss: 0.04 - Iteration Time: 0:00:01.313311
[9/9, 60/94] Training Loss: 0.04 - Iteration Time: 0:00:01.303850
[9/9, 70/94] Training Loss: 0.04 - Iteration Time: 0:00:01.279075
[9/9, 80/94] Training Loss: 0.04 - Iteration Time: 0:00:01.286437
[9/9, 90/94] Training Loss: 0.04 - Iteration Time: 0:00:01.272111
Testing - 2024-06-13 23:22:52.068568
[9/9, 10/16]
Testing Loss: 0.04 - Epoch Time: 0:02:17.717885
Training and Testing Finished - Time: 0:20:38.939723
Assembling test data for t-sne projection
-- 1/16 --
-- 2/16 --
-- 3/16 --
-- 4/16 --
-- 5/16 --
-- 6/16 --
-- 7/16 --
-- 8/16 --
-- 9/16 --
-- 10/16 --
-- 11/16 --
-- 12/16 --
-- 13/16 --
-- 14/16 --
-- 15/16 --
-- 16/16 --
Plotting Results Grid
Plotting Spiking Input MNIST
Plotting Spiking Input MNIST Animation - 2
WARNING    c:\users\lukea\documents\masters project code\msc_project\main.py:189: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  fig, ax = plt.subplots()
 [py.warnings]
Plotting Spiking Output MNIST
Plotting Spiking Output MNIST Animation - 2