Starting Sweep: Batch Size: 64, Learning Rate: 0.0001
Making datasets and defining subsets
Training: 60000 -> 3000
Testing: 10000 -> 500
Making Subsets
Training: 3000
Testing: 500
Making Dataloaders
Defining network
2024-06-05 11:10:38.640358
Training!
0:00:00.001488
0:00:00.001488
0:00:00.038689
0:00:00.039185
0:00:00.162247
0:00:00.162743
0:00:01.042477
0:00:01.064813
0:00:01.609647
0:00:01.630479
tensor(17.4480, device='cuda:0', grad_fn=<DivBackward0>)
tensor(17.4480, device='cuda:0')
0:00:01.669363
0:00:01.683749
0:00:01.684245
0:00:01.804307
0:00:01.804803
0:00:02.424469
0:00:02.429925
0:00:02.898974
0:00:02.899967
tensor(14.5501, device='cuda:0', grad_fn=<DivBackward0>)
tensor(14.5501, device='cuda:0')
0:00:02.901965
0:00:02.919326
0:00:02.919821
0:00:03.036399
0:00:03.036895
0:00:03.616416
0:00:03.621873
0:00:04.071335
0:00:04.072327
tensor(10.8432, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.8432, device='cuda:0')
0:00:04.074807
[1/1, 3/47] Training Loss: 14.28040885925293
0:00:04.092665
0:00:04.093161
0:00:04.218698
0:00:04.219195
0:00:04.824953
0:00:04.830409
0:00:05.308356
0:00:05.309347
tensor(10.0879, device='cuda:0', grad_fn=<DivBackward0>)
tensor(10.0879, device='cuda:0')
0:00:05.311828
0:00:05.331171
0:00:05.331171
0:00:05.469078
0:00:05.469575
0:00:06.227679
0:00:06.233134
0:00:06.689102
0:00:06.690095
tensor(7.9785, device='cuda:0', grad_fn=<DivBackward0>)
tensor(7.9785, device='cuda:0')
0:00:06.692574
0:00:06.707952
0:00:06.707952
0:00:06.819583
0:00:06.820079
0:00:07.419083
0:00:07.425033
0:00:07.882932
0:00:07.883923
tensor(6.4013, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4013, device='cuda:0')
0:00:07.886899
[1/1, 6/47] Training Loss: 8.155900001525879
0:00:07.905254
0:00:07.905254
0:00:08.032756
0:00:08.033252
0:00:08.633094
0:00:08.639045
0:00:09.086763
0:00:09.088249
tensor(6.4151, device='cuda:0', grad_fn=<DivBackward0>)
tensor(6.4151, device='cuda:0')
0:00:09.090730
0:00:09.116541
0:00:09.117037
0:00:09.236107
0:00:09.237099
0:00:09.825975
0:00:09.831430
0:00:10.298275
0:00:10.299763
tensor(5.4067, device='cuda:0', grad_fn=<DivBackward0>)
tensor(5.4067, device='cuda:0')
0:00:10.301748
0:00:10.319124
0:00:10.319619
0:00:10.436694
0:00:10.437191
0:00:11.034519
0:00:11.040469
0:00:11.498057
0:00:11.499049
tensor(4.2018, device='cuda:0', grad_fn=<DivBackward0>)
tensor(4.2018, device='cuda:0')
0:00:11.501033
[1/1, 9/47] Training Loss: 5.341194152832031
0:00:11.518408
0:00:11.518905
0:00:11.637980
0:00:11.638476
0:00:12.207018
0:00:12.212473
0:00:12.660486
0:00:12.661478
tensor(3.9041, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.9041, device='cuda:0')
0:00:12.663959
0:00:12.680823
0:00:12.680823
0:00:12.808836
0:00:12.809828
0:00:13.390288
0:00:13.396239
0:00:13.845873
0:00:13.846865
tensor(3.3639, device='cuda:0', grad_fn=<DivBackward0>)
tensor(3.3639, device='cuda:0')
0:00:13.848849
0:00:13.866210
0:00:13.866705
0:00:13.982788
0:00:13.983780
0:00:14.552333
0:00:14.557790
0:00:14.998346
0:00:14.999834
tensor(2.9329, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.9329, device='cuda:0')
0:00:15.001818
[1/1, 12/47] Training Loss: 3.4002957344055176
0:00:15.023149
0:00:15.023644
0:00:15.134298
0:00:15.135290
0:00:15.721876
0:00:15.727345
0:00:16.174099
0:00:16.175091
tensor(2.7773, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.7773, device='cuda:0')
0:00:16.177075
0:00:16.194435
0:00:16.194435
0:00:16.303573
0:00:16.304070
0:00:16.875622
0:00:16.881077
0:00:17.326593
0:00:17.327585
tensor(2.4229, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.4229, device='cuda:0')
0:00:17.330077
0:00:17.350415
0:00:17.350415
0:00:17.471951
0:00:17.472448
0:00:18.060982
0:00:18.066439
0:00:18.511952
0:00:18.513440
tensor(2.1830, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.1830, device='cuda:0')
0:00:18.515920
[1/1, 15/47] Training Loss: 2.4610748291015625
0:00:18.535777
0:00:18.536272
0:00:18.649934
0:00:18.650430
0:00:19.227401
0:00:19.232858
0:00:19.676882
0:00:19.677873
tensor(2.0861, device='cuda:0', grad_fn=<DivBackward0>)
tensor(2.0861, device='cuda:0')
0:00:19.680353
0:00:19.697219
0:00:19.697715
0:00:19.815781
0:00:19.816277
0:00:20.397861
0:00:20.403317
0:00:20.841902
0:00:20.842895
tensor(1.8822, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.8822, device='cuda:0')
0:00:20.845374
0:00:20.862734
0:00:20.862734
0:00:20.989728
0:00:20.990719
0:00:21.595003
0:00:21.600460
0:00:22.055396
0:00:22.056884
tensor(1.7341, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.7341, device='cuda:0')
0:00:22.059364
[1/1, 18/47] Training Loss: 1.9008088111877441
0:00:22.088744
0:00:22.088744
0:00:22.218272
0:00:22.218768
0:00:22.877619
0:00:22.883571
0:00:23.392138
0:00:23.393130
tensor(1.6282, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6282, device='cuda:0')
0:00:23.395610
0:00:23.412475
0:00:23.412971
0:00:23.530046
0:00:23.530543
0:00:24.130346
0:00:24.135802
0:00:24.586972
0:00:24.587965
tensor(1.6003, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.6003, device='cuda:0')
0:00:24.589949
0:00:24.611775
0:00:24.611775
0:00:24.734325
0:00:24.734821
0:00:25.322728
0:00:25.328183
0:00:25.779650
0:00:25.781137
tensor(1.4794, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.4794, device='cuda:0')
0:00:25.783121
[1/1, 21/47] Training Loss: 1.5693252086639404
0:00:25.800979
0:00:25.800979
0:00:25.927480
0:00:25.927976
0:00:26.509427
0:00:26.514883
0:00:26.973354
0:00:26.974347
tensor(1.3845, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3845, device='cuda:0')
0:00:26.976331
0:00:26.999147
0:00:26.999643
0:00:27.115725
0:00:27.116221
0:00:27.703141
0:00:27.708596
0:00:28.157609
0:00:28.158600
tensor(1.3865, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.3865, device='cuda:0')
0:00:28.161080
0:00:28.177945
0:00:28.177945
0:00:28.297003
0:00:28.297499
0:00:28.882605
0:00:28.888060
0:00:29.327131
0:00:29.328123
tensor(1.2764, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2764, device='cuda:0')
0:00:29.330107
[1/1, 24/47] Training Loss: 1.3491328954696655
0:00:29.347966
0:00:29.348461
0:00:29.468029
0:00:29.468525
0:00:30.059887
0:00:30.065343
0:00:30.536161
0:00:30.537649
tensor(1.2525, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2525, device='cuda:0')
0:00:30.539632
0:00:30.556003
0:00:30.557007
0:00:30.667167
0:00:30.668159
0:00:31.248796
0:00:31.254251
0:00:31.689349
0:00:31.690837
tensor(1.2333, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2333, device='cuda:0')
0:00:31.692820
0:00:31.710181
0:00:31.710181
0:00:31.824294
0:00:31.824790
0:00:32.422116
0:00:32.427571
0:00:32.878558
0:00:32.880047
tensor(1.1618, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1618, device='cuda:0')
0:00:32.882031
[1/1, 27/47] Training Loss: 1.2158536911010742
0:00:32.899889
0:00:32.900385
0:00:33.017197
0:00:33.017694
0:00:33.601138
0:00:33.607091
0:00:34.062012
0:00:34.063004
tensor(1.2069, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2069, device='cuda:0')
0:00:34.065497
0:00:34.082362
0:00:34.082857
0:00:34.197492
0:00:34.197988
0:00:34.782910
0:00:34.788366
0:00:35.233532
0:00:35.234524
tensor(1.1852, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1852, device='cuda:0')
0:00:35.237004
0:00:35.254366
0:00:35.254366
0:00:35.363015
0:00:35.363511
0:00:35.943464
0:00:35.948423
0:00:36.414296
0:00:36.415288
tensor(1.1308, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1308, device='cuda:0')
0:00:36.417768
[1/1, 30/47] Training Loss: 1.1743162870407104
0:00:36.437113
0:00:36.437113
0:00:36.559642
0:00:36.560634
0:00:37.171045
0:00:37.178981
0:00:37.662176
0:00:37.663168
tensor(1.0955, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0955, device='cuda:0')
0:00:37.665648
0:00:37.687489
0:00:37.687489
0:00:37.805584
0:00:37.806080
0:00:38.393501
0:00:38.398460
0:00:38.838021
0:00:38.839509
tensor(1.2071, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.2071, device='cuda:0')
0:00:38.841990
0:00:38.858855
0:00:38.859351
0:00:38.972982
0:00:38.972982
0:00:39.573966
0:00:39.580429
0:00:40.043782
0:00:40.044773
tensor(1.0447, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0447, device='cuda:0')
0:00:40.047254
[1/1, 33/47] Training Loss: 1.1157898902893066
0:00:40.067098
0:00:40.067098
0:00:40.197609
0:00:40.198106
0:00:40.793948
0:00:40.799403
0:00:41.250873
0:00:41.252360
tensor(1.0260, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0260, device='cuda:0')
0:00:41.254344
0:00:41.271708
0:00:41.272204
0:00:41.384333
0:00:41.384829
0:00:42.022039
0:00:42.027495
0:00:42.514691
0:00:42.516179
tensor(1.0825, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0825, device='cuda:0')
0:00:42.518163
0:00:42.535524
0:00:42.536019
0:00:42.657093
0:00:42.657589
0:00:43.263344
0:00:43.268803
0:00:43.731337
0:00:43.732329
tensor(1.1061, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1061, device='cuda:0')
0:00:43.734809
[1/1, 36/47] Training Loss: 1.0715367794036865
0:00:43.753162
0:00:43.753162
0:00:43.875195
0:00:43.875691
0:00:44.493371
0:00:44.498827
0:00:44.947317
0:00:44.948310
tensor(1.0308, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0308, device='cuda:0')
0:00:44.950789
0:00:44.967654
0:00:44.968150
0:00:45.092680
0:00:45.093176
0:00:45.733186
0:00:45.739138
0:00:46.190854
0:00:46.192343
tensor(1.1047, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.1047, device='cuda:0')
0:00:46.195814
0:00:46.215654
0:00:46.215654
0:00:46.351082
0:00:46.351578
0:00:46.926607
0:00:46.932063
0:00:47.387020
0:00:47.388509
tensor(0.9936, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9936, device='cuda:0')
0:00:47.389996
[1/1, 39/47] Training Loss: 1.043029546737671
0:00:47.406380
0:00:47.406875
0:00:47.536349
0:00:47.537340
0:00:48.231181
0:00:48.238621
0:00:48.694081
0:00:48.695073
tensor(0.9738, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9738, device='cuda:0')
0:00:48.697554
0:00:48.714418
0:00:48.714914
0:00:48.830995
0:00:48.831491
0:00:49.404020
0:00:49.409475
0:00:49.847535
0:00:49.848527
tensor(1.0862, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0862, device='cuda:0')
0:00:49.850511
0:00:49.867871
0:00:49.867871
0:00:49.984948
0:00:49.985444
0:00:50.567122
0:00:50.572577
0:00:51.010682
0:00:51.011673
tensor(1.0504, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0504, device='cuda:0')
0:00:51.014153
[1/1, 42/47] Training Loss: 1.0368398427963257
0:00:51.032009
0:00:51.032009
0:00:51.133708
0:00:51.134204
0:00:51.711210
0:00:51.716666
0:00:52.154729
0:00:52.156217
tensor(1.0134, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0134, device='cuda:0')
0:00:52.158201
0:00:52.175576
0:00:52.175576
0:00:52.301739
0:00:52.302236
0:00:52.895084
0:00:52.900540
0:00:53.339128
0:00:53.340120
tensor(1.0411, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0411, device='cuda:0')
0:00:53.342600
0:00:53.360458
0:00:53.360952
0:00:53.484037
0:00:53.484533
0:00:54.063488
0:00:54.068943
0:00:54.512253
0:00:54.513245
tensor(1.0475, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0475, device='cuda:0')
0:00:54.515725
[1/1, 45/47] Training Loss: 1.033993124961853
0:00:54.533582
0:00:54.533582
0:00:54.652677
0:00:54.653173
0:00:55.223740
0:00:55.229195
0:00:55.672709
0:00:55.673702
tensor(0.9105, device='cuda:0', grad_fn=<DivBackward0>)
tensor(0.9105, device='cuda:0')
0:00:55.676182
0:00:55.692055
0:00:55.692055
0:00:55.788298
0:00:55.789290
0:00:56.534955
0:00:56.540412
0:00:57.029143
0:00:57.030136
tensor(1.0235, device='cuda:0', grad_fn=<DivBackward0>)
tensor(1.0235, device='cuda:0')
0:00:57.032119
Testing!
[1/1, 1/8]
[1/1, 2/8]
[1/1, 3/8]
[1/1, 4/8]
[1/1, 5/8]
[1/1, 6/8]
[1/1, 7/8]
[1/1, 8/8]