Making Datasets...
Making Dataloaders...
Defining network...
5/5: Training network...
[1/3, 10/938] Training Loss: 1.0801329135894775
[1/3, 20/938] Training Loss: 1.0690864145755767
[1/3, 30/938] Training Loss: 1.0758377134799957
[1/3, 40/938] Training Loss: 1.0654727458953857
[1/3, 50/938] Training Loss: 1.0253552317619323
[1/3, 60/938] Training Loss: 1.0628627300262452
[1/3, 70/938] Training Loss: 1.0195378243923188
[1/3, 80/938] Training Loss: 1.0436198949813842
[1/3, 90/938] Training Loss: 1.034772163629532
[1/3, 100/938] Training Loss: 1.0361847162246705
[1/3, 110/938] Training Loss: 1.004613322019577
[1/3, 120/938] Training Loss: 0.9928660750389099
[1/3, 130/938] Training Loss: 1.0210096836090088
[1/3, 140/938] Training Loss: 1.0042659878730773
[1/3, 150/938] Training Loss: 0.9985517024993896
[1/3, 160/938] Training Loss: 0.9626680195331574
[1/3, 170/938] Training Loss: 0.9521257400512695
[1/3, 180/938] Training Loss: 0.9929309844970703
[1/3, 190/938] Training Loss: 0.9446967899799347
[1/3, 200/938] Training Loss: 0.9919232130050659
[1/3, 210/938] Training Loss: 0.9837528169155121
[1/3, 220/938] Training Loss: 0.9443625152111054
[1/3, 230/938] Training Loss: 0.919029277563095
[1/3, 240/938] Training Loss: 0.9353987574577332
[1/3, 250/938] Training Loss: 0.9370924770832062
[1/3, 260/938] Training Loss: 0.9389018416404724
[1/3, 270/938] Training Loss: 0.9241318821907043
[1/3, 280/938] Training Loss: 0.9355046451091766
[1/3, 290/938] Training Loss: 0.940503841638565
[1/3, 300/938] Training Loss: 0.9205121159553528
[1/3, 310/938] Training Loss: 0.9189786612987518
[1/3, 320/938] Training Loss: 0.9037931978702545
[1/3, 330/938] Training Loss: 0.885867440700531
[1/3, 340/938] Training Loss: 0.9110864758491516
[1/3, 350/938] Training Loss: 0.8880100548267365
[1/3, 360/938] Training Loss: 0.8884781897068024
[1/3, 370/938] Training Loss: 0.8863394975662231
[1/3, 380/938] Training Loss: 0.8906973719596862
[1/3, 390/938] Training Loss: 0.8811409115791321
[1/3, 400/938] Training Loss: 0.8594307243824005
[1/3, 410/938] Training Loss: 0.8794565320014953
[1/3, 420/938] Training Loss: 0.8458204627037048
[1/3, 430/938] Training Loss: 0.8665565192699433
[1/3, 440/938] Training Loss: 0.831132584810257
[1/3, 450/938] Training Loss: 0.8185393691062928
[1/3, 460/938] Training Loss: 0.8370113253593445
[1/3, 470/938] Training Loss: 0.822452062368393
[1/3, 480/938] Training Loss: 0.8277883112430573
[1/3, 490/938] Training Loss: 0.8166431307792663
[1/3, 500/938] Training Loss: 0.813078510761261
[1/3, 510/938] Training Loss: 0.8016110301017761
[1/3, 520/938] Training Loss: 0.7860053837299347
[1/3, 530/938] Training Loss: 0.7937868416309357
[1/3, 540/938] Training Loss: 0.8108416438102722
[1/3, 550/938] Training Loss: 0.7888046264648437
[1/3, 560/938] Training Loss: 0.8123570442199707
[1/3, 570/938] Training Loss: 0.8069659054279328
[1/3, 580/938] Training Loss: 0.7750035345554351
[1/3, 590/938] Training Loss: 0.8051151275634766
[1/3, 600/938] Training Loss: 0.7946712851524353
[1/3, 610/938] Training Loss: 0.7878002643585205
[1/3, 620/938] Training Loss: 0.7897056639194489
[1/3, 630/938] Training Loss: 0.7850375652313233
[1/3, 640/938] Training Loss: 0.7693700850009918
[1/3, 650/938] Training Loss: 0.7756981670856475
[1/3, 660/938] Training Loss: 0.7722715973854065
[1/3, 670/938] Training Loss: 0.7618642568588256
[1/3, 680/938] Training Loss: 0.7583730340003967
[1/3, 690/938] Training Loss: 0.7242717385292053
[1/3, 700/938] Training Loss: 0.7606816232204437
[1/3, 710/938] Training Loss: 0.7424807369709014
[1/3, 720/938] Training Loss: 0.7514276921749115
[1/3, 730/938] Training Loss: 0.7239724576473237
[1/3, 740/938] Training Loss: 0.7429196238517761
[1/3, 750/938] Training Loss: 0.727208036184311
[1/3, 760/938] Training Loss: 0.7438727021217346
[1/3, 770/938] Training Loss: 0.7475699007511138
[1/3, 780/938] Training Loss: 0.7676677763462066
[1/3, 790/938] Training Loss: 0.72510324716568
[1/3, 800/938] Training Loss: 0.7364074528217316
[1/3, 810/938] Training Loss: 0.7644767940044404
[1/3, 820/938] Training Loss: 0.7264721393585205
[1/3, 830/938] Training Loss: 0.7314518868923188
[1/3, 840/938] Training Loss: 0.7235684216022491
[1/3, 850/938] Training Loss: 0.7174282073974609
[1/3, 860/938] Training Loss: 0.7417298913002014
[1/3, 870/938] Training Loss: 0.6860297977924347
[1/3, 880/938] Training Loss: 0.7273168921470642
[1/3, 890/938] Training Loss: 0.711171007156372
[1/3, 900/938] Training Loss: 0.7265419960021973
[1/3, 910/938] Training Loss: 0.7154871761798859
[1/3, 920/938] Training Loss: 0.6965970575809479
[1/3, 930/938] Training Loss: 0.686192125082016
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.7129067476590476
5/5: Training network...
[2/3, 10/938] Training Loss: 0.7184681177139283
[2/3, 20/938] Training Loss: 0.7122944235801697
[2/3, 30/938] Training Loss: 0.6991805016994477
[2/3, 40/938] Training Loss: 0.7070499062538147
[2/3, 50/938] Training Loss: 0.711313784122467
[2/3, 60/938] Training Loss: 0.6982529997825623
[2/3, 70/938] Training Loss: 0.6837145745754242
[2/3, 80/938] Training Loss: 0.6813082456588745
[2/3, 90/938] Training Loss: 0.6753684520721436
[2/3, 100/938] Training Loss: 0.6737085998058319
[2/3, 110/938] Training Loss: 0.6678655207157135
[2/3, 120/938] Training Loss: 0.6935693204402924
[2/3, 130/938] Training Loss: 0.7026262521743775
[2/3, 140/938] Training Loss: 0.6704967856407166
[2/3, 150/938] Training Loss: 0.6787999033927917
[2/3, 160/938] Training Loss: 0.6773003995418548
[2/3, 170/938] Training Loss: 0.6828689575195312
[2/3, 180/938] Training Loss: 0.6789269328117371
[2/3, 190/938] Training Loss: 0.6834218859672546
[2/3, 200/938] Training Loss: 0.6834358632564544
[2/3, 210/938] Training Loss: 0.6829940855503083
[2/3, 220/938] Training Loss: 0.6621710598468781
[2/3, 230/938] Training Loss: 0.6415404379367828
[2/3, 240/938] Training Loss: 0.6687198281288147
[2/3, 250/938] Training Loss: 0.648853999376297
[2/3, 260/938] Training Loss: 0.6661141037940979
[2/3, 270/938] Training Loss: 0.6569796442985535
[2/3, 280/938] Training Loss: 0.6510336458683014
[2/3, 290/938] Training Loss: 0.64480140209198
[2/3, 300/938] Training Loss: 0.6472396671772003
[2/3, 310/938] Training Loss: 0.6515600860118866
[2/3, 320/938] Training Loss: 0.6509354412555695
[2/3, 330/938] Training Loss: 0.6282081782817841
[2/3, 340/938] Training Loss: 0.6375010728836059
[2/3, 350/938] Training Loss: 0.6202250063419342
[2/3, 360/938] Training Loss: 0.6395458996295929
[2/3, 370/938] Training Loss: 0.6436144828796386
[2/3, 380/938] Training Loss: 0.6405398845672607
[2/3, 390/938] Training Loss: 0.6336363017559051
[2/3, 400/938] Training Loss: 0.6316812396049499
[2/3, 410/938] Training Loss: 0.6323240637779236
[2/3, 420/938] Training Loss: 0.6095328390598297
[2/3, 430/938] Training Loss: 0.6282785415649415
[2/3, 440/938] Training Loss: 0.6194612622261048
[2/3, 450/938] Training Loss: 0.5900936365127564
[2/3, 460/938] Training Loss: 0.5888793766498566
[2/3, 470/938] Training Loss: 0.6155149281024933
[2/3, 480/938] Training Loss: 0.6210569977760315
[2/3, 490/938] Training Loss: 0.6032083928585052
[2/3, 500/938] Training Loss: 0.6141440033912658
[2/3, 510/938] Training Loss: 0.6184009313583374
[2/3, 520/938] Training Loss: 0.6087970912456513
[2/3, 530/938] Training Loss: 0.6032132029533386
[2/3, 540/938] Training Loss: 0.6133155405521393
[2/3, 550/938] Training Loss: 0.6056323170661926
[2/3, 560/938] Training Loss: 0.5896141350269317
[2/3, 570/938] Training Loss: 0.6205076456069947
[2/3, 580/938] Training Loss: 0.6025427222251892
[2/3, 590/938] Training Loss: 0.6205022215843201
[2/3, 600/938] Training Loss: 0.602239853143692
[2/3, 610/938] Training Loss: 0.5879366993904114
[2/3, 620/938] Training Loss: 0.5957832038402557
[2/3, 630/938] Training Loss: 0.608692729473114
[2/3, 640/938] Training Loss: 0.6004641711711883
[2/3, 650/938] Training Loss: 0.594092857837677
[2/3, 660/938] Training Loss: 0.6041593015193939
[2/3, 670/938] Training Loss: 0.5977541148662567
[2/3, 680/938] Training Loss: 0.5999964654445649
[2/3, 690/938] Training Loss: 0.5594766110181808
[2/3, 700/938] Training Loss: 0.5986991703510285
[2/3, 710/938] Training Loss: 0.5890446126461029
[2/3, 720/938] Training Loss: 0.5948953449726104
[2/3, 730/938] Training Loss: 0.5978135526180267
[2/3, 740/938] Training Loss: 0.5958273231983184
[2/3, 750/938] Training Loss: 0.5780447542667388
[2/3, 760/938] Training Loss: 0.5839162737131118
[2/3, 770/938] Training Loss: 0.5811141073703766
[2/3, 780/938] Training Loss: 0.5897892653942108
[2/3, 790/938] Training Loss: 0.5811284005641937
[2/3, 800/938] Training Loss: 0.5715781450271606
[2/3, 810/938] Training Loss: 0.5741755306720734
[2/3, 820/938] Training Loss: 0.5522367209196091
[2/3, 830/938] Training Loss: 0.5703611493110656
[2/3, 840/938] Training Loss: 0.5696227610111236
[2/3, 850/938] Training Loss: 0.5719805240631104
[2/3, 860/938] Training Loss: 0.5727990925312042
[2/3, 870/938] Training Loss: 0.556144580245018
[2/3, 880/938] Training Loss: 0.5538230061531066
[2/3, 890/938] Training Loss: 0.5545092195272445
[2/3, 900/938] Training Loss: 0.5629984498023987
[2/3, 910/938] Training Loss: 0.5601046025753021
[2/3, 920/938] Training Loss: 0.5645116031169891
[2/3, 930/938] Training Loss: 0.5683749318122864
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.6351279344161351
5/5: Training network...
[3/3, 10/938] Training Loss: 0.5605299234390259
[3/3, 20/938] Training Loss: 0.5649721324443817
[3/3, 30/938] Training Loss: 0.5459079146385193
[3/3, 40/938] Training Loss: 0.5636097490787506
[3/3, 50/938] Training Loss: 0.5450511753559113
[3/3, 60/938] Training Loss: 0.5411505401134491
[3/3, 70/938] Training Loss: 0.5481266409158707
[3/3, 80/938] Training Loss: 0.5434838861227036
[3/3, 90/938] Training Loss: 0.5404399454593658
[3/3, 100/938] Training Loss: 0.556350153684616
[3/3, 110/938] Training Loss: 0.5455861628055573
[3/3, 120/938] Training Loss: 0.5438000082969665
[3/3, 130/938] Training Loss: 0.5368427038192749
[3/3, 140/938] Training Loss: 0.5475284576416015
[3/3, 150/938] Training Loss: 0.5388552993535995
[3/3, 160/938] Training Loss: 0.5256037831306457
[3/3, 170/938] Training Loss: 0.5375178694725037
[3/3, 180/938] Training Loss: 0.5421737372875214
[3/3, 190/938] Training Loss: 0.5433888256549835
[3/3, 200/938] Training Loss: 0.5413323879241944
[3/3, 210/938] Training Loss: 0.5376224100589753
[3/3, 220/938] Training Loss: 0.512810543179512
[3/3, 230/938] Training Loss: 0.5433838903903961
[3/3, 240/938] Training Loss: 0.5347915142774582
[3/3, 250/938] Training Loss: 0.5299114763736725
[3/3, 260/938] Training Loss: 0.5305253028869629
[3/3, 270/938] Training Loss: 0.5332229793071747
[3/3, 280/938] Training Loss: 0.5345924496650696
[3/3, 290/938] Training Loss: 0.5204068899154664
[3/3, 300/938] Training Loss: 0.5507530152797699
[3/3, 310/938] Training Loss: 0.5343032330274582
[3/3, 320/938] Training Loss: 0.5147455036640167
[3/3, 330/938] Training Loss: 0.5318881511688233
[3/3, 340/938] Training Loss: 0.5204006373882294
[3/3, 350/938] Training Loss: 0.5207183480262756
[3/3, 360/938] Training Loss: 0.5095370769500732
[3/3, 370/938] Training Loss: 0.5176051974296569
[3/3, 380/938] Training Loss: 0.5380355328321457
[3/3, 390/938] Training Loss: 0.5212265431880951
[3/3, 400/938] Training Loss: 0.525440439581871
[3/3, 410/938] Training Loss: 0.5315688043832779
[3/3, 420/938] Training Loss: 0.5114417016506195
[3/3, 430/938] Training Loss: 0.5213565409183503
[3/3, 440/938] Training Loss: 0.5402307569980621
[3/3, 450/938] Training Loss: 0.5002260237932206
[3/3, 460/938] Training Loss: 0.5209908902645111
[3/3, 470/938] Training Loss: 0.5304511666297913
[3/3, 480/938] Training Loss: 0.5220146119594574
[3/3, 490/938] Training Loss: 0.5127031952142715
[3/3, 500/938] Training Loss: 0.5252422630786896
[3/3, 510/938] Training Loss: 0.5217854470014572
[3/3, 520/938] Training Loss: 0.5078369617462158
[3/3, 530/938] Training Loss: 0.5418531477451325
[3/3, 540/938] Training Loss: 0.5197001487016678
[3/3, 550/938] Training Loss: 0.5244534105062485
[3/3, 560/938] Training Loss: 0.5333760261535645
[3/3, 570/938] Training Loss: 0.5142987847328186
[3/3, 580/938] Training Loss: 0.5070464044809342
[3/3, 590/938] Training Loss: 0.5197083294391632
[3/3, 600/938] Training Loss: 0.5133859127759933
[3/3, 610/938] Training Loss: 0.5208701193332672
[3/3, 620/938] Training Loss: 0.49649969637393954
[3/3, 630/938] Training Loss: 0.48853146731853486
[3/3, 640/938] Training Loss: 0.5139929175376892
[3/3, 650/938] Training Loss: 0.5089583307504654
[3/3, 660/938] Training Loss: 0.49529518783092497
[3/3, 670/938] Training Loss: 0.5043693512678147
[3/3, 680/938] Training Loss: 0.5106109619140625
[3/3, 690/938] Training Loss: 0.5062441885471344
[3/3, 700/938] Training Loss: 0.5047974705696106
[3/3, 710/938] Training Loss: 0.4928936272859573
[3/3, 720/938] Training Loss: 0.48819621205329894
[3/3, 730/938] Training Loss: 0.4915377140045166
[3/3, 740/938] Training Loss: 0.5054865837097168
[3/3, 750/938] Training Loss: 0.4965778052806854
[3/3, 760/938] Training Loss: 0.4852607011795044
[3/3, 770/938] Training Loss: 0.49317303001880647
[3/3, 780/938] Training Loss: 0.475131756067276
[3/3, 790/938] Training Loss: 0.4730223000049591
[3/3, 800/938] Training Loss: 0.4823883265256882
[3/3, 810/938] Training Loss: 0.47262153029441833
[3/3, 820/938] Training Loss: 0.4897939383983612
[3/3, 830/938] Training Loss: 0.4905280560255051
[3/3, 840/938] Training Loss: 0.5074705630540848
[3/3, 850/938] Training Loss: 0.4763280004262924
[3/3, 860/938] Training Loss: 0.48975583612918855
[3/3, 870/938] Training Loss: 0.4844145059585571
[3/3, 880/938] Training Loss: 0.4708250492811203
[3/3, 890/938] Training Loss: 0.47727990448474883
[3/3, 900/938] Training Loss: 0.4818161576986313
[3/3, 910/938] Training Loss: 0.47612704932689665
[3/3, 920/938] Training Loss: 0.476036936044693
[3/3, 930/938] Training Loss: 0.4872995376586914
5/5: Testing network...
Testing: 1/157
Testing: 2/157
Testing: 3/157
Testing: 4/157
Testing: 5/157
Testing: 6/157
Testing: 7/157
Testing: 8/157
Testing: 9/157
Testing: 10/157
Testing: 11/157
Testing: 12/157
Testing: 13/157
Testing: 14/157
Testing: 15/157
Testing: 16/157
Testing: 17/157
Testing: 18/157
Testing: 19/157
Testing: 20/157
Testing: 21/157
Testing: 22/157
Testing: 23/157
Testing: 24/157
Testing: 25/157
Testing: 26/157
Testing: 27/157
Testing: 28/157
Testing: 29/157
Testing: 30/157
Testing: 31/157
Testing: 32/157
Testing: 33/157
Testing: 34/157
Testing: 35/157
Testing: 36/157
Testing: 37/157
Testing: 38/157
Testing: 39/157
Testing: 40/157
Testing: 41/157
Testing: 42/157
Testing: 43/157
Testing: 44/157
Testing: 45/157
Testing: 46/157
Testing: 47/157
Testing: 48/157
Testing: 49/157
Testing: 50/157
Testing: 51/157
Testing: 52/157
Testing: 53/157
Testing: 54/157
Testing: 55/157
Testing: 56/157
Testing: 57/157
Testing: 58/157
Testing: 59/157
Testing: 60/157
Testing: 61/157
Testing: 62/157
Testing: 63/157
Testing: 64/157
Testing: 65/157
Testing: 66/157
Testing: 67/157
Testing: 68/157
Testing: 69/157
Testing: 70/157
Testing: 71/157
Testing: 72/157
Testing: 73/157
Testing: 74/157
Testing: 75/157
Testing: 76/157
Testing: 77/157
Testing: 78/157
Testing: 79/157
Testing: 80/157
Testing: 81/157
Testing: 82/157
Testing: 83/157
Testing: 84/157
Testing: 85/157
Testing: 86/157
Testing: 87/157
Testing: 88/157
Testing: 89/157
Testing: 90/157
Testing: 91/157
Testing: 92/157
Testing: 93/157
Testing: 94/157
Testing: 95/157
Testing: 96/157
Testing: 97/157
Testing: 98/157
Testing: 99/157
Testing: 100/157
Testing: 101/157
Testing: 102/157
Testing: 103/157
Testing: 104/157
Testing: 105/157
Testing: 106/157
Testing: 107/157
Testing: 108/157
Testing: 109/157
Testing: 110/157
Testing: 111/157
Testing: 112/157
Testing: 113/157
Testing: 114/157
Testing: 115/157
Testing: 116/157
Testing: 117/157
Testing: 118/157
Testing: 119/157
Testing: 120/157
Testing: 121/157
Testing: 122/157
Testing: 123/157
Testing: 124/157
Testing: 125/157
Testing: 126/157
Testing: 127/157
Testing: 128/157
Testing: 129/157
Testing: 130/157
Testing: 131/157
Testing: 132/157
Testing: 133/157
Testing: 134/157
Testing: 135/157
Testing: 136/157
Testing: 137/157
Testing: 138/157
Testing: 139/157
Testing: 140/157
Testing: 141/157
Testing: 142/157
Testing: 143/157
Testing: 144/157
Testing: 145/157
Testing: 146/157
Testing: 147/157
Testing: 148/157
Testing: 149/157
Testing: 150/157
Testing: 151/157
Testing: 152/157
Testing: 153/157
Testing: 154/157
Testing: 155/157
Testing: 156/157
Testing: 157/157
Testing Loss: 0.582654953598976
Training and Testing Finished
Assembling test data for t-sne projection
Batch: 1/157
Batch: 2/157
Batch: 3/157
Batch: 4/157
Batch: 5/157
Batch: 6/157
Batch: 7/157
Batch: 8/157
Batch: 9/157
Batch: 10/157
Batch: 11/157
Batch: 12/157
Batch: 13/157
Batch: 14/157
Batch: 15/157
Batch: 16/157
Batch: 17/157
Batch: 18/157
Batch: 19/157
Batch: 20/157
Batch: 21/157
Batch: 22/157
Batch: 23/157
Batch: 24/157
Batch: 25/157
Batch: 26/157
Batch: 27/157
Batch: 28/157
Batch: 29/157
Batch: 30/157
Batch: 31/157
Batch: 32/157
Batch: 33/157
Batch: 34/157
Batch: 35/157
Batch: 36/157
Batch: 37/157
Batch: 38/157
Batch: 39/157
Batch: 40/157
Batch: 41/157
Batch: 42/157
Batch: 43/157
Batch: 44/157
Batch: 45/157
Batch: 46/157
Batch: 47/157
Batch: 48/157
Batch: 49/157
Batch: 50/157
Batch: 51/157
Batch: 52/157
Batch: 53/157
Batch: 54/157
Batch: 55/157
Batch: 56/157
Batch: 57/157
Batch: 58/157
Batch: 59/157
Batch: 60/157
Batch: 61/157
Batch: 62/157
Batch: 63/157
Batch: 64/157
Batch: 65/157
Batch: 66/157
Batch: 67/157
Batch: 68/157
Batch: 69/157
Batch: 70/157
Batch: 71/157
Batch: 72/157
Batch: 73/157
Batch: 74/157
Batch: 75/157
Batch: 76/157
Batch: 77/157
Batch: 78/157
Batch: 79/157
Batch: 80/157
Batch: 81/157
Batch: 82/157
Batch: 83/157
Batch: 84/157
Batch: 85/157
Batch: 86/157
Batch: 87/157
Batch: 88/157
Batch: 89/157
Batch: 90/157
Batch: 91/157
Batch: 92/157
Batch: 93/157
Batch: 94/157
Batch: 95/157
Batch: 96/157
Batch: 97/157
Batch: 98/157
Batch: 99/157
Batch: 100/157
Batch: 101/157
Batch: 102/157
Batch: 103/157
Batch: 104/157
Batch: 105/157
Batch: 106/157
Batch: 107/157
Batch: 108/157
Batch: 109/157
Batch: 110/157
Batch: 111/157
Batch: 112/157
Batch: 113/157
Batch: 114/157
Batch: 115/157
Batch: 116/157
Batch: 117/157
Batch: 118/157
Batch: 119/157
Batch: 120/157
Batch: 121/157
Batch: 122/157
Batch: 123/157
Batch: 124/157
Batch: 125/157
Batch: 126/157
Batch: 127/157
Batch: 128/157
Batch: 129/157
Batch: 130/157
Batch: 131/157
Batch: 132/157
Batch: 133/157
Batch: 134/157
Batch: 135/157
Batch: 136/157
Batch: 137/157
Batch: 138/157
Batch: 139/157
Batch: 140/157
Batch: 141/157
Batch: 142/157
Batch: 143/157
Batch: 144/157
Batch: 145/157
Batch: 146/157
Batch: 147/157
Batch: 148/157
Batch: 149/157
Batch: 150/157
Batch: 151/157
Batch: 152/157
Batch: 153/157
Batch: 154/157
Batch: 155/157
Batch: 156/157
Batch: 157/157
0 added
3 added
6 added
7 added
1 added
9 added
8 added
4 added
5 added
2 added
Applying t-SNE
WARNING    C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py:110: UserWarning: Could not find the number of physical cores for the following reason:
found 0 physical cores < 1
Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.
  warnings.warn(
 [py.warnings]
  File "C:\Users\lukea\anaconda3\envs\diss\Lib\site-packages\joblib\externals\loky\backend\context.py", line 217, in _count_physical_cores
    raise ValueError(